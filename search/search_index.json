{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Overview \u00b6 Welcome to the Amazon EKS Blueprints Quick Start documentation site. This repository contains the source code for the eks-blueprints NPM module. It can be used by AWS customers, partners, and internal AWS teams to configure and manage complete EKS clusters that are fully bootstrapped with the operational software that is needed to deploy and operate workloads. What is EKS Blueprints? \u00b6 EKS Blueprints helps you compose complete EKS clusters that are fully bootstrapped with the operational software that is needed to deploy and operate workloads. With EKS Blueprints, you describe the configuration for the desired state of your EKS environment, such as the control plane, worker nodes, and Kubernetes add-ons, as an IaC blueprint. Once a blueprint is configured, you can use it to stamp out consistent environments across multiple AWS accounts and Regions using continuous deployment automation. You can use EKS Blueprints to easily bootstrap an EKS cluster with Amazon EKS add-ons as well as a wide range of popular open-source add-ons, including Prometheus, Karpenter, Nginx, Traefik, AWS Load Balancer Controller, Fluent Bit, Keda, ArgoCD, and more. EKS Blueprints also helps you implement relevant security controls needed to operate workloads from multiple teams in the same cluster. What can I do with this QuickStart? \u00b6 Customers can use this Quick Start to easily architect and deploy a multi-tenant Blueprints built on EKS. Specifically, customers can leverage the eks-blueprints module to: Deploy Well-Architected EKS clusters across any number of accounts and regions. Manage cluster configuration, including add-ons that run in each cluster, from a single Git repository. Define teams, namespaces, and their associated access permissions for your clusters. Create Continuous Delivery (CD) pipelines that are responsible for deploying your infrastructure. Leverage GitOps-based workflows for onboarding and managing workloads for your teams. Examples \u00b6 To view a library of examples for how you can leverage the eks-blueprints , please see our Blueprints Patterns Repository . Workshop \u00b6 We maintain a hands-on self-paced workshop, the EKS Blueprints for CDK workshop helps you with foundational setup of your EKS cluster, and it gradually adds complexity via existing and new modules. To post feedback, submit feature ideas, or report bugs regarding the workshop, use the Issues section of this GitHub repo, and tag it with workshop .","title":"Overview"},{"location":"#overview","text":"Welcome to the Amazon EKS Blueprints Quick Start documentation site. This repository contains the source code for the eks-blueprints NPM module. It can be used by AWS customers, partners, and internal AWS teams to configure and manage complete EKS clusters that are fully bootstrapped with the operational software that is needed to deploy and operate workloads.","title":"Overview"},{"location":"#what-is-eks-blueprints","text":"EKS Blueprints helps you compose complete EKS clusters that are fully bootstrapped with the operational software that is needed to deploy and operate workloads. With EKS Blueprints, you describe the configuration for the desired state of your EKS environment, such as the control plane, worker nodes, and Kubernetes add-ons, as an IaC blueprint. Once a blueprint is configured, you can use it to stamp out consistent environments across multiple AWS accounts and Regions using continuous deployment automation. You can use EKS Blueprints to easily bootstrap an EKS cluster with Amazon EKS add-ons as well as a wide range of popular open-source add-ons, including Prometheus, Karpenter, Nginx, Traefik, AWS Load Balancer Controller, Fluent Bit, Keda, ArgoCD, and more. EKS Blueprints also helps you implement relevant security controls needed to operate workloads from multiple teams in the same cluster.","title":"What is EKS Blueprints?"},{"location":"#what-can-i-do-with-this-quickstart","text":"Customers can use this Quick Start to easily architect and deploy a multi-tenant Blueprints built on EKS. Specifically, customers can leverage the eks-blueprints module to: Deploy Well-Architected EKS clusters across any number of accounts and regions. Manage cluster configuration, including add-ons that run in each cluster, from a single Git repository. Define teams, namespaces, and their associated access permissions for your clusters. Create Continuous Delivery (CD) pipelines that are responsible for deploying your infrastructure. Leverage GitOps-based workflows for onboarding and managing workloads for your teams.","title":"What can I do with this QuickStart?"},{"location":"#examples","text":"To view a library of examples for how you can leverage the eks-blueprints , please see our Blueprints Patterns Repository .","title":"Examples"},{"location":"#workshop","text":"We maintain a hands-on self-paced workshop, the EKS Blueprints for CDK workshop helps you with foundational setup of your EKS cluster, and it gradually adds complexity via existing and new modules. To post feedback, submit feature ideas, or report bugs regarding the workshop, use the Issues section of this GitHub repo, and tag it with workshop .","title":"Workshop"},{"location":"cluster-management/","text":"Multi-Cluster Management \u00b6 Multi-cluster management refers to the strategies associated with managing and updating cluster configuration across many Amazon EKS clusters. Infrastructure as code (IaC) tools like the AWS CDK provides the ability to bring automation and consistency when deploying your clusters. You need the ability to apply the same configurations to as many of your clusters as necessary and by defining all of your resources via Infrastructure as Code (IaC), it removes the problem of having to generate or apply custom YAML files for each one of your clusters allowing your teams to move faster. Defining your clusters resources using the AWS CDK allows your teams to focus on the underlying workloads as the infrastructure components will be taken care of via the AWS CDK. The main benefits organizations can see using the AWS CDK to manage their Amazon EKS clusters can be summarized as follows: - Consistency across all clusters and environments - Streamlining access control across your organization - Ease of management for multiple clusters - Access to GitOps methodologies and best practices - Automated lifecycle management for cluster deployment The Amazon EKS Blueprints Quick Start references the eks-blueprints-patterns repository repository that includes examples of different deployment patterns and options which includes patterns for multi-cluster that can be deployed across multiple regions. If you take a look at the main.ts file in the patterns repository, you will notice that the stacks that define our Amazon EKS clusters and associated pipelines that are deployed to different regions as shown in the snippet below: #!/usr/bin/env node import * as cdk from 'aws-cdk-lib' ; const app = new cdk . App (); //------------------------------------------- // Single Cluster with multiple teams. //------------------------------------------- import MultiTeamConstruct from '../lib/multi-team-construct' new MultiTeamConstruct ( app , 'multi-team' ); //------------------------------------------- // Multiple clusters, multiple regions. //------------------------------------------- import MultiRegionConstruct from '../lib/multi-region-construct' new MultiRegionConstruct ( app , 'multi-region' ); //------------------------------------------- // Single Fargate cluster. //------------------------------------------- import FargateConstruct from '../lib/fargate-construct' new FargateConstruct ( app , 'fargate' ); //------------------------------------------- // Multiple clusters with deployment pipeline. //------------------------------------------- import PipelineStack from '../lib/pipeline-stack' const account = process . env . CDK_DEFAULT_ACCOUNT const region = process . env . CDK_DEFAULT_REGION const env = { account , region } new PipelineStack ( app , 'pipeline' , { env }); Using the AWS CDK, you can define the specific region to deploy your clusters using environment variables as a construct in Typescript as shown in the example above. If you were to deploy all the stacks in your main.ts file you would be able to view your running clusters by region by running the following command aws eks list-cluster --region <insert region> If for example you chose the region us-west-2, you would get a similar output: { \"clusters\" : [ \"all clusters in this region\" ] } Multi-Region Management \u00b6 In a production environment, it is common to have clusters that reside in different locations. This could be in different regions, on-prem, or follow a hybrid cloud model. Some of the common design patterns that come in to play when it comes to multi-cluster management across these different operational models include things like high availability, data replication, networking, traffic routing, and the underlying management of those clusters. In the eks-blueprints-patterns/lib/multi-region-construct directory, you will find the index.ts file which shows a concrete example of how to deploy multiple clusters to different regions as shown below import * as cdk from 'aws-cdk-lib' ; // Blueprints Lib import * as blueprints from '@aws-quickstart/eks-blueprints' // Team implementations import * as team from '../teams' export default class MultiRegionConstruct extends cdk . Construct { constructor ( scope : cdk.Construct , id : string ) { super ( scope , id ); // Setup platform team const accountID = process . env . CDK_DEFAULT_ACCOUNT ! const platformTeam = new team . TeamPlatform ( accountID ) const teams : Array < blueprints . Team > = [ platformTeam ]; // AddOns for the cluster. const addOns : Array < blueprints . ClusterAddOn > = [ new blueprints . addons . NginxAddOn , new blueprints . addons . ArgoCDAddOn , new blueprints . addons . CalicoAddOn , new blueprints . addons . MetricsServerAddOn , new blueprints . addons . ClusterAutoScalerAddOn , new blueprints . addons . ContainerInsightsAddOn , new blueprints . addons . VpcCniAddOn (), new blueprints . addons . CoreDnsAddOn (), new blueprints . addons . KubeProxyAddOn () ]; const east = 'blueprint-us-east-2' new blueprints . EksBlueprint ( scope , { id : ` ${ id } - ${ east } ` , addOns , teams }, { env : { region : east } }); const central = 'blueprint-us-central-2' new blueprints . EksBlueprint ( scope , { id : ` ${ id } - ${ central } ` , addOns , teams }, { env : { region : central } }); const west = 'blueprint-us-west-2' new blueprints . EksBlueprint ( scope , { id : ` ${ id } - ${ west } ` , addOns , teams }, { env : { region : west } }); } } This construct imports all of the core components of the EKS Blueprints framework, Teams construct, and Addons as modules which then deploys our clusters to different regions. The main point to take away from this is that we are adding automation and consistency to our clusters as we deploy multiple clusters to multiple regions since all of our components have already been defined in the EKS Blueprints library along with Teams and Addons.","title":"Multi-Cluster Management"},{"location":"cluster-management/#multi-cluster-management","text":"Multi-cluster management refers to the strategies associated with managing and updating cluster configuration across many Amazon EKS clusters. Infrastructure as code (IaC) tools like the AWS CDK provides the ability to bring automation and consistency when deploying your clusters. You need the ability to apply the same configurations to as many of your clusters as necessary and by defining all of your resources via Infrastructure as Code (IaC), it removes the problem of having to generate or apply custom YAML files for each one of your clusters allowing your teams to move faster. Defining your clusters resources using the AWS CDK allows your teams to focus on the underlying workloads as the infrastructure components will be taken care of via the AWS CDK. The main benefits organizations can see using the AWS CDK to manage their Amazon EKS clusters can be summarized as follows: - Consistency across all clusters and environments - Streamlining access control across your organization - Ease of management for multiple clusters - Access to GitOps methodologies and best practices - Automated lifecycle management for cluster deployment The Amazon EKS Blueprints Quick Start references the eks-blueprints-patterns repository repository that includes examples of different deployment patterns and options which includes patterns for multi-cluster that can be deployed across multiple regions. If you take a look at the main.ts file in the patterns repository, you will notice that the stacks that define our Amazon EKS clusters and associated pipelines that are deployed to different regions as shown in the snippet below: #!/usr/bin/env node import * as cdk from 'aws-cdk-lib' ; const app = new cdk . App (); //------------------------------------------- // Single Cluster with multiple teams. //------------------------------------------- import MultiTeamConstruct from '../lib/multi-team-construct' new MultiTeamConstruct ( app , 'multi-team' ); //------------------------------------------- // Multiple clusters, multiple regions. //------------------------------------------- import MultiRegionConstruct from '../lib/multi-region-construct' new MultiRegionConstruct ( app , 'multi-region' ); //------------------------------------------- // Single Fargate cluster. //------------------------------------------- import FargateConstruct from '../lib/fargate-construct' new FargateConstruct ( app , 'fargate' ); //------------------------------------------- // Multiple clusters with deployment pipeline. //------------------------------------------- import PipelineStack from '../lib/pipeline-stack' const account = process . env . CDK_DEFAULT_ACCOUNT const region = process . env . CDK_DEFAULT_REGION const env = { account , region } new PipelineStack ( app , 'pipeline' , { env }); Using the AWS CDK, you can define the specific region to deploy your clusters using environment variables as a construct in Typescript as shown in the example above. If you were to deploy all the stacks in your main.ts file you would be able to view your running clusters by region by running the following command aws eks list-cluster --region <insert region> If for example you chose the region us-west-2, you would get a similar output: { \"clusters\" : [ \"all clusters in this region\" ] }","title":"Multi-Cluster Management"},{"location":"cluster-management/#multi-region-management","text":"In a production environment, it is common to have clusters that reside in different locations. This could be in different regions, on-prem, or follow a hybrid cloud model. Some of the common design patterns that come in to play when it comes to multi-cluster management across these different operational models include things like high availability, data replication, networking, traffic routing, and the underlying management of those clusters. In the eks-blueprints-patterns/lib/multi-region-construct directory, you will find the index.ts file which shows a concrete example of how to deploy multiple clusters to different regions as shown below import * as cdk from 'aws-cdk-lib' ; // Blueprints Lib import * as blueprints from '@aws-quickstart/eks-blueprints' // Team implementations import * as team from '../teams' export default class MultiRegionConstruct extends cdk . Construct { constructor ( scope : cdk.Construct , id : string ) { super ( scope , id ); // Setup platform team const accountID = process . env . CDK_DEFAULT_ACCOUNT ! const platformTeam = new team . TeamPlatform ( accountID ) const teams : Array < blueprints . Team > = [ platformTeam ]; // AddOns for the cluster. const addOns : Array < blueprints . ClusterAddOn > = [ new blueprints . addons . NginxAddOn , new blueprints . addons . ArgoCDAddOn , new blueprints . addons . CalicoAddOn , new blueprints . addons . MetricsServerAddOn , new blueprints . addons . ClusterAutoScalerAddOn , new blueprints . addons . ContainerInsightsAddOn , new blueprints . addons . VpcCniAddOn (), new blueprints . addons . CoreDnsAddOn (), new blueprints . addons . KubeProxyAddOn () ]; const east = 'blueprint-us-east-2' new blueprints . EksBlueprint ( scope , { id : ` ${ id } - ${ east } ` , addOns , teams }, { env : { region : east } }); const central = 'blueprint-us-central-2' new blueprints . EksBlueprint ( scope , { id : ` ${ id } - ${ central } ` , addOns , teams }, { env : { region : central } }); const west = 'blueprint-us-west-2' new blueprints . EksBlueprint ( scope , { id : ` ${ id } - ${ west } ` , addOns , teams }, { env : { region : west } }); } } This construct imports all of the core components of the EKS Blueprints framework, Teams construct, and Addons as modules which then deploys our clusters to different regions. The main point to take away from this is that we are adding automation and consistency to our clusters as we deploy multiple clusters to multiple regions since all of our components have already been defined in the EKS Blueprints library along with Teams and Addons.","title":"Multi-Region Management"},{"location":"core-concepts/","text":"Core Concepts \u00b6 This document provides a high level overview of the Core Concepts that are embedded in the eks-blueprints framework. For the purposes of this document, we will assume the reader is familiar with Git, Docker, Kubernetes and AWS. Concept Description Blueprint A blueprint combines clusters , add-ons , and teams into a cohesive object that can deployed as a whole Cluster A Well-Architected EKS Cluster. Resource Provider Resource providers are abstractions that supply external AWS resources to the cluster (e.g. hosted zones, VPCs, etc.) Add-on Allow you to configure, deploy, and update the operational software, or add-ons, that provide key functionality to support your Kubernetes applications. Team A logical grouping of IAM identities that have access to a Kubernetes namespace(s). Pipeline Continuous Delivery pipelines for deploying clusters and add-ons . Application An application that runs within an EKS Cluster. Blueprint \u00b6 The eks-blueprints framework allows you to configure and deploy what we call blueprint clusters. A blueprint consists of an EKS cluster, a set of add-ons that will be deployed into the cluster, and a set of teams who will have access to a cluster. Once a blueprint is configured, it can be easily deployed across any number of AWS accounts and regions. Blueprints also leverage GitOps tooling to facilitate cluster bootstrapping and workload onboarding. To view sample blueprint implementations, please see our patterns repository . Cluster \u00b6 A cluster is simply an EKS cluster. The eks-blueprints framework provides for customizing the compute options you leverage with your clusters . The framework currently supports EC2 , Fargate and BottleRocket instances. To specify the type of compute you want to use for your cluster , you supply a ClusterProvider object to your blueprint . The framework defaults to leveraging the MngClusterProvider . Each ClusterProvider provides additional configuration options as well. For example, the MngClusterProvider allows you to configure instance types, min and max instance counts, and amiType, among other options. See our Cluster Providers documentation page for detailed information. Resource Provider \u00b6 A resource is a CDK construct that implements IResource interface from aws-cdk-lib which is a generic interface for any AWS resource. An example of a resource could be a hosted zone in Route53 IHostedZone , an ACM certificate ICertificate , a VPC or even a DynamoDB table which could be leveraged either in add-ons or teams. ResourceProviders enable customers to supply resources for add-ons, teams and/or post-deployment steps. Resources may be imported (e.g., if created outside of the platform) or created with the blueprint. See our Resource Providers documentation page for detailed information. Add-on \u00b6 Add-ons allow you to configure the tools and services that you would like to run in order to support your EKS workloads. When you configure add-ons for a blueprint , the add-ons will be provisioned at deploy time. Add-ons can deploy both Kubernetes specific resources and AWS resources needed to support add-on functionality. For example, the MetricsServerAddOn only deploys the Kubernetes manifests that are needed to run the Kubernetes Metrics Server (as a Helm chart). By contrast, the AWSLoadBalancerControllerAddon deploys Kubernetes resources, in addition to creating resources via AWS APIs that are needed to support the AWS Load Balancer Controller. The most common case to address via an add-on is configuration of IAM roles and permissions and the Kubernetes service account, leveraging IRSA to access AWS resources. See our Add-ons documentation page for detailed information. Team \u00b6 Teams allow you to configure the logical grouping of users that have access to your EKS clusters, in addition to the access permissions they are granted. The framework currently supports two types of teams : ApplicationTeam and PlatformTeam . ApplicationTeam members are granted access to specific namespaces. PlatformTeam members are granted administrative access to your clusters. See our Teams documentation page for detailed information. Pipeline \u00b6 Pipelines allow you to configure Continuous Delivery (CD) pipelines for your cluster blueprints that are directly integrated with your Git provider. See our Pipelines documentation page for detailed information. Application \u00b6 Applications represent the actual workloads that run within a Kubernetes cluster. The framework leverages a GitOps approach for deploying applications onto clusters. See our Workload Bootstrapping documentation for detailed information.","title":"Core Concepts"},{"location":"core-concepts/#core-concepts","text":"This document provides a high level overview of the Core Concepts that are embedded in the eks-blueprints framework. For the purposes of this document, we will assume the reader is familiar with Git, Docker, Kubernetes and AWS. Concept Description Blueprint A blueprint combines clusters , add-ons , and teams into a cohesive object that can deployed as a whole Cluster A Well-Architected EKS Cluster. Resource Provider Resource providers are abstractions that supply external AWS resources to the cluster (e.g. hosted zones, VPCs, etc.) Add-on Allow you to configure, deploy, and update the operational software, or add-ons, that provide key functionality to support your Kubernetes applications. Team A logical grouping of IAM identities that have access to a Kubernetes namespace(s). Pipeline Continuous Delivery pipelines for deploying clusters and add-ons . Application An application that runs within an EKS Cluster.","title":"Core Concepts"},{"location":"core-concepts/#blueprint","text":"The eks-blueprints framework allows you to configure and deploy what we call blueprint clusters. A blueprint consists of an EKS cluster, a set of add-ons that will be deployed into the cluster, and a set of teams who will have access to a cluster. Once a blueprint is configured, it can be easily deployed across any number of AWS accounts and regions. Blueprints also leverage GitOps tooling to facilitate cluster bootstrapping and workload onboarding. To view sample blueprint implementations, please see our patterns repository .","title":"Blueprint"},{"location":"core-concepts/#cluster","text":"A cluster is simply an EKS cluster. The eks-blueprints framework provides for customizing the compute options you leverage with your clusters . The framework currently supports EC2 , Fargate and BottleRocket instances. To specify the type of compute you want to use for your cluster , you supply a ClusterProvider object to your blueprint . The framework defaults to leveraging the MngClusterProvider . Each ClusterProvider provides additional configuration options as well. For example, the MngClusterProvider allows you to configure instance types, min and max instance counts, and amiType, among other options. See our Cluster Providers documentation page for detailed information.","title":"Cluster"},{"location":"core-concepts/#resource-provider","text":"A resource is a CDK construct that implements IResource interface from aws-cdk-lib which is a generic interface for any AWS resource. An example of a resource could be a hosted zone in Route53 IHostedZone , an ACM certificate ICertificate , a VPC or even a DynamoDB table which could be leveraged either in add-ons or teams. ResourceProviders enable customers to supply resources for add-ons, teams and/or post-deployment steps. Resources may be imported (e.g., if created outside of the platform) or created with the blueprint. See our Resource Providers documentation page for detailed information.","title":"Resource Provider"},{"location":"core-concepts/#add-on","text":"Add-ons allow you to configure the tools and services that you would like to run in order to support your EKS workloads. When you configure add-ons for a blueprint , the add-ons will be provisioned at deploy time. Add-ons can deploy both Kubernetes specific resources and AWS resources needed to support add-on functionality. For example, the MetricsServerAddOn only deploys the Kubernetes manifests that are needed to run the Kubernetes Metrics Server (as a Helm chart). By contrast, the AWSLoadBalancerControllerAddon deploys Kubernetes resources, in addition to creating resources via AWS APIs that are needed to support the AWS Load Balancer Controller. The most common case to address via an add-on is configuration of IAM roles and permissions and the Kubernetes service account, leveraging IRSA to access AWS resources. See our Add-ons documentation page for detailed information.","title":"Add-on"},{"location":"core-concepts/#team","text":"Teams allow you to configure the logical grouping of users that have access to your EKS clusters, in addition to the access permissions they are granted. The framework currently supports two types of teams : ApplicationTeam and PlatformTeam . ApplicationTeam members are granted access to specific namespaces. PlatformTeam members are granted administrative access to your clusters. See our Teams documentation page for detailed information.","title":"Team"},{"location":"core-concepts/#pipeline","text":"Pipelines allow you to configure Continuous Delivery (CD) pipelines for your cluster blueprints that are directly integrated with your Git provider. See our Pipelines documentation page for detailed information.","title":"Pipeline"},{"location":"core-concepts/#application","text":"Applications represent the actual workloads that run within a Kubernetes cluster. The framework leverages a GitOps approach for deploying applications onto clusters. See our Workload Bootstrapping documentation for detailed information.","title":"Application"},{"location":"extensibility/","text":"Extensibility \u00b6 This guide provides an overview of extensibility options focusing on add-on extensions as the primary mechanism for the partners and customers. Overview \u00b6 Blueprints Framework is designed to be extensible. In the context of this guide, extensibility refers to the ability of customers and partners to both add new capabilities to the framework or platforms based on Blueprints as well as customize existing behavior, including the ability to modify or override existing behavior. The following abstractions can be leveraged to add new features to the framework: Add-on . Customers and partners can implement new add-ons which could be leveraged exactly the same way as the core add-ons (supplied by the framework). Resource Provider . This construct allows customers to create resources that can be reused across multiple add-ons and/or teams. For example, IAM roles, VPC, hosted zone. Cluster Provider . This construct allows creation of custom code that provisions an EKS cluster with node groups. It can be leveraged to extend behavior such as control plane customization, custom settings for node groups. Team . This abstraction allows to create team templates for application and platform teams and set custom setting for network isolation, policies (network, security), software wiring (auto injection of proxies, team specific service mesh configuration) and other extensions pertinent to the teams. Add-on Extensions \u00b6 In a general case, implementation of an add-on is a class which implements the ClusterAddOn interface. export declare interface ClusterAddOn { id? : string ; deploy ( clusterInfo : types.ClusterInfo ) : Promise < Construct > | void ; } Note : The add-on implementation can optionally supply the id attribute if the target add-on can be added to a blueprint more than once. Implementation of the add-on is expected to be an exported class that implements the interface and supplies the implementation of the deploy method. In order for the add-on to receive the deployment contextual information about the provisioned cluster, region, resource providers and/or other add-ons, the deploy method takes the ClusterInfo parameter (see types ), which represents a structure defined in the SPI (service provider interface) contracts. The API for the cluster info structure is stable and provides access to the provisioned EKS cluster, scheduled add-ons (that have not been installed yet but are part of the blueprint) or provisioned add-ons and other contexts. Post Deployment Hooks \u00b6 In certain cases, add-on provisioning may require logic to be executed after provisioning of the add-ons (and teams) is complete. For such cases, add-on can optionally implement ClusterPostDeploy interface. /** * Optional interface to allow cluster bootstrapping after provisioning of add-ons and teams is complete. * Can be leveraged to bootstrap workloads, perform cluster checks. * ClusterAddOn implementation may implement this interface in order to get post deployment hook point. */ export declare interface ClusterPostDeploy { postDeploy ( clusterInfo : types.ClusterInfo , teams : Team []) : void ; } This capability is leveraged for example in ArgoCD add-on to bootstrap workloads after all add-ons finished provisioning. Note, in this case unlike the standard deploy method implementation, the add-on also gets access to the provisioned teams. Helm Add-ons \u00b6 Helm add-ons are the most common case that generally combines provisioning of a helm chart as well as supporting infrastructure such as wiring of proper IAM policies for the Kubernetes service account, provisioning or configuring other AWS resources (VPC, subnets, node groups). In order to provide consistency across all Helm add-ons supplied by the Blueprints framework all Helm add-ons are implemented as derivatives of the HelmAddOn base class and support properties based on HelmAddOnUserProps . See the example extension section below for more details. Use cases that are enabled by leveraging the base HelmAddOn class: Consistency across all helm based add-on will reduce effort to understand how to apply and configure standard add-on options. Ability to override helm chart repository can enable leveraging private helm chart repository by the customer and facilitate add-on usage for private EKS clusters. Extensibility mechanisms available in the Blueprints framework can allow to intercept helm deployments and leverage GitOps driven add-on configuration. Non-helm Add-ons \u00b6 Add-ons that don't leverage helm but require to install arbitrary Kubernetes manifests will not be able to leverage the benefits provided by the HelmAddOn however, they are still relatively easy to implement. Deployment of arbitrary kubernetes manifests can leverage the following construct: import { KubernetesManifest } from \"aws-cdk-lib/aws-eks\" ; import { ClusterAddOn , ClusterInfo } from \"../../spi\" ; import { loadYaml , readYamlDocument } from \"../../utils/yaml-utils\" ; export class MyNonHelmAddOn implements ClusterAddOn { deploy ( clusterInfo : ClusterInfo ) : void { const cluster = clusterInfo . cluster ; // Apply manifest const doc = readYamlDocument ( __dirname + '/my-product.yaml' ); // ... apply any substitutions for dynamic values const manifest = docArray . split ( \"---\" ). map ( e => loadYaml ( e )); new KubernetesManifest ( cluster . stack , \"myproduct-manifest\" , { cluster , manifest , overwrite : true }); } } Note: When leveraging this approach consider how customers can apply the add-on for fully private clusters. It may be reasonable to bundle the manifest with the add-on in the npm package. Add-on Dependencies \u00b6 Add-ons can depend on other add-ons and that dependency may be soft or hard. Hard dependency implies that add-on provisioning must fail if the dependency is not available. For example, if an add-on requires access to AWS Secrets Manager for a secret containing a license key, credentials or other sensitive information, it can declare dependency on the CSI Secret Store Driver. Dependency management for direct hard dependency are implemented using a decorator @dependable . Example: import { Construct } from \"constructs\" ; import { ClusterInfo } from \"../../spi\" ; import { dependable } from \"../../utils\" ; import { HelmAddOn , HelmAddOnUserProps } from \"../helm-addon\" ; export class MyProductAddOn extends HelmAddOn { readonly options : MyProductAddOnProps ; // extends HelmAddOnUserProps ... @dependable ( 'AwsLoadBalancerControllerAddOn' ) // depends on AwsLoadBalancerController deploy ( clusterInfo : ClusterInfo ) : Promise < Construct > { ... } Passing Secrets to Add-ons \u00b6 Secrets from the AWS Secrets Manager or AWS Systems Manager Parameter Store can be made available as files mounted in Amazon EKS pods. It can be achieved with the help of AWS Secrets and Configuration Provider (ASCP) for the Kubernetes Secrets Store CSI Driver . The ASCP works with Amazon Elastic Kubernetes Service (Amazon EKS) 1.17+. More information on general concepts for leveraging ASCP can be found here . Blueprints Framework provides support for both Secrets Store CSI Driver as well as ASCP with the Secrets Store Add-on . Add-ons requiring support for secrets can declare dependency on the secret store add-on: export class MyAddOn extends blueprints . addons . HelmAddOn { ... // Declares dependency on secret store add-on if secrets are needed. // Customers will have to explicitly add this add-on to the blueprint. @blueprints . utils . dependable ( blueprints . SecretsStoreAddOn . name ) deploy ( clusterInfo : blueprints.ClusterInfo ) : Promise < Construct > { ... } In order to propagate the secret from the Secrets Manager to the Kubernetes cluster, the add-on should create a SecretProviderClass Kubernetes object. leveraging the blueprints.addons.SecretProviderClass . The framework will take care of wiring the Kubernetes service account with the correct IAM permissions to pull the secret: const sa = clusterInfo . cluster . addServiceAccount (...); const csiSecret : blueprints.addons.CsiSecretProps = { secretProvider : new blueprints . LookupSecretsManagerSecretByName ( licenseKeySecret ), // the secret must be defined upfront and available in the region with the name specified in the licenseKeySecret kubernetesSecret : { secretName : 'my-addon-license-secret' , data : [ { key : 'licenseKey' } ] } }; const secretProviderClass = new blueprints . addons . SecretProviderClass ( clusterInfo , sa , \"my-addon-license-secret-class\" , csiSecret ); Note: you can also leverage LookupSecretsManagerSecretByArn , LookupSsmSecretByAttrs or a custom implementation of the secret provider interface blueprints.addons.SecretProvider . After the secret provider class is created, it should be mounted on any pod in the namespace to make the secret accessible. Mounting the secret volume also creates a regular Kubernetes Secret object based on the supplied description ( my-addon-license-secret ). This capability is controlled by the configuration of the Blueprints Secret Store add-on and is enabled by default. Many Helm charts provide options to mount additional volumes and mounts to the provisioned product. For example, a Helm chart (ArgoCD, FluentBit) allows specifying volumes and volumeMounts as the helm chart values. Mounting the secret in such cases is simple and does not require an additional pod for secrets. Here is an example of a secret volume and volume mount passed as values to a Helm chart: const chart = this . addHelmChart ( clusterInfo , { ... // standard values , volumes : [ { name : \"secrets-store-inline\" , csi : { driver : \"secrets-store.csi.k8s.io\" , readOnly : true , volumeAttributes : { secretProviderClass : \"my-addon-license-secret-class\" } } } ], volumeMounts : [ { name : \"secrets-store-inline\" , mountPath : \"/mnt/secret-store\" } ] }); After the secret volume is mounted (on any pod), you will see that a Kubernetes secret (for example my-addon-license-secret ) is also created in the target namespace. See the supplied code example for more details. Private Extensions \u00b6 Extensions specific to a customer instance of Blueprints can be implemented inline with the blueprint in the same codebase. Such extensions are scoped to the customer base and cannot be reused. Example of a private extension: class MyAddOn extends HelmAddOn { constructor () { super ({ chart : 'mychart' , ... }); } deploy ( clusterInfo : blueprints.ClusterInfo ) : Promise < Construct > { return Promise . resolve ( this . addHelmChart ( clusterInfo , {})); } } blueprints . EksBlueprint . builder () . addOns ( new MyAddOn ()) . build ( app , 'my-extension-test-blueprint' ); Public Extensions \u00b6 The life-cycle of a public extension should be decoupled from the life-cycle of the EKS Blueprints main repository . When decoupled, extensions can be released at any arbitrary cadence specific to the extension, enabling better agility when it comes to new features or bug fixes. In order to enable this model the following workflow outline steps required to create and release a public extension: Public extensions are created in a separate repository. Public GitHub repository is preferred as it aligns with the open-source spirit of the framework and enables external reviews/feedback. Extensions are released and consumed as distinct public NPM packages. Public Extensions are expected to have sufficient documentation to allow customers to consume them independently. Documentation can reside in GitHub or external resources referenced in the documentation bundled with the extension. Public extensions are expected to be tested and validated against released Blueprints versions, e.g. with a CICD pipeline. Pipeline can be created with the pipelines support from the Blueprints framework or leveraging customer/partner specific tools. Partner Extensions \u00b6 Partner extensions (APN Partner) are expected to comply with the public extension workflow and additional items required to ensure proper validation and documentation support for a partner extension. Documentation PR should be created to the main Blueprints Quickstart repository to update the AddOns section. Example of add-on documentation can be found here along with the list of other add-ons. An example that shows a ready to use pattern leveraging the add-on should be submitted to the Blueprints Patterns Repository . This step will enable AWS PSAs to validate the add-on as well as provide a ready to use pattern to the customers, that could be copied/cloned in their Blueprints implementation. Example Extension \u00b6 Example extension contains a sample implementation of a FluentBit log forwarder add-on and covers the following aspects of an extension workflow: Pre-requisite configuration related to nodejs, npm, typescript. Project template with support to build, test and run the extension. Example blueprint (can be found in ./bin/main.ts) that references the add-on. Example of configuring a Kubernetes service account with IRSA (IAM roles for service accounts) and required IAM policies. Example of the helm chart provisioning. Example of passing secret values to the add-on (such as credentials and/or licenseKeys) by leveraging CSI Secret Store Driver. Outlines support to build, package and publish the add-on in an NPM repository.","title":"Extensibility"},{"location":"extensibility/#extensibility","text":"This guide provides an overview of extensibility options focusing on add-on extensions as the primary mechanism for the partners and customers.","title":"Extensibility"},{"location":"extensibility/#overview","text":"Blueprints Framework is designed to be extensible. In the context of this guide, extensibility refers to the ability of customers and partners to both add new capabilities to the framework or platforms based on Blueprints as well as customize existing behavior, including the ability to modify or override existing behavior. The following abstractions can be leveraged to add new features to the framework: Add-on . Customers and partners can implement new add-ons which could be leveraged exactly the same way as the core add-ons (supplied by the framework). Resource Provider . This construct allows customers to create resources that can be reused across multiple add-ons and/or teams. For example, IAM roles, VPC, hosted zone. Cluster Provider . This construct allows creation of custom code that provisions an EKS cluster with node groups. It can be leveraged to extend behavior such as control plane customization, custom settings for node groups. Team . This abstraction allows to create team templates for application and platform teams and set custom setting for network isolation, policies (network, security), software wiring (auto injection of proxies, team specific service mesh configuration) and other extensions pertinent to the teams.","title":"Overview"},{"location":"extensibility/#add-on-extensions","text":"In a general case, implementation of an add-on is a class which implements the ClusterAddOn interface. export declare interface ClusterAddOn { id? : string ; deploy ( clusterInfo : types.ClusterInfo ) : Promise < Construct > | void ; } Note : The add-on implementation can optionally supply the id attribute if the target add-on can be added to a blueprint more than once. Implementation of the add-on is expected to be an exported class that implements the interface and supplies the implementation of the deploy method. In order for the add-on to receive the deployment contextual information about the provisioned cluster, region, resource providers and/or other add-ons, the deploy method takes the ClusterInfo parameter (see types ), which represents a structure defined in the SPI (service provider interface) contracts. The API for the cluster info structure is stable and provides access to the provisioned EKS cluster, scheduled add-ons (that have not been installed yet but are part of the blueprint) or provisioned add-ons and other contexts.","title":"Add-on Extensions"},{"location":"extensibility/#post-deployment-hooks","text":"In certain cases, add-on provisioning may require logic to be executed after provisioning of the add-ons (and teams) is complete. For such cases, add-on can optionally implement ClusterPostDeploy interface. /** * Optional interface to allow cluster bootstrapping after provisioning of add-ons and teams is complete. * Can be leveraged to bootstrap workloads, perform cluster checks. * ClusterAddOn implementation may implement this interface in order to get post deployment hook point. */ export declare interface ClusterPostDeploy { postDeploy ( clusterInfo : types.ClusterInfo , teams : Team []) : void ; } This capability is leveraged for example in ArgoCD add-on to bootstrap workloads after all add-ons finished provisioning. Note, in this case unlike the standard deploy method implementation, the add-on also gets access to the provisioned teams.","title":"Post Deployment Hooks"},{"location":"extensibility/#helm-add-ons","text":"Helm add-ons are the most common case that generally combines provisioning of a helm chart as well as supporting infrastructure such as wiring of proper IAM policies for the Kubernetes service account, provisioning or configuring other AWS resources (VPC, subnets, node groups). In order to provide consistency across all Helm add-ons supplied by the Blueprints framework all Helm add-ons are implemented as derivatives of the HelmAddOn base class and support properties based on HelmAddOnUserProps . See the example extension section below for more details. Use cases that are enabled by leveraging the base HelmAddOn class: Consistency across all helm based add-on will reduce effort to understand how to apply and configure standard add-on options. Ability to override helm chart repository can enable leveraging private helm chart repository by the customer and facilitate add-on usage for private EKS clusters. Extensibility mechanisms available in the Blueprints framework can allow to intercept helm deployments and leverage GitOps driven add-on configuration.","title":"Helm Add-ons"},{"location":"extensibility/#non-helm-add-ons","text":"Add-ons that don't leverage helm but require to install arbitrary Kubernetes manifests will not be able to leverage the benefits provided by the HelmAddOn however, they are still relatively easy to implement. Deployment of arbitrary kubernetes manifests can leverage the following construct: import { KubernetesManifest } from \"aws-cdk-lib/aws-eks\" ; import { ClusterAddOn , ClusterInfo } from \"../../spi\" ; import { loadYaml , readYamlDocument } from \"../../utils/yaml-utils\" ; export class MyNonHelmAddOn implements ClusterAddOn { deploy ( clusterInfo : ClusterInfo ) : void { const cluster = clusterInfo . cluster ; // Apply manifest const doc = readYamlDocument ( __dirname + '/my-product.yaml' ); // ... apply any substitutions for dynamic values const manifest = docArray . split ( \"---\" ). map ( e => loadYaml ( e )); new KubernetesManifest ( cluster . stack , \"myproduct-manifest\" , { cluster , manifest , overwrite : true }); } } Note: When leveraging this approach consider how customers can apply the add-on for fully private clusters. It may be reasonable to bundle the manifest with the add-on in the npm package.","title":"Non-helm Add-ons"},{"location":"extensibility/#add-on-dependencies","text":"Add-ons can depend on other add-ons and that dependency may be soft or hard. Hard dependency implies that add-on provisioning must fail if the dependency is not available. For example, if an add-on requires access to AWS Secrets Manager for a secret containing a license key, credentials or other sensitive information, it can declare dependency on the CSI Secret Store Driver. Dependency management for direct hard dependency are implemented using a decorator @dependable . Example: import { Construct } from \"constructs\" ; import { ClusterInfo } from \"../../spi\" ; import { dependable } from \"../../utils\" ; import { HelmAddOn , HelmAddOnUserProps } from \"../helm-addon\" ; export class MyProductAddOn extends HelmAddOn { readonly options : MyProductAddOnProps ; // extends HelmAddOnUserProps ... @dependable ( 'AwsLoadBalancerControllerAddOn' ) // depends on AwsLoadBalancerController deploy ( clusterInfo : ClusterInfo ) : Promise < Construct > { ... }","title":"Add-on Dependencies"},{"location":"extensibility/#passing-secrets-to-add-ons","text":"Secrets from the AWS Secrets Manager or AWS Systems Manager Parameter Store can be made available as files mounted in Amazon EKS pods. It can be achieved with the help of AWS Secrets and Configuration Provider (ASCP) for the Kubernetes Secrets Store CSI Driver . The ASCP works with Amazon Elastic Kubernetes Service (Amazon EKS) 1.17+. More information on general concepts for leveraging ASCP can be found here . Blueprints Framework provides support for both Secrets Store CSI Driver as well as ASCP with the Secrets Store Add-on . Add-ons requiring support for secrets can declare dependency on the secret store add-on: export class MyAddOn extends blueprints . addons . HelmAddOn { ... // Declares dependency on secret store add-on if secrets are needed. // Customers will have to explicitly add this add-on to the blueprint. @blueprints . utils . dependable ( blueprints . SecretsStoreAddOn . name ) deploy ( clusterInfo : blueprints.ClusterInfo ) : Promise < Construct > { ... } In order to propagate the secret from the Secrets Manager to the Kubernetes cluster, the add-on should create a SecretProviderClass Kubernetes object. leveraging the blueprints.addons.SecretProviderClass . The framework will take care of wiring the Kubernetes service account with the correct IAM permissions to pull the secret: const sa = clusterInfo . cluster . addServiceAccount (...); const csiSecret : blueprints.addons.CsiSecretProps = { secretProvider : new blueprints . LookupSecretsManagerSecretByName ( licenseKeySecret ), // the secret must be defined upfront and available in the region with the name specified in the licenseKeySecret kubernetesSecret : { secretName : 'my-addon-license-secret' , data : [ { key : 'licenseKey' } ] } }; const secretProviderClass = new blueprints . addons . SecretProviderClass ( clusterInfo , sa , \"my-addon-license-secret-class\" , csiSecret ); Note: you can also leverage LookupSecretsManagerSecretByArn , LookupSsmSecretByAttrs or a custom implementation of the secret provider interface blueprints.addons.SecretProvider . After the secret provider class is created, it should be mounted on any pod in the namespace to make the secret accessible. Mounting the secret volume also creates a regular Kubernetes Secret object based on the supplied description ( my-addon-license-secret ). This capability is controlled by the configuration of the Blueprints Secret Store add-on and is enabled by default. Many Helm charts provide options to mount additional volumes and mounts to the provisioned product. For example, a Helm chart (ArgoCD, FluentBit) allows specifying volumes and volumeMounts as the helm chart values. Mounting the secret in such cases is simple and does not require an additional pod for secrets. Here is an example of a secret volume and volume mount passed as values to a Helm chart: const chart = this . addHelmChart ( clusterInfo , { ... // standard values , volumes : [ { name : \"secrets-store-inline\" , csi : { driver : \"secrets-store.csi.k8s.io\" , readOnly : true , volumeAttributes : { secretProviderClass : \"my-addon-license-secret-class\" } } } ], volumeMounts : [ { name : \"secrets-store-inline\" , mountPath : \"/mnt/secret-store\" } ] }); After the secret volume is mounted (on any pod), you will see that a Kubernetes secret (for example my-addon-license-secret ) is also created in the target namespace. See the supplied code example for more details.","title":"Passing Secrets to Add-ons"},{"location":"extensibility/#private-extensions","text":"Extensions specific to a customer instance of Blueprints can be implemented inline with the blueprint in the same codebase. Such extensions are scoped to the customer base and cannot be reused. Example of a private extension: class MyAddOn extends HelmAddOn { constructor () { super ({ chart : 'mychart' , ... }); } deploy ( clusterInfo : blueprints.ClusterInfo ) : Promise < Construct > { return Promise . resolve ( this . addHelmChart ( clusterInfo , {})); } } blueprints . EksBlueprint . builder () . addOns ( new MyAddOn ()) . build ( app , 'my-extension-test-blueprint' );","title":"Private Extensions"},{"location":"extensibility/#public-extensions","text":"The life-cycle of a public extension should be decoupled from the life-cycle of the EKS Blueprints main repository . When decoupled, extensions can be released at any arbitrary cadence specific to the extension, enabling better agility when it comes to new features or bug fixes. In order to enable this model the following workflow outline steps required to create and release a public extension: Public extensions are created in a separate repository. Public GitHub repository is preferred as it aligns with the open-source spirit of the framework and enables external reviews/feedback. Extensions are released and consumed as distinct public NPM packages. Public Extensions are expected to have sufficient documentation to allow customers to consume them independently. Documentation can reside in GitHub or external resources referenced in the documentation bundled with the extension. Public extensions are expected to be tested and validated against released Blueprints versions, e.g. with a CICD pipeline. Pipeline can be created with the pipelines support from the Blueprints framework or leveraging customer/partner specific tools.","title":"Public Extensions"},{"location":"extensibility/#partner-extensions","text":"Partner extensions (APN Partner) are expected to comply with the public extension workflow and additional items required to ensure proper validation and documentation support for a partner extension. Documentation PR should be created to the main Blueprints Quickstart repository to update the AddOns section. Example of add-on documentation can be found here along with the list of other add-ons. An example that shows a ready to use pattern leveraging the add-on should be submitted to the Blueprints Patterns Repository . This step will enable AWS PSAs to validate the add-on as well as provide a ready to use pattern to the customers, that could be copied/cloned in their Blueprints implementation.","title":"Partner Extensions"},{"location":"extensibility/#example-extension","text":"Example extension contains a sample implementation of a FluentBit log forwarder add-on and covers the following aspects of an extension workflow: Pre-requisite configuration related to nodejs, npm, typescript. Project template with support to build, test and run the extension. Example blueprint (can be found in ./bin/main.ts) that references the add-on. Example of configuring a Kubernetes service account with IRSA (IAM roles for service accounts) and required IAM policies. Example of the helm chart provisioning. Example of passing secret values to the add-on (such as credentials and/or licenseKeys) by leveraging CSI Secret Store Driver. Outlines support to build, package and publish the add-on in an NPM repository.","title":"Example Extension"},{"location":"getting-started/","text":"Getting Started \u00b6 This getting started guide will walk you through setting up a new CDK project which leverages the eks-blueprints NPM module to deploy a simple Blueprints. Project Setup \u00b6 Before proceeding, make sure AWS CLI is installed on your machine. To use the eks-blueprints module, you must have Node.js and npm installed. We will also use make to simplify build and other common actions. You can do it using the following instructions: Mac brew install make brew install node Ubuntu sudo apt install make sudo apt install nodejs Create a directory that represents you project (e.g. my-blueprints ) and then create a new typescript CDK project in that directory. npm install -g n # may require sudo n stable # may require sudo npm install -g aws-cdk@2.72.0 # may require sudo (Ubuntu) depending on configuration cdk --version # must produce 2.72.0 mkdir my-blueprints cd my-blueprints cdk init app --language typescript Configure and Deploy EKS Clusters \u00b6 Install the eks-blueprints NPM package via the following. npm i @aws-quickstart/eks-blueprints Replace the contents of bin/<your-main-file>.ts (where your-main-file by default is the name of the root project directory) with the following code. This code will deploy a new EKS Cluster and install the ArgoCD addon. import 'source-map-support/register' ; import * as cdk from 'aws-cdk-lib' ; import * as blueprints from '@aws-quickstart/eks-blueprints' ; const app = new cdk . App (); const account = 'XXXXXXXXXXXXX' ; const region = 'us-east-2' ; const addOns : Array < blueprints . ClusterAddOn > = [ new blueprints . addons . ArgoCDAddOn (), new blueprints . addons . CalicoOperatorAddOn (), new blueprints . addons . MetricsServerAddOn (), new blueprints . addons . ClusterAutoScalerAddOn (), new blueprints . addons . AwsLoadBalancerControllerAddOn (), new blueprints . addons . VpcCniAddOn (), new blueprints . addons . CoreDnsAddOn (), new blueprints . addons . KubeProxyAddOn () ]; const stack = blueprints . EksBlueprint . builder () . account ( account ) . region ( region ) . addOns (... addOns ) . useDefaultSecretEncryption ( true ) // set to false to turn secret encryption off (non-production/demo cases) . build ( app , 'eks-blueprint' ); Each combination of target account and region must be bootstrapped prior to deploying stacks. Bootstrapping is a process of creating IAM roles and lambda functions that can execute some of the common CDK constructs. For application of the EKS Blueprints Framework with AWS Organizations , Multi-account framework and Control Tower consider a process to automatically or manually CDK-bootstrapping new (workload/environment) accounts when they are added to the organization. More info on account bootstrapping here . Bootstrap your environment with the following command. cdk bootstrap Note: if the account/region combination used in the code example above is different from the initial combination used with cdk bootstrap , you will need to perform cdk bootstrap again to avoid error. Please reference CDK usage doc for detail. Deploy the stack using the following command. This command will take roughly 20 minutes to complete. cdk deploy Congratulations! You have deployed your first EKS cluster with eks-blueprints . The above code will provision the following: A new Well-Architected VPC with both Public and Private subnets. A new Well-Architected EKS cluster in the region and account you specify. Nginx into your cluster to serve as a reverse proxy for your workloads. ArgoCD into your cluster to support GitOps deployments. Calico into your cluster to support Network policies. Metrics Server into your cluster to support metrics collection. AWS and Kubernetes resources needed to support Cluster Autoscaler . AWS and Kubernetes resources needed to forward logs and metrics to Container Insights . AWS and Kubernetes resources needed to support AWS Load Balancer Controller . Amazon VPC CNI add-on (VpcCni) into your cluster to support native VPC networking for Amazon EKS. CoreDNS Amazon EKS add-on (CoreDns) into your cluster. CoreDns is a flexible, extensible DNS server that can serve as the Kubernetes cluster DNS kube-proxy Amazon EKS add-on (KubeProxy) into your cluster to maintains network rules on each Amazon EC2 node AWS and Kubernetes resources needed to support AWS X-Ray . Cluster Access \u00b6 Once the deploy completes, you will see output in your terminal window similar to the following: Outputs: east-test-1.easttest1ClusterName8D8E5E5E = east-test-1 east-test-1.easttest1ConfigCommand25ABB520 = aws eks update-kubeconfig --name east-test-1 --region us-east-1 --role-arn <ROLE_ARN> east-test-1.easttest1GetTokenCommand337FE3DD = aws eks get-token --cluster-name east-test-1 --region us-east-1 --role-arn <ROLE_ARN> Stack ARN: arn:aws:cloudformation:us-east-1:115717706081:stack/east-test-1/e1b9e6a0-d5f6-11eb-8498-0a374cd00e27 To update your Kubernetes config for you new cluster, copy and run the east-test-1.easttest1ConfigCommand25ABB520 command (the second command) in your terminal. aws eks update-kubeconfig --name east-test-1 --region us-east-1 --role-arn <ROLE_ARN> Validate that you now have kubectl access to your cluster via the following: kubectl get namespace You should see output that lists all namespaces in your cluster. Deploy workloads with ArgoCD \u00b6 Next, let's walk you through how to deploy workloads to your cluster with ArgoCD. This approach leverages the App of Apps pattern to deploy multiple workloads across multiple namespaces. The sample app of apps repository that we use in this getting started guide can be found here . You can leverage Automatic Bootstrapping for automatic onboarding of workloads. This feature may be leveraged even when workload repositories are not ready yet, as it creates a placeholder for future workloads and decouples workload onboarding for the infrastructure provisioning pipeline. The next steps, described in this guide apply for cases when customer prefer to bootstrap their workloads manually through ArgoCD UI console. Install ArgoCD CLI \u00b6 These steps are needed for manual workload onboarding. For automatic bootstrapping please refer to the Automatic Bootstrapping . Follow the instructions found here as it will include instructions for your specific OS. You can test that the ArgoCD CLI was installed correctly using the following: argocd version --short --client You should see output similar to the following: argocd: v2.0.1+33eaf11.dirty Exposing ArgoCD \u00b6 To access ArgoCD running in your Kubernetes cluster, we can leverage Kubernetes Port Forwarding . To do so, first capture the ArgoCD service name in an environment variable. export ARGO_SERVER=$(kubectl get svc -n argocd -l app.kubernetes.io/name=argocd-server -o name) Next, in a new terminal tab, expose the service locally. kubectl port-forward $ARGO_SERVER -n argocd 8080:443 Open your browser to http://localhost:8080 and you should see the ArgoCD login screen. Logging Into ArgoCD \u00b6 ArgoCD will create an admin user and password on a fresh install. To get the ArgoCD admin password, run the following. export ARGO_PASSWORD=$(kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath=\"{.data.password}\" | base64 -d) While still port-forwarding, login via the following. argocd login localhost:8080 --username admin --password $ARGO_PASSWORD You can also login to the ArgoCD UI with generated password and the username admin . echo $ARGO_PASSWORD Deploy workloads to your cluster \u00b6 Create a project in Argo by running the following command argocd proj create sample \\ -d https://kubernetes.default.svc,argocd \\ -s https://github.com/aws-samples/eks-blueprints-workloads.git Create the application within Argo by running the following command argocd app create dev-apps \\ --dest-namespace argocd \\ --dest-server https://kubernetes.default.svc \\ --repo https://github.com/aws-samples/eks-blueprints-workloads.git \\ --path \"envs/dev\" Sync the apps by running the following command argocd app sync dev-apps Validate deployments. \u00b6 To validate your deployments, leverage kubectl port-forwarding to access the guestbook-ui service for team-riker . kubectl port-forward svc/guestbook-ui -n team-riker 4040:80 Open up localhost:4040 in your browser and you should see the application. Next Steps \u00b6 For information on onboarding teams to your clusters, see Team documentation . For information on deploying Continuous Delivery pipelines for your infrastructure, see Pipelines documentation . For information on supported add-ons, see Add-on documentation For information on Onboarding and managing workloads in your clusters, see Workload documentation .","title":"Getting Started"},{"location":"getting-started/#getting-started","text":"This getting started guide will walk you through setting up a new CDK project which leverages the eks-blueprints NPM module to deploy a simple Blueprints.","title":"Getting Started"},{"location":"getting-started/#project-setup","text":"Before proceeding, make sure AWS CLI is installed on your machine. To use the eks-blueprints module, you must have Node.js and npm installed. We will also use make to simplify build and other common actions. You can do it using the following instructions: Mac brew install make brew install node Ubuntu sudo apt install make sudo apt install nodejs Create a directory that represents you project (e.g. my-blueprints ) and then create a new typescript CDK project in that directory. npm install -g n # may require sudo n stable # may require sudo npm install -g aws-cdk@2.72.0 # may require sudo (Ubuntu) depending on configuration cdk --version # must produce 2.72.0 mkdir my-blueprints cd my-blueprints cdk init app --language typescript","title":"Project Setup"},{"location":"getting-started/#configure-and-deploy-eks-clusters","text":"Install the eks-blueprints NPM package via the following. npm i @aws-quickstart/eks-blueprints Replace the contents of bin/<your-main-file>.ts (where your-main-file by default is the name of the root project directory) with the following code. This code will deploy a new EKS Cluster and install the ArgoCD addon. import 'source-map-support/register' ; import * as cdk from 'aws-cdk-lib' ; import * as blueprints from '@aws-quickstart/eks-blueprints' ; const app = new cdk . App (); const account = 'XXXXXXXXXXXXX' ; const region = 'us-east-2' ; const addOns : Array < blueprints . ClusterAddOn > = [ new blueprints . addons . ArgoCDAddOn (), new blueprints . addons . CalicoOperatorAddOn (), new blueprints . addons . MetricsServerAddOn (), new blueprints . addons . ClusterAutoScalerAddOn (), new blueprints . addons . AwsLoadBalancerControllerAddOn (), new blueprints . addons . VpcCniAddOn (), new blueprints . addons . CoreDnsAddOn (), new blueprints . addons . KubeProxyAddOn () ]; const stack = blueprints . EksBlueprint . builder () . account ( account ) . region ( region ) . addOns (... addOns ) . useDefaultSecretEncryption ( true ) // set to false to turn secret encryption off (non-production/demo cases) . build ( app , 'eks-blueprint' ); Each combination of target account and region must be bootstrapped prior to deploying stacks. Bootstrapping is a process of creating IAM roles and lambda functions that can execute some of the common CDK constructs. For application of the EKS Blueprints Framework with AWS Organizations , Multi-account framework and Control Tower consider a process to automatically or manually CDK-bootstrapping new (workload/environment) accounts when they are added to the organization. More info on account bootstrapping here . Bootstrap your environment with the following command. cdk bootstrap Note: if the account/region combination used in the code example above is different from the initial combination used with cdk bootstrap , you will need to perform cdk bootstrap again to avoid error. Please reference CDK usage doc for detail. Deploy the stack using the following command. This command will take roughly 20 minutes to complete. cdk deploy Congratulations! You have deployed your first EKS cluster with eks-blueprints . The above code will provision the following: A new Well-Architected VPC with both Public and Private subnets. A new Well-Architected EKS cluster in the region and account you specify. Nginx into your cluster to serve as a reverse proxy for your workloads. ArgoCD into your cluster to support GitOps deployments. Calico into your cluster to support Network policies. Metrics Server into your cluster to support metrics collection. AWS and Kubernetes resources needed to support Cluster Autoscaler . AWS and Kubernetes resources needed to forward logs and metrics to Container Insights . AWS and Kubernetes resources needed to support AWS Load Balancer Controller . Amazon VPC CNI add-on (VpcCni) into your cluster to support native VPC networking for Amazon EKS. CoreDNS Amazon EKS add-on (CoreDns) into your cluster. CoreDns is a flexible, extensible DNS server that can serve as the Kubernetes cluster DNS kube-proxy Amazon EKS add-on (KubeProxy) into your cluster to maintains network rules on each Amazon EC2 node AWS and Kubernetes resources needed to support AWS X-Ray .","title":"Configure and Deploy EKS Clusters"},{"location":"getting-started/#cluster-access","text":"Once the deploy completes, you will see output in your terminal window similar to the following: Outputs: east-test-1.easttest1ClusterName8D8E5E5E = east-test-1 east-test-1.easttest1ConfigCommand25ABB520 = aws eks update-kubeconfig --name east-test-1 --region us-east-1 --role-arn <ROLE_ARN> east-test-1.easttest1GetTokenCommand337FE3DD = aws eks get-token --cluster-name east-test-1 --region us-east-1 --role-arn <ROLE_ARN> Stack ARN: arn:aws:cloudformation:us-east-1:115717706081:stack/east-test-1/e1b9e6a0-d5f6-11eb-8498-0a374cd00e27 To update your Kubernetes config for you new cluster, copy and run the east-test-1.easttest1ConfigCommand25ABB520 command (the second command) in your terminal. aws eks update-kubeconfig --name east-test-1 --region us-east-1 --role-arn <ROLE_ARN> Validate that you now have kubectl access to your cluster via the following: kubectl get namespace You should see output that lists all namespaces in your cluster.","title":"Cluster Access"},{"location":"getting-started/#deploy-workloads-with-argocd","text":"Next, let's walk you through how to deploy workloads to your cluster with ArgoCD. This approach leverages the App of Apps pattern to deploy multiple workloads across multiple namespaces. The sample app of apps repository that we use in this getting started guide can be found here . You can leverage Automatic Bootstrapping for automatic onboarding of workloads. This feature may be leveraged even when workload repositories are not ready yet, as it creates a placeholder for future workloads and decouples workload onboarding for the infrastructure provisioning pipeline. The next steps, described in this guide apply for cases when customer prefer to bootstrap their workloads manually through ArgoCD UI console.","title":"Deploy workloads with ArgoCD"},{"location":"getting-started/#install-argocd-cli","text":"These steps are needed for manual workload onboarding. For automatic bootstrapping please refer to the Automatic Bootstrapping . Follow the instructions found here as it will include instructions for your specific OS. You can test that the ArgoCD CLI was installed correctly using the following: argocd version --short --client You should see output similar to the following: argocd: v2.0.1+33eaf11.dirty","title":"Install ArgoCD CLI"},{"location":"getting-started/#exposing-argocd","text":"To access ArgoCD running in your Kubernetes cluster, we can leverage Kubernetes Port Forwarding . To do so, first capture the ArgoCD service name in an environment variable. export ARGO_SERVER=$(kubectl get svc -n argocd -l app.kubernetes.io/name=argocd-server -o name) Next, in a new terminal tab, expose the service locally. kubectl port-forward $ARGO_SERVER -n argocd 8080:443 Open your browser to http://localhost:8080 and you should see the ArgoCD login screen.","title":"Exposing ArgoCD"},{"location":"getting-started/#logging-into-argocd","text":"ArgoCD will create an admin user and password on a fresh install. To get the ArgoCD admin password, run the following. export ARGO_PASSWORD=$(kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath=\"{.data.password}\" | base64 -d) While still port-forwarding, login via the following. argocd login localhost:8080 --username admin --password $ARGO_PASSWORD You can also login to the ArgoCD UI with generated password and the username admin . echo $ARGO_PASSWORD","title":"Logging Into ArgoCD"},{"location":"getting-started/#deploy-workloads-to-your-cluster","text":"Create a project in Argo by running the following command argocd proj create sample \\ -d https://kubernetes.default.svc,argocd \\ -s https://github.com/aws-samples/eks-blueprints-workloads.git Create the application within Argo by running the following command argocd app create dev-apps \\ --dest-namespace argocd \\ --dest-server https://kubernetes.default.svc \\ --repo https://github.com/aws-samples/eks-blueprints-workloads.git \\ --path \"envs/dev\" Sync the apps by running the following command argocd app sync dev-apps","title":"Deploy workloads to your cluster"},{"location":"getting-started/#validate-deployments","text":"To validate your deployments, leverage kubectl port-forwarding to access the guestbook-ui service for team-riker . kubectl port-forward svc/guestbook-ui -n team-riker 4040:80 Open up localhost:4040 in your browser and you should see the application.","title":"Validate deployments."},{"location":"getting-started/#next-steps","text":"For information on onboarding teams to your clusters, see Team documentation . For information on deploying Continuous Delivery pipelines for your infrastructure, see Pipelines documentation . For information on supported add-ons, see Add-on documentation For information on Onboarding and managing workloads in your clusters, see Workload documentation .","title":"Next Steps"},{"location":"opa-gatekeeper/","text":"What is OPA Gatekeeper? \u00b6 The Open Policy Agent (OPA, pronounced \u201coh-pa\u201d) is an open source, general-purpose policy engine that unifies policy enforcement across the stack. OPA provides a high-level declarative language that lets you specify policy as code and simple APIs to offload policy decision-making from your software. You can use OPA to enforce policies in microservices, Kubernetes, CI/CD pipelines, API gateways, and more. OPA uses a policy language known as Rego which is a query language which was purpose built to support structured document models such as JSON. To learn more about Rego check out this link . OPA Gatekeeper is an open-source project that provides a first-class integration between OPA and Kubernetes. What Gatekeeper adds is an extensible parameterized policy library that includes native Kubernetes CRD's for instantiating and extending the OPA policy library. The Kubernetes API Server is configured to query OPA for admission control decisions when objects (e.g., Pods, Services, etc.) are created, updated, or deleted. The API Server sends the entire Kubernetes object in the webhook request to OPA. OPA evaluates the policies it has loaded using the admission review as input. Gatekeeper also provides audit functionality as well. The diagram below shows the flow between a user making a request to the Kube-API server and how AdmissionReview and AdmissionRequests are made through OPA Gatekeeper. ) In the context of a development platform running on Amazon EKS, platform teams and administrators need a way of being able to set policies to adhere to governance and security requirements for all workloads and teams working on the same cluster. Examples of standard use cases for using policies via OPA Gatekeeper are listed below: Which users can access which resources. Which subnets egress traffic is allowed to. Which clusters a workload must be deployed to. Which registries binaries can be downloaded from. Which OS capabilities a container can execute with. Which times of day the system can be accessed at. RBAC (role-based access control) can help with some of the scenarios above but roles are nothing but a group of permissions that you then assign to users leveraging rolebindings. If for example, a user tries to perform an operation (get, list, watch, create, etc...) that particular user may do so if they have the appropriate role. Please note that RBAC should be used in conjunction with OPA Gatekeeper policies to fully secure your cluster. Key Terminology \u00b6 OPA Constraint Framework - Framework that enforces CRD-based policies and allow declaratively configured policies to be reliably shareable Constraint - A Constraint is a declaration that its author wants a system to meet a given set of requirements. Each Constraint is written with Rego, a declarative query language used by OPA to enumerate instances of data that violate the expected state of the system. All Constraints are evaluated as a logical AND. If one Constraint is not satisfied, then the whole request is rejected. Enforcement Point - Places where constraints can be enforced. Examples are Git hooks, Kubernetes admission controllers, and audit systems. Constraint Template - Templates that allows users to declare new constraints Target - Represents a coherent set of objects sharing a common identification and/or selection scheme, generic purpose, and can be analyzed in the same validation context Usage \u00b6 import * as blueprints from '@aws-quickstart/eks-blueprints' ; const app = new cdk . App (); const account = < AWS_ACCOUNT_ID > ; const region = < AWS_REGION > ; const blueprint = blueprints . EksBlueprint . builder () . account ( account ) . region ( region ) . addOns ( new blueprints . addons . OpaGatekeeperAddOn () ) . teams (). build ( app , 'my-stack-name' ); To validate that OPA Gatekeeper is running within your cluster run the following command: k get po -n gatekeeper-system You should see the following output: NAME READY STATUS RESTARTS AGE gatekeeper-audit-7c5998d4c-b5n7j 1 /1 Running 0 1d gatekeeper-controller-manager-5894545cc9-b86zm 1 /1 Running 0 1d gatekeeper-controller-manager-5894545cc9-bntdt 1 /1 Running 0 1d gatekeeper-controller-manager-5894545cc9-tb7fz 1 /1 Running 0 1d You will notice the gatekeeper-audit-7c5998d4c-b5n7j pod that is created when we deploy the OpaGatekeeperAddOn . The audit functionality enables periodic evaluations of replicated resources against the Constraints enforced in the cluster to detect pre-existing misconfigurations. Gatekeeper stores audit results as violations listed in the status field of the relevant Constraint. The gatekeeper-controller-manager is simply there to manage the OpaGatekeeperAddOn . Example with OPA Gatekeeper \u00b6 For the purposes of operating within a platform defined by EKS Blueprints , we will be focusing on how to use a policy driven approach to secure our cluster using OPA Gatekeeper. The OPA Gatekeeper community has created a library of example policies and constraint templates which can be found here . In this example we will create a policy that enforces including labels for newly created namespaces and pods. The ConstraintTemplate can be found here . Run the following command to create the ConstraintTemplate: kubectl apply -f https://raw.githubusercontent.com/open-policy-agent/gatekeeper-library/master/library/general/requiredlabels/template.yaml To verify that the ConstraintTemplate was created run the following command: kubectl get constrainttemplate You should see the following output: NAME AGE k8srequiredlabels 45s You will notice that if you create a new namespace without any labels that the request will go through and that is because we now need to create the individual Constraint CRD as defined by the Constraint Template that we created above. Let's create the individal Constraint CRD using the command below: k apply -f https://raw.githubusercontent.com/open-policy-agent/gatekeeper-library/master/library/general/requiredlabels/samples/all-must-have-owner/constraint.yaml If we then try and create a namespace by running kubectl create ns test (notice that we are not adding any labels) you will get the following error message: Error from server ([ all-must-have-owner ] All namespaces must have an ` owner ` label that points to your company username ) : admission webhook \"validation.gatekeeper.sh\" denied the request: [ all-must-have-owner ] All namespaces must have an ` owner ` label that points to your company username For more information on OPA Gatekeeper please refer to the links below: https://github.com/open-policy-agent https://open-policy-agent.github.io/gatekeeper/website/docs/ https://github.com/open-policy-agent/gatekeeper-library","title":"OPA Gatekeeper"},{"location":"opa-gatekeeper/#what-is-opa-gatekeeper","text":"The Open Policy Agent (OPA, pronounced \u201coh-pa\u201d) is an open source, general-purpose policy engine that unifies policy enforcement across the stack. OPA provides a high-level declarative language that lets you specify policy as code and simple APIs to offload policy decision-making from your software. You can use OPA to enforce policies in microservices, Kubernetes, CI/CD pipelines, API gateways, and more. OPA uses a policy language known as Rego which is a query language which was purpose built to support structured document models such as JSON. To learn more about Rego check out this link . OPA Gatekeeper is an open-source project that provides a first-class integration between OPA and Kubernetes. What Gatekeeper adds is an extensible parameterized policy library that includes native Kubernetes CRD's for instantiating and extending the OPA policy library. The Kubernetes API Server is configured to query OPA for admission control decisions when objects (e.g., Pods, Services, etc.) are created, updated, or deleted. The API Server sends the entire Kubernetes object in the webhook request to OPA. OPA evaluates the policies it has loaded using the admission review as input. Gatekeeper also provides audit functionality as well. The diagram below shows the flow between a user making a request to the Kube-API server and how AdmissionReview and AdmissionRequests are made through OPA Gatekeeper. ) In the context of a development platform running on Amazon EKS, platform teams and administrators need a way of being able to set policies to adhere to governance and security requirements for all workloads and teams working on the same cluster. Examples of standard use cases for using policies via OPA Gatekeeper are listed below: Which users can access which resources. Which subnets egress traffic is allowed to. Which clusters a workload must be deployed to. Which registries binaries can be downloaded from. Which OS capabilities a container can execute with. Which times of day the system can be accessed at. RBAC (role-based access control) can help with some of the scenarios above but roles are nothing but a group of permissions that you then assign to users leveraging rolebindings. If for example, a user tries to perform an operation (get, list, watch, create, etc...) that particular user may do so if they have the appropriate role. Please note that RBAC should be used in conjunction with OPA Gatekeeper policies to fully secure your cluster.","title":"What is OPA Gatekeeper?"},{"location":"opa-gatekeeper/#key-terminology","text":"OPA Constraint Framework - Framework that enforces CRD-based policies and allow declaratively configured policies to be reliably shareable Constraint - A Constraint is a declaration that its author wants a system to meet a given set of requirements. Each Constraint is written with Rego, a declarative query language used by OPA to enumerate instances of data that violate the expected state of the system. All Constraints are evaluated as a logical AND. If one Constraint is not satisfied, then the whole request is rejected. Enforcement Point - Places where constraints can be enforced. Examples are Git hooks, Kubernetes admission controllers, and audit systems. Constraint Template - Templates that allows users to declare new constraints Target - Represents a coherent set of objects sharing a common identification and/or selection scheme, generic purpose, and can be analyzed in the same validation context","title":"Key Terminology"},{"location":"opa-gatekeeper/#usage","text":"import * as blueprints from '@aws-quickstart/eks-blueprints' ; const app = new cdk . App (); const account = < AWS_ACCOUNT_ID > ; const region = < AWS_REGION > ; const blueprint = blueprints . EksBlueprint . builder () . account ( account ) . region ( region ) . addOns ( new blueprints . addons . OpaGatekeeperAddOn () ) . teams (). build ( app , 'my-stack-name' ); To validate that OPA Gatekeeper is running within your cluster run the following command: k get po -n gatekeeper-system You should see the following output: NAME READY STATUS RESTARTS AGE gatekeeper-audit-7c5998d4c-b5n7j 1 /1 Running 0 1d gatekeeper-controller-manager-5894545cc9-b86zm 1 /1 Running 0 1d gatekeeper-controller-manager-5894545cc9-bntdt 1 /1 Running 0 1d gatekeeper-controller-manager-5894545cc9-tb7fz 1 /1 Running 0 1d You will notice the gatekeeper-audit-7c5998d4c-b5n7j pod that is created when we deploy the OpaGatekeeperAddOn . The audit functionality enables periodic evaluations of replicated resources against the Constraints enforced in the cluster to detect pre-existing misconfigurations. Gatekeeper stores audit results as violations listed in the status field of the relevant Constraint. The gatekeeper-controller-manager is simply there to manage the OpaGatekeeperAddOn .","title":"Usage"},{"location":"opa-gatekeeper/#example-with-opa-gatekeeper","text":"For the purposes of operating within a platform defined by EKS Blueprints , we will be focusing on how to use a policy driven approach to secure our cluster using OPA Gatekeeper. The OPA Gatekeeper community has created a library of example policies and constraint templates which can be found here . In this example we will create a policy that enforces including labels for newly created namespaces and pods. The ConstraintTemplate can be found here . Run the following command to create the ConstraintTemplate: kubectl apply -f https://raw.githubusercontent.com/open-policy-agent/gatekeeper-library/master/library/general/requiredlabels/template.yaml To verify that the ConstraintTemplate was created run the following command: kubectl get constrainttemplate You should see the following output: NAME AGE k8srequiredlabels 45s You will notice that if you create a new namespace without any labels that the request will go through and that is because we now need to create the individual Constraint CRD as defined by the Constraint Template that we created above. Let's create the individal Constraint CRD using the command below: k apply -f https://raw.githubusercontent.com/open-policy-agent/gatekeeper-library/master/library/general/requiredlabels/samples/all-must-have-owner/constraint.yaml If we then try and create a namespace by running kubectl create ns test (notice that we are not adding any labels) you will get the following error message: Error from server ([ all-must-have-owner ] All namespaces must have an ` owner ` label that points to your company username ) : admission webhook \"validation.gatekeeper.sh\" denied the request: [ all-must-have-owner ] All namespaces must have an ` owner ` label that points to your company username For more information on OPA Gatekeeper please refer to the links below: https://github.com/open-policy-agent https://open-policy-agent.github.io/gatekeeper/website/docs/ https://github.com/open-policy-agent/gatekeeper-library","title":"Example with OPA Gatekeeper"},{"location":"pipelines/","text":"Pipelines \u00b6 While it is convenient to leverage the CDK command line tool to deploy your first cluster, we recommend setting up automated pipelines that will be responsible for deploying and updating your EKS infrastructure. To accomplish this, the EKS Blueprints - Reference Solution leverages the Pipelines CDK module. This module makes it trivial to create Continuous Delivery (CD) pipelines via CodePipeline that are responsible for deploying and updating your infrastructure. Additionally, the EKS Blueprints - Reference Solution leverages the GitHub integration that the Pipelines CDK module provides in order to integrate our pipelines with GitHub. The end result is that any new configuration pushed to a GitHub repository containing our CDK will be automatically deployed. Defining your blueprint to use with pipeline \u00b6 Pipeline support requires enabling a setting for modern stack synthesis. This setting should be enabled for blueprints that leverage CDKv1 explicitly, for CDKv2 it is enabled by default. Creation of a pipeline starts with defining the blueprint that will be deployed across the pipeline stages. The framework allows defining a blueprint builder without instantiating the stack. import * as blueprints from '@aws-quickstart/eks-blueprints' import * as team from 'path/to/teams' const blueprint = blueprints . EksBlueprint . builder () . account ( account ) // the supplied default will fail, but build and synth will pass . region ( 'us-west-1' ) . addOns ( new blueprints . AwsLoadBalancerControllerAddOn , new blueprints . ExternalDnsAddOn , new blueprints . NginxAddOn , new blueprints . CalicoAddOn , new blueprints . MetricsServerAddOn , new blueprints . ClusterAutoScalerAddOn , new blueprints . ContainerInsightsAddOn ) . teams ( new team . TeamRikerSetup ); The difference between the above code and a normal way of instantiating the stack is lack of .build() at the end of the blueprint definition. This code will produce a blueprint builder that can be instantiated inside the pipeline stages. Creating a pipeline \u00b6 We can create a new CodePipeline resource via the following. Using GitHub as CodePipeline repository source. \u00b6 import * as blueprints from '@aws-quickstart/eks-blueprints' const blueprint = blueprints . EksBlueprint . builder () ...; // configure your blueprint builder blueprints . CodePipelineStack . builder () . name ( \"eks-blueprints-pipeline\" ) . repository ({ owner : \"aws-samples\" , repoUrl : 'cdk-eks-blueprints-patterns' , credentialsSecretName : 'github-token' , targetRevision : 'main' // optional, default is \"main\" }) Note: the above code depends on the AWS secret github-token defined in the target account/region. The secret may be fined in one main region, and replicated to all target regions. Using AWS CodeCommit as CodePipeline repository source. \u00b6 import * as blueprints from '@aws-quickstart/eks-blueprints' const blueprint = blueprints . EksBlueprint . builder () ...; // configure your blueprint builder blueprints . CodePipelineStack . builder () . name ( \"eks-blueprints-pipeline\" ) . repository ({ codeCommitRepoName : 'cdk-eks-blueprints-patterns' , // should be in the same region with pipeline-stack targetRevision : 'master' // optional, default is \"master\" }) Creating stages \u00b6 Once our pipeline is created, we need to define stages for the pipeline. To do so, we can leverage blueprints.StackStage convenience class and builder support for it. Let's continue leveraging the pipeline builder defined in the previous step. const blueprint = blueprints . EksBlueprint . builder () ...; // configure your blueprint builder blueprints . CodePipelineStack . builder () . name ( \"blueprints-eks-pipeline\" ) . owner ( \"aws-samples\" ) . repository ({ // your repo info }) . stage ({ id : 'us-west-1-managed-blueprints-test' , stackBuilder : blueprint.clone ( 'us-west-1' ) // clone the blueprint to customize for the stage. You can add more add-ons, teams here. }) . stage ({ id : 'us-east-2-managed-blueprints-prod' , stackBuilder : blueprint.clone ( 'us-east-2' ), // clone the blueprint to customize for the stage. You can add more add-ons, team, here. stageProps : { manualApprovals : true } }) Consider adding ArgoCDAddOn with your specific workload bootstrap repository to automatically bootstrap workloads in the provisioned clusters. See Bootstrapping for more details. Adding Waves \u00b6 In many case, when enterprises configure their SDLC environments, such as dev/test/staging/prod, each environment may contain more than a single cluster. It is convenient to provision and maintain (update/upgrade) such clusters in parallel within the limits of the environment. Such environments may be represented as waves of the pipeline. The wave concept is not limited to just a logical environment. It may represent any grouping of clusters that should be executed in parallel. An important advantage of running stages in parallel is the time gain associated with it. Each stage may potentially take tens of minutes (e.g. initial provisioning, upgrade, etc.) and as the number of clusters increase, the overall pipeline run may become very lengthy and won't provide enough agility for the enterprise. Running parallel stages within a wave provides roughly the time performance equivalent to a single stage. Pipeline functionality provides wave support to express waves with blueprints. You can mix individual stages and waves together. An individual stage can be viewed as a wave with a single stage. blueprints . CodePipelineStack . builder () . name ( \"eks-blueprints-pipeline\" ) . owner ( \"aws-samples\" ) . repository ({ //... }) . stage ({ id : 'us-west-1-managed-blueprints' , stackBuilder : blueprint.clone ( 'us-west-1' ) }) . wave ( { // adding two clusters for dev env id : \"dev\" , stages : [ { id : \"dev-west-1\" , stackBuilder : blueprint.clone ( 'us-west-1' ). account ( DEV_ACCOUNT )}, // requires trust relationship with the code pipeline role { id : \"dev-east-2\" , stackBuilder : blueprint.clone ( 'us-east-2' ). account ( DEV_ACCOUNT )}, // See https://docs.aws.amazon.com/cdk/api/v1/docs/pipelines-readme.html#cdk-environment-bootstrapping ] }) . wave ( { id : \"prod\" , stages : [ { id : \"prod-west-1\" , stackBuilder : blueprint.clone ( 'us-west-1' )}, { id : \"prod-east-2\" , stackBuilder : blueprint.clone ( 'us-east-2' )}, ] }) Handling Build Time Access \u00b6 CodePipeline leverages CodeBuild to build artifacts. During build time, add-ons may require various look-ups. For example, the add-on or stack may look up VPC, subnets, certificates, hosted zones as well as secrets. By default, such look-ups at build time are restricted by the CodeBuild role created by the pipeline dynamically. That means that builds that require look-ups will fail with access denied exception. Customers can configure the IAM policies used by the CodeBuild as part of the pipeline execution. The framework provides a convenience default build policies blueprints.DEFAULT_BUILD_POLICIES that enable look-ups (including secret look-ups). Example with default policies: const pipeline = blueprints . CodePipelineStack . builder () . name ( \"blueprints-pipeline-with-build-roles\" ) . owner ( 'aws-samples' ) . codeBuildPolicies ( blueprints . DEFAULT_BUILD_POLICIES ) Example with customer supplied policies: import { PolicyStatement } from 'aws-cdk-lib/aws-iam' ; const pipeline = blueprints . CodePipelineStack . builder () . name ( \"blueprints-pipeline-inaction\" ) . owner ( 'your-owner' ) . codeBuildPolicies ([ new PolicyStatement ({ resources : [ mySecretArn ], actions : [ \"secretsmanager:GetSecretValue\" , \"secretsmanager:DescribeSecret\" , ] }) ]) . build (...) Build the pipeline stack \u00b6 Now that we have defined the blueprint builder, the pipeline with repository and stages we just need to invoke the build() step to create the stack. const blueprint = blueprints . EksBlueprint . builder () ...; // configure your blueprint builder blueprints . CodePipelineStack . builder () . name ( \"eks-blueprints-pipeline\" ) . owner ( \"aws-samples\" ) // owner of your repo . repository ({ // your repo info }) . stage ({ id : 'dev' , stackBuilder : blueprint.clone ( 'us-west-1' ) // clone the blueprint to customize for the stage. You can add more add-ons, teams here. }) . stage ({ id : 'test' , stackBuilder : blueprint.clone ( 'us-east-2' ), // clone the blueprint to customize for the stage. You can add more add-ons, team, here. }) . stage ({ id : 'prod' , stackBuilder : blueprint.clone ( 'us-west-2' ), // clone the blueprint to customize for the stage. You can add more add-ons, team, here. }) . build ( scope , \"blueprints-pipeline-stack\" , props ); // will produce the self-mutating pipeline in the target region and start provisioning the defined blueprints. Deploying Pipelines \u00b6 In order to deploy pipelines, each environment (account and region) where pipeline will either be running and each environment to which it will be deploying should be bootstrapped based on CodePipeline documentation . Examples of bootstrapping (from the original documentation): To bootstrap an environment for provisioning the pipeline: $ env CDK_NEW_BOOTSTRAP = 1 npx cdk bootstrap \\ [ --profile admin-profile-1 ] \\ --cloudformation-execution-policies arn:aws:iam::aws:policy/AdministratorAccess \\ aws://111111111111/us-east-1 To bootstrap a different environment for deploying CDK applications into using a pipeline in account 111111111111: $ env CDK_NEW_BOOTSTRAP = 1 npx cdk bootstrap \\ [ --profile admin-profile-2 ] \\ --cloudformation-execution-policies arn:aws:iam::aws:policy/AdministratorAccess \\ --trust 11111111111 \\ aws://222222222222/us-east-2 If you only want to trust an account to do lookups (e.g, when your CDK application has a Vpc.fromLookup() call), use the option --trust-for-lookup: $ env CDK_NEW_BOOTSTRAP = 1 npx cdk bootstrap \\ [ --profile admin-profile-2 ] \\ --cloudformation-execution-policies arn:aws:iam::aws:policy/AdministratorAccess \\ --trust-for-lookup 11111111111 \\ aws://222222222222/us-east-2 Troubleshooting \u00b6 Blueprints Build can fail with AccessDenied exception during build phase. Typical error messages: Error: AccessDeniedException: User: arn:aws:sts::<account>:assumed-role/blueprints-pipeline-stack-blueprintsekspipelinePipelineBuildSynt-1NPFJRH6H7TB1/AWSCodeBuild-e95830ee-07f6-46f5-aaee-90e269c7eb5f is not authorized to perform: current credentials could not be used to assume 'arn:aws:iam::<account>:role/cdk-hnb659fds-lookup-role-,account>-eu-west-3', but are for the right account. Proceeding anyway. Error: you are not authorized to perform this operation. This can happen for a few reasons, but most typical is related to the stack requiring elevated permissions at build time. Such permissions may be required to perform lookups, such as look up fo VPC, Hosted Zone, Certificate (if imported) and those are handled during stack synthesis. Resolution You can take advantage of supplying the require IAM policies to the pipeline to avoid this error. See this for more details. To address this issue \"manually\" (without the change to the pipeline), you can locate the role leveraged for Code Build and provide required permissions. Depending on the scope of the build role, the easiest resolution is to add AdministratorAccess permission to the build role which typically looks similar to this blueprints-pipeline-stack-blueprintsekspipelinePipelineBuildSynt-1NPFJRH6H7TB1 provided your pipeline stack was named blueprints-pipeline-stack . If adding administrative access to the role solves the issue, you can then consider tightening the role scope to just the required permissions, such as access to specific resources needed for the build.","title":"Pipelines"},{"location":"pipelines/#pipelines","text":"While it is convenient to leverage the CDK command line tool to deploy your first cluster, we recommend setting up automated pipelines that will be responsible for deploying and updating your EKS infrastructure. To accomplish this, the EKS Blueprints - Reference Solution leverages the Pipelines CDK module. This module makes it trivial to create Continuous Delivery (CD) pipelines via CodePipeline that are responsible for deploying and updating your infrastructure. Additionally, the EKS Blueprints - Reference Solution leverages the GitHub integration that the Pipelines CDK module provides in order to integrate our pipelines with GitHub. The end result is that any new configuration pushed to a GitHub repository containing our CDK will be automatically deployed.","title":"Pipelines"},{"location":"pipelines/#defining-your-blueprint-to-use-with-pipeline","text":"Pipeline support requires enabling a setting for modern stack synthesis. This setting should be enabled for blueprints that leverage CDKv1 explicitly, for CDKv2 it is enabled by default. Creation of a pipeline starts with defining the blueprint that will be deployed across the pipeline stages. The framework allows defining a blueprint builder without instantiating the stack. import * as blueprints from '@aws-quickstart/eks-blueprints' import * as team from 'path/to/teams' const blueprint = blueprints . EksBlueprint . builder () . account ( account ) // the supplied default will fail, but build and synth will pass . region ( 'us-west-1' ) . addOns ( new blueprints . AwsLoadBalancerControllerAddOn , new blueprints . ExternalDnsAddOn , new blueprints . NginxAddOn , new blueprints . CalicoAddOn , new blueprints . MetricsServerAddOn , new blueprints . ClusterAutoScalerAddOn , new blueprints . ContainerInsightsAddOn ) . teams ( new team . TeamRikerSetup ); The difference between the above code and a normal way of instantiating the stack is lack of .build() at the end of the blueprint definition. This code will produce a blueprint builder that can be instantiated inside the pipeline stages.","title":"Defining your blueprint to use with pipeline"},{"location":"pipelines/#creating-a-pipeline","text":"We can create a new CodePipeline resource via the following.","title":"Creating a pipeline"},{"location":"pipelines/#using-github-as-codepipeline-repository-source","text":"import * as blueprints from '@aws-quickstart/eks-blueprints' const blueprint = blueprints . EksBlueprint . builder () ...; // configure your blueprint builder blueprints . CodePipelineStack . builder () . name ( \"eks-blueprints-pipeline\" ) . repository ({ owner : \"aws-samples\" , repoUrl : 'cdk-eks-blueprints-patterns' , credentialsSecretName : 'github-token' , targetRevision : 'main' // optional, default is \"main\" }) Note: the above code depends on the AWS secret github-token defined in the target account/region. The secret may be fined in one main region, and replicated to all target regions.","title":"Using GitHub as CodePipeline repository source."},{"location":"pipelines/#using-aws-codecommit-as-codepipeline-repository-source","text":"import * as blueprints from '@aws-quickstart/eks-blueprints' const blueprint = blueprints . EksBlueprint . builder () ...; // configure your blueprint builder blueprints . CodePipelineStack . builder () . name ( \"eks-blueprints-pipeline\" ) . repository ({ codeCommitRepoName : 'cdk-eks-blueprints-patterns' , // should be in the same region with pipeline-stack targetRevision : 'master' // optional, default is \"master\" })","title":"Using AWS CodeCommit as CodePipeline repository source."},{"location":"pipelines/#creating-stages","text":"Once our pipeline is created, we need to define stages for the pipeline. To do so, we can leverage blueprints.StackStage convenience class and builder support for it. Let's continue leveraging the pipeline builder defined in the previous step. const blueprint = blueprints . EksBlueprint . builder () ...; // configure your blueprint builder blueprints . CodePipelineStack . builder () . name ( \"blueprints-eks-pipeline\" ) . owner ( \"aws-samples\" ) . repository ({ // your repo info }) . stage ({ id : 'us-west-1-managed-blueprints-test' , stackBuilder : blueprint.clone ( 'us-west-1' ) // clone the blueprint to customize for the stage. You can add more add-ons, teams here. }) . stage ({ id : 'us-east-2-managed-blueprints-prod' , stackBuilder : blueprint.clone ( 'us-east-2' ), // clone the blueprint to customize for the stage. You can add more add-ons, team, here. stageProps : { manualApprovals : true } }) Consider adding ArgoCDAddOn with your specific workload bootstrap repository to automatically bootstrap workloads in the provisioned clusters. See Bootstrapping for more details.","title":"Creating stages"},{"location":"pipelines/#adding-waves","text":"In many case, when enterprises configure their SDLC environments, such as dev/test/staging/prod, each environment may contain more than a single cluster. It is convenient to provision and maintain (update/upgrade) such clusters in parallel within the limits of the environment. Such environments may be represented as waves of the pipeline. The wave concept is not limited to just a logical environment. It may represent any grouping of clusters that should be executed in parallel. An important advantage of running stages in parallel is the time gain associated with it. Each stage may potentially take tens of minutes (e.g. initial provisioning, upgrade, etc.) and as the number of clusters increase, the overall pipeline run may become very lengthy and won't provide enough agility for the enterprise. Running parallel stages within a wave provides roughly the time performance equivalent to a single stage. Pipeline functionality provides wave support to express waves with blueprints. You can mix individual stages and waves together. An individual stage can be viewed as a wave with a single stage. blueprints . CodePipelineStack . builder () . name ( \"eks-blueprints-pipeline\" ) . owner ( \"aws-samples\" ) . repository ({ //... }) . stage ({ id : 'us-west-1-managed-blueprints' , stackBuilder : blueprint.clone ( 'us-west-1' ) }) . wave ( { // adding two clusters for dev env id : \"dev\" , stages : [ { id : \"dev-west-1\" , stackBuilder : blueprint.clone ( 'us-west-1' ). account ( DEV_ACCOUNT )}, // requires trust relationship with the code pipeline role { id : \"dev-east-2\" , stackBuilder : blueprint.clone ( 'us-east-2' ). account ( DEV_ACCOUNT )}, // See https://docs.aws.amazon.com/cdk/api/v1/docs/pipelines-readme.html#cdk-environment-bootstrapping ] }) . wave ( { id : \"prod\" , stages : [ { id : \"prod-west-1\" , stackBuilder : blueprint.clone ( 'us-west-1' )}, { id : \"prod-east-2\" , stackBuilder : blueprint.clone ( 'us-east-2' )}, ] })","title":"Adding Waves"},{"location":"pipelines/#handling-build-time-access","text":"CodePipeline leverages CodeBuild to build artifacts. During build time, add-ons may require various look-ups. For example, the add-on or stack may look up VPC, subnets, certificates, hosted zones as well as secrets. By default, such look-ups at build time are restricted by the CodeBuild role created by the pipeline dynamically. That means that builds that require look-ups will fail with access denied exception. Customers can configure the IAM policies used by the CodeBuild as part of the pipeline execution. The framework provides a convenience default build policies blueprints.DEFAULT_BUILD_POLICIES that enable look-ups (including secret look-ups). Example with default policies: const pipeline = blueprints . CodePipelineStack . builder () . name ( \"blueprints-pipeline-with-build-roles\" ) . owner ( 'aws-samples' ) . codeBuildPolicies ( blueprints . DEFAULT_BUILD_POLICIES ) Example with customer supplied policies: import { PolicyStatement } from 'aws-cdk-lib/aws-iam' ; const pipeline = blueprints . CodePipelineStack . builder () . name ( \"blueprints-pipeline-inaction\" ) . owner ( 'your-owner' ) . codeBuildPolicies ([ new PolicyStatement ({ resources : [ mySecretArn ], actions : [ \"secretsmanager:GetSecretValue\" , \"secretsmanager:DescribeSecret\" , ] }) ]) . build (...)","title":"Handling Build Time Access"},{"location":"pipelines/#build-the-pipeline-stack","text":"Now that we have defined the blueprint builder, the pipeline with repository and stages we just need to invoke the build() step to create the stack. const blueprint = blueprints . EksBlueprint . builder () ...; // configure your blueprint builder blueprints . CodePipelineStack . builder () . name ( \"eks-blueprints-pipeline\" ) . owner ( \"aws-samples\" ) // owner of your repo . repository ({ // your repo info }) . stage ({ id : 'dev' , stackBuilder : blueprint.clone ( 'us-west-1' ) // clone the blueprint to customize for the stage. You can add more add-ons, teams here. }) . stage ({ id : 'test' , stackBuilder : blueprint.clone ( 'us-east-2' ), // clone the blueprint to customize for the stage. You can add more add-ons, team, here. }) . stage ({ id : 'prod' , stackBuilder : blueprint.clone ( 'us-west-2' ), // clone the blueprint to customize for the stage. You can add more add-ons, team, here. }) . build ( scope , \"blueprints-pipeline-stack\" , props ); // will produce the self-mutating pipeline in the target region and start provisioning the defined blueprints.","title":"Build the pipeline stack"},{"location":"pipelines/#deploying-pipelines","text":"In order to deploy pipelines, each environment (account and region) where pipeline will either be running and each environment to which it will be deploying should be bootstrapped based on CodePipeline documentation . Examples of bootstrapping (from the original documentation): To bootstrap an environment for provisioning the pipeline: $ env CDK_NEW_BOOTSTRAP = 1 npx cdk bootstrap \\ [ --profile admin-profile-1 ] \\ --cloudformation-execution-policies arn:aws:iam::aws:policy/AdministratorAccess \\ aws://111111111111/us-east-1 To bootstrap a different environment for deploying CDK applications into using a pipeline in account 111111111111: $ env CDK_NEW_BOOTSTRAP = 1 npx cdk bootstrap \\ [ --profile admin-profile-2 ] \\ --cloudformation-execution-policies arn:aws:iam::aws:policy/AdministratorAccess \\ --trust 11111111111 \\ aws://222222222222/us-east-2 If you only want to trust an account to do lookups (e.g, when your CDK application has a Vpc.fromLookup() call), use the option --trust-for-lookup: $ env CDK_NEW_BOOTSTRAP = 1 npx cdk bootstrap \\ [ --profile admin-profile-2 ] \\ --cloudformation-execution-policies arn:aws:iam::aws:policy/AdministratorAccess \\ --trust-for-lookup 11111111111 \\ aws://222222222222/us-east-2","title":"Deploying Pipelines"},{"location":"pipelines/#troubleshooting","text":"Blueprints Build can fail with AccessDenied exception during build phase. Typical error messages: Error: AccessDeniedException: User: arn:aws:sts::<account>:assumed-role/blueprints-pipeline-stack-blueprintsekspipelinePipelineBuildSynt-1NPFJRH6H7TB1/AWSCodeBuild-e95830ee-07f6-46f5-aaee-90e269c7eb5f is not authorized to perform: current credentials could not be used to assume 'arn:aws:iam::<account>:role/cdk-hnb659fds-lookup-role-,account>-eu-west-3', but are for the right account. Proceeding anyway. Error: you are not authorized to perform this operation. This can happen for a few reasons, but most typical is related to the stack requiring elevated permissions at build time. Such permissions may be required to perform lookups, such as look up fo VPC, Hosted Zone, Certificate (if imported) and those are handled during stack synthesis. Resolution You can take advantage of supplying the require IAM policies to the pipeline to avoid this error. See this for more details. To address this issue \"manually\" (without the change to the pipeline), you can locate the role leveraged for Code Build and provide required permissions. Depending on the scope of the build role, the easiest resolution is to add AdministratorAccess permission to the build role which typically looks similar to this blueprints-pipeline-stack-blueprintsekspipelinePipelineBuildSynt-1NPFJRH6H7TB1 provided your pipeline stack was named blueprints-pipeline-stack . If adding administrative access to the role solves the issue, you can then consider tightening the role scope to just the required permissions, such as access to specific resources needed for the build.","title":"Troubleshooting"},{"location":"addons/","text":"Add-ons \u00b6 The eks-blueprints framework leverages a modular approach to managing Add-ons that run within the context of a Kubernetes cluster. Customers are free to select the add-ons that run in each of their blueprint clusters. Within the context of the eks-blueprints framework, an add-on is abstracted as ClusterAddOn interface, and the implementation of the add-on interface can do whatever is necessary to support the desired add-on functionality. This can include applying manifests to a Kubernetes cluster or calling AWS APIs to provision new resources. Supported Add-ons \u00b6 The framework currently supports the following add-ons. Add-on Description ACKAddOn Adds ACK (AWS Controllers for Kubernetes . AdotAddOn Adds AWS Distro for OpenTelemetry (ADOT) Operator. AmpAdotAddOn Deploys ADOT Collector for Prometheus to remote write metrics from AMP. AppMeshAddOn Adds an AppMesh controller and CRDs. ArgoCDAddOn Provisions Argo CD into your cluster. AWS Batch for EKS Enables EKS cluster to be used with AWS Batch on EKS AWS for Fluent Bit Provisions Fluent Bit into your cluster for log aggregation and consumption. AWS Load Balancer Controller Provisions the AWS Load Balancer Controller into your cluster. AWS Node Termination Handler Provisions Node Termination Handler into your cluster. AWS Private CA Issuer Installs AWS Private CA Issuer into your cluster. CertManagerAddOn Adds Certificate Manager to your EKS cluster. CalicoOperatorAddOn Adds the Calico CNI/Network policy cluster. CloudWatchAdotAddOn Adds Cloudwatch exporter based on ADOT operator integrating monitoring with CloudWatch. ClusterAutoscalerAddOn Adds the standard cluster autoscaler. CoreDnsAddOn Adds CoreDNS Amazon EKS add-on. CoreDNS is a flexible, extensible DNS server that can serve as the Kubernetes cluster DNS. DatadogAddOn Adds Datadog Amazon EKS add-on. Datadog is the monitoring and security platform for cloud applications. Dynatrace Adds the Dynatrace OneAgent Operator . EbsCsiDriverAddOn Adds EBS CSI Driver Amazon EKS add-on. This driver manages the lifecycle of Amazon EBS volumes for persistent storage. EfsCsiDriverAddOn Adds EFS CSI Driver Amazon EKS add-on. This driver manages the lifecycle of Amazon EFS volumes for persistent storage. EmrOnEksAddOn Enable EKS cluster to be used with EMR on EKS ExternalDnsAddOn Adds External DNS support for AWS to the cluster, integrating with Amazon Route 53. ExternalSecretsAddOn Adds External Secrets Operator to the cluster. FluxcdAddOn Setting up Fluxcd to manage one or more Kubernetes clusters. GrafanaOperatorAddon Deploys GrafanaOperatorAddon on your EKS Cluster to manage Amazon Managed Grafana and other external Grafana instances. IstioBaseAddOn Adds support for Istio base chart to the EKS cluster. IstioControlPlaneAddOn Installs Istio Control Plane addon to the EKS cluster. JupyterHubAddOn Adds JupyterHub support for AWS to the cluster. Kasten-K10AddOn Kasten K10 add-on installs Kasten K10 into your Amazon EKS cluster. KedaAddOn Installs Keda into EKS cluster. Keptn Keptn Control Plane and Execution Plane AddOn. KnativeAddOn Deploys the KNative Operator to ease setting up the rest of KNatives CRDs KubecostAddOn Adds Kubecost cost analyzer to the EKS cluster. KubeflowAddOn Adds kubeflow Kubeflow pipeline addon the EKS cluster. KubeviousAddOn Adds Kubevious open source Kubernetes dashboard to an EKS cluster. KarpenterAddOn Adds Karpenter support for Amazon EKS. KubeProxyAddOn Adds kube-proxy Amazon EKS add-on. Kube-proxy maintains network rules on each Amazon EC2 node. KubeStateMetricsAddOn Adds kube-state-metrics into the EKS cluster. MetricsServerAddOn Adds metrics server (pre-req for HPA and other monitoring tools). NewRelicAddOn Adds New Relic and Pixie observability for Amazon EKS. NginxAddOn Adds NGINX ingress controller OpaGatekeeperAddOn Adds OPA Gatekeeper PixieAddOn Adds Pixie to the EKS Cluster. Pixie provides auto-telemetry for requests, metrics, application profiles, and more. PrometheusNodeExporterAddOn Adds prometheus-node-exporter to the EKS Cluster. Prometheus Node Exporter enables you to measure various machine resources such as memory, disk and CPU utilization. SecretsStoreAddOn Adds AWS Secrets Manager and Config Provider for Secret Store CSI Driver to the EKS Cluster. Snyk Adds the Snyk Monitor to the EKS Cluster. SSMAgentAddOn Adds Amazon SSM Agent to worker nodes. UpboundUniversalCrossplaneAddOn Allows Elastic Kubernetes Service (Amazon EKS) clusters to manage the lifecycle of Crossplane distribution. VpcCniAddOn Adds the Amazon VPC CNI Amazon EKS addon to support native VPC networking for Amazon EKS. VeleroAddOn Adds Velero to the EKS Cluster. XrayAddOn Adds XRay Daemon to the EKS Cluster. XrayAdotAddOn Deploys ADOT Collector for Xray to receive traces from your workloads. Standard Helm Add-On Configuration Options \u00b6 Many add-ons leverage helm to provision and maintain deployments. All provided add-ons that leverage helm allow specifying the following add-on attributes: /** * Name of the helm chart (add-on) */ name? : string , /** * Namespace where helm release will be installed */ namespace? : string , /** * Chart name */ chart? : string , /** * Helm chart version. */ version? : string , /** * Helm release */ release? : string , /** * Helm repository */ repository? : string , /** * When global helm version validation is enabled with HelmAddOn.validateHelmVersions = true * allows to skip validation for a particular helm add-on. */ skipVersionValidation? : boolean , /** * Optional values for the helm chart. */ values? : Values Ability to set repository url may be leveraged for private repositories. Version field can be modified from the default chart version, e.g. if the add-on should be upgraded to the desired version, however, since the helm chart version supplied by the customer may not have been tested as part of the Blueprints release process, Blueprints community may not be able to reproduce/fix issues related to the helm chart version upgrade. Helm Version Validation \u00b6 All add-ons that derive from HelmAddOn support optional version validation against the latest published version in the target helm repository. Helm version validation can result either in a warning on console during list , synth and deploy operations or an exception if the target helm repository contains higher version than the one leveraged in the add-on. Example output: INFO Chart argo-cd-4.9.12 is at the latest version. INFO Chart external-dns-6.6.0 is at the latest version. WARN Upgrade is needed for chart gatekeeper-3.8.1: latest version is 3.9.0-beta.2. INFO Chart appmesh-controller-1.5.0 is at the latest version. INFO Chart tigera-operator-v3.23.2 is at the latest version. WARN Upgrade is needed for chart adot-exporter-for-eks-on-ec2-0.1.0: latest version is 0.6.0. INFO Chart aws-load-balancer-controller-1.4.2 is at the latest version. INFO Chart nginx-ingress-0.14.0 is at the latest version. INFO Chart velero-2.30.1 is at the latest version. INFO Chart falco-1.19.4 is at the latest version. WARN Upgrade is needed for chart karpenter-0.13.1: latest version is 0.13.2. INFO Chart kubevious-1.0.10 is at the latest version. INFO Chart aws-efs-csi-driver-2.2.7 is at the latest version. INFO Chart keda-2.7.2 is at the latest version. INFO Chart secrets-store-csi-driver-1.2.1 is at the latest version. Enable/Disable Helm version validation globally import { HelmAddOn } from '@aws-quickstart/eks-blueprints' ; HelmAddOn . validateHelmVersions = true ; // by default will print out warnings HelmAddOn . failOnVersionValidation = true ; // enable synth to throw exceptions on validation check failures Enable/Disable Helm version validation per add-on new blueprints . addons . MetricsServerAddOn ({ skipVersionValidation : true })","title":"Overview"},{"location":"addons/#add-ons","text":"The eks-blueprints framework leverages a modular approach to managing Add-ons that run within the context of a Kubernetes cluster. Customers are free to select the add-ons that run in each of their blueprint clusters. Within the context of the eks-blueprints framework, an add-on is abstracted as ClusterAddOn interface, and the implementation of the add-on interface can do whatever is necessary to support the desired add-on functionality. This can include applying manifests to a Kubernetes cluster or calling AWS APIs to provision new resources.","title":"Add-ons"},{"location":"addons/#supported-add-ons","text":"The framework currently supports the following add-ons. Add-on Description ACKAddOn Adds ACK (AWS Controllers for Kubernetes . AdotAddOn Adds AWS Distro for OpenTelemetry (ADOT) Operator. AmpAdotAddOn Deploys ADOT Collector for Prometheus to remote write metrics from AMP. AppMeshAddOn Adds an AppMesh controller and CRDs. ArgoCDAddOn Provisions Argo CD into your cluster. AWS Batch for EKS Enables EKS cluster to be used with AWS Batch on EKS AWS for Fluent Bit Provisions Fluent Bit into your cluster for log aggregation and consumption. AWS Load Balancer Controller Provisions the AWS Load Balancer Controller into your cluster. AWS Node Termination Handler Provisions Node Termination Handler into your cluster. AWS Private CA Issuer Installs AWS Private CA Issuer into your cluster. CertManagerAddOn Adds Certificate Manager to your EKS cluster. CalicoOperatorAddOn Adds the Calico CNI/Network policy cluster. CloudWatchAdotAddOn Adds Cloudwatch exporter based on ADOT operator integrating monitoring with CloudWatch. ClusterAutoscalerAddOn Adds the standard cluster autoscaler. CoreDnsAddOn Adds CoreDNS Amazon EKS add-on. CoreDNS is a flexible, extensible DNS server that can serve as the Kubernetes cluster DNS. DatadogAddOn Adds Datadog Amazon EKS add-on. Datadog is the monitoring and security platform for cloud applications. Dynatrace Adds the Dynatrace OneAgent Operator . EbsCsiDriverAddOn Adds EBS CSI Driver Amazon EKS add-on. This driver manages the lifecycle of Amazon EBS volumes for persistent storage. EfsCsiDriverAddOn Adds EFS CSI Driver Amazon EKS add-on. This driver manages the lifecycle of Amazon EFS volumes for persistent storage. EmrOnEksAddOn Enable EKS cluster to be used with EMR on EKS ExternalDnsAddOn Adds External DNS support for AWS to the cluster, integrating with Amazon Route 53. ExternalSecretsAddOn Adds External Secrets Operator to the cluster. FluxcdAddOn Setting up Fluxcd to manage one or more Kubernetes clusters. GrafanaOperatorAddon Deploys GrafanaOperatorAddon on your EKS Cluster to manage Amazon Managed Grafana and other external Grafana instances. IstioBaseAddOn Adds support for Istio base chart to the EKS cluster. IstioControlPlaneAddOn Installs Istio Control Plane addon to the EKS cluster. JupyterHubAddOn Adds JupyterHub support for AWS to the cluster. Kasten-K10AddOn Kasten K10 add-on installs Kasten K10 into your Amazon EKS cluster. KedaAddOn Installs Keda into EKS cluster. Keptn Keptn Control Plane and Execution Plane AddOn. KnativeAddOn Deploys the KNative Operator to ease setting up the rest of KNatives CRDs KubecostAddOn Adds Kubecost cost analyzer to the EKS cluster. KubeflowAddOn Adds kubeflow Kubeflow pipeline addon the EKS cluster. KubeviousAddOn Adds Kubevious open source Kubernetes dashboard to an EKS cluster. KarpenterAddOn Adds Karpenter support for Amazon EKS. KubeProxyAddOn Adds kube-proxy Amazon EKS add-on. Kube-proxy maintains network rules on each Amazon EC2 node. KubeStateMetricsAddOn Adds kube-state-metrics into the EKS cluster. MetricsServerAddOn Adds metrics server (pre-req for HPA and other monitoring tools). NewRelicAddOn Adds New Relic and Pixie observability for Amazon EKS. NginxAddOn Adds NGINX ingress controller OpaGatekeeperAddOn Adds OPA Gatekeeper PixieAddOn Adds Pixie to the EKS Cluster. Pixie provides auto-telemetry for requests, metrics, application profiles, and more. PrometheusNodeExporterAddOn Adds prometheus-node-exporter to the EKS Cluster. Prometheus Node Exporter enables you to measure various machine resources such as memory, disk and CPU utilization. SecretsStoreAddOn Adds AWS Secrets Manager and Config Provider for Secret Store CSI Driver to the EKS Cluster. Snyk Adds the Snyk Monitor to the EKS Cluster. SSMAgentAddOn Adds Amazon SSM Agent to worker nodes. UpboundUniversalCrossplaneAddOn Allows Elastic Kubernetes Service (Amazon EKS) clusters to manage the lifecycle of Crossplane distribution. VpcCniAddOn Adds the Amazon VPC CNI Amazon EKS addon to support native VPC networking for Amazon EKS. VeleroAddOn Adds Velero to the EKS Cluster. XrayAddOn Adds XRay Daemon to the EKS Cluster. XrayAdotAddOn Deploys ADOT Collector for Xray to receive traces from your workloads.","title":"Supported Add-ons"},{"location":"addons/#standard-helm-add-on-configuration-options","text":"Many add-ons leverage helm to provision and maintain deployments. All provided add-ons that leverage helm allow specifying the following add-on attributes: /** * Name of the helm chart (add-on) */ name? : string , /** * Namespace where helm release will be installed */ namespace? : string , /** * Chart name */ chart? : string , /** * Helm chart version. */ version? : string , /** * Helm release */ release? : string , /** * Helm repository */ repository? : string , /** * When global helm version validation is enabled with HelmAddOn.validateHelmVersions = true * allows to skip validation for a particular helm add-on. */ skipVersionValidation? : boolean , /** * Optional values for the helm chart. */ values? : Values Ability to set repository url may be leveraged for private repositories. Version field can be modified from the default chart version, e.g. if the add-on should be upgraded to the desired version, however, since the helm chart version supplied by the customer may not have been tested as part of the Blueprints release process, Blueprints community may not be able to reproduce/fix issues related to the helm chart version upgrade.","title":"Standard Helm Add-On Configuration Options"},{"location":"addons/#helm-version-validation","text":"All add-ons that derive from HelmAddOn support optional version validation against the latest published version in the target helm repository. Helm version validation can result either in a warning on console during list , synth and deploy operations or an exception if the target helm repository contains higher version than the one leveraged in the add-on. Example output: INFO Chart argo-cd-4.9.12 is at the latest version. INFO Chart external-dns-6.6.0 is at the latest version. WARN Upgrade is needed for chart gatekeeper-3.8.1: latest version is 3.9.0-beta.2. INFO Chart appmesh-controller-1.5.0 is at the latest version. INFO Chart tigera-operator-v3.23.2 is at the latest version. WARN Upgrade is needed for chart adot-exporter-for-eks-on-ec2-0.1.0: latest version is 0.6.0. INFO Chart aws-load-balancer-controller-1.4.2 is at the latest version. INFO Chart nginx-ingress-0.14.0 is at the latest version. INFO Chart velero-2.30.1 is at the latest version. INFO Chart falco-1.19.4 is at the latest version. WARN Upgrade is needed for chart karpenter-0.13.1: latest version is 0.13.2. INFO Chart kubevious-1.0.10 is at the latest version. INFO Chart aws-efs-csi-driver-2.2.7 is at the latest version. INFO Chart keda-2.7.2 is at the latest version. INFO Chart secrets-store-csi-driver-1.2.1 is at the latest version. Enable/Disable Helm version validation globally import { HelmAddOn } from '@aws-quickstart/eks-blueprints' ; HelmAddOn . validateHelmVersions = true ; // by default will print out warnings HelmAddOn . failOnVersionValidation = true ; // enable synth to throw exceptions on validation check failures Enable/Disable Helm version validation per add-on new blueprints . addons . MetricsServerAddOn ({ skipVersionValidation : true })","title":"Helm Version Validation"},{"location":"addons/ack-addon/","text":"AWS Controller for Kubernetes Add-on \u00b6 This add-on installs aws-controller-8s . AWS Controllers for Kubernetes (ACK) lets you define and use AWS service resources directly from Kubernetes. With ACK, you can take advantage of AWS managed services for your Kubernetes applications without needing to define resources outside of the cluster or run services that provide supporting capabilities like databases or message queues within the cluster. ACK is an open source project built with \u2764\ufe0f by AWS. The project is composed of many source code repositories containing a common runtime, a code generator, common testing tools and Kubernetes custom controllers for individual AWS service APIs. Usage \u00b6 Pattern # 1 : This installs AWS Controller for Kubernetes for IAM ACK Controller. This uses all default parameters for installation of the IAM Controller. import * as cdk from 'aws-cdk-lib' ; import * as blueprints from '@aws-quickstart/eks-blueprints' ; const app = new cdk . App (); const addOn = new blueprints . addons . AckAddOn ({ serviceName : AckServiceName.IAM , }), const blueprint = blueprints . EksBlueprint . builder () . addOns ( addOn ) . build ( app , 'my-stack-name' ); Pattern # 2 : This installs AWS Controller for Kubernetes for EC2 ACK controller using service name internally referencing service mapping values for helm options. After Installing this EC2 ACK Controller, the instructions in Provision ACK Resource can be used to provision EC2 namespaces SecurityGroup resources required for creating Amazon RDS database as an example. import * as cdk from 'aws-cdk-lib' ; import * as blueprints from '@aws-quickstart/eks-blueprints' ; const app = new cdk . App (); const addOn = new blueprints . addons . AckAddOn ({ id : \"ec2-ack\" , // Having this field is important if you are using multiple iterations of this Addon. createNamespace : false , //This is essential if you are using multiple iterations of this Addon to run in same namespace. serviceName : AckServiceName.EC2 // This value can be references from supported service section below, }), const blueprint = blueprints . EksBlueprint . builder () . addOns ( addOn ) . build ( app , 'my-stack-name' ); Pattern # 3 : This installs AWS Controller for Kubernetes for RDS ACK controller with user specified values. After Installing this RDS ACK Controller, the instructions in Provision ACK Resource can be used to provision Amazon RDS database using the RDS ACK controller as an example. import * as cdk from 'aws-cdk-lib' ; import * as blueprints from '@aws-quickstart/eks-blueprints' ; const app = new cdk . App (); const addOn = new blueprints . addons . AckAddOn ({ id : \"rds-ack\" , serviceName : AckServiceName.RDS , name : \"rds-chart\" , chart : \"rds-chart\" , version : \"v0.1.1\" , release : \"rds-chart\" , repository : \"oci://public.ecr.aws/aws-controllers-k8s/rds-chart\" , managedPolicyName : \"AmazonRDSFullAccess\" , createNamespace : false , saName : \"rds-chart\" }), const blueprint = blueprints . EksBlueprint . builder () . addOns ( addOn ) . build ( app , 'my-stack-name' ); Configuration Options \u00b6 id : Unique identifier of the Addon especially if you are using ACK Addon multiple times serviceName : Name of the service and this is mandatory name : Name of the ACK Chart chart : Chart Name of the ACK Chart version : Version of the ACK Chart release : Release Name of the ACK Chart repository : Repository URI of the specific ACK Chart managedPolicyName : Policy Name required to be added to the IAM role for that ACK createNamespace : (boolean) This should be false if you are using for the second time saName : Name to create the service account. values : Arbitrary values to pass to the chart Standard helm configuration options . Validation \u00b6 To validate that ack-controller-k8s is installed properly in the cluster, check if the namespace is created and pods are running in the ack-system namespace. Verify if the namespace is created correctly kubectl get all -n ack-system There should be list the following resources in the namespace NAME READY STATUS RESTARTS AGE pod/iam-chart-64c8fd7f6-wpb5k 1 /1 Running 0 34m pod/rds-chart-5f6f5b8fc7-hp55l 1 /1 Running 0 5m26s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/iam-chart 1 /1 1 1 35m deployment.apps/rds-chart 1 /1 1 1 5m36s NAME DESIRED CURRENT READY AGE replicaset.apps/iam-chart-64c8fd7f6 1 1 1 35m replicaset.apps/rds-chart-5f6f5b8fc7 1 1 1 5m36s aws-controller-8s references \u00b6 Please refer to following aws-controller-8s references for more information : - ACK Workshop - ECR Gallery for ACK - ACK GitHub Supported AWS Services by ACK Addon \u00b6 You can use this ACK Addon today to provision resources for below mentioned 22 AWS services: IAM RDS EC2 S3 DYNAMODB ECR SNS APIGATEWAYV2 ELASTICACHE OPENSEARCHSERVICE MQ LAMBDA KMS MEMORYDB EKS APPLICATIONAUTOSCALING ELASTICSEARCHSERVICE PROMETHEUSSERVICE EMRCONTAINERS SFN KINESIS CLOUDTRAIL ACM ROUTE53 SQS SAGEMAKER EVENTBRIDGE We highly recommend you to contribute to this ACK Addon whenever there is a newer service or new version of supported service by this Addon is published to ECR Gallery for ACK .","title":"ACK"},{"location":"addons/ack-addon/#aws-controller-for-kubernetes-add-on","text":"This add-on installs aws-controller-8s . AWS Controllers for Kubernetes (ACK) lets you define and use AWS service resources directly from Kubernetes. With ACK, you can take advantage of AWS managed services for your Kubernetes applications without needing to define resources outside of the cluster or run services that provide supporting capabilities like databases or message queues within the cluster. ACK is an open source project built with \u2764\ufe0f by AWS. The project is composed of many source code repositories containing a common runtime, a code generator, common testing tools and Kubernetes custom controllers for individual AWS service APIs.","title":"AWS Controller for Kubernetes Add-on"},{"location":"addons/ack-addon/#usage","text":"Pattern # 1 : This installs AWS Controller for Kubernetes for IAM ACK Controller. This uses all default parameters for installation of the IAM Controller. import * as cdk from 'aws-cdk-lib' ; import * as blueprints from '@aws-quickstart/eks-blueprints' ; const app = new cdk . App (); const addOn = new blueprints . addons . AckAddOn ({ serviceName : AckServiceName.IAM , }), const blueprint = blueprints . EksBlueprint . builder () . addOns ( addOn ) . build ( app , 'my-stack-name' ); Pattern # 2 : This installs AWS Controller for Kubernetes for EC2 ACK controller using service name internally referencing service mapping values for helm options. After Installing this EC2 ACK Controller, the instructions in Provision ACK Resource can be used to provision EC2 namespaces SecurityGroup resources required for creating Amazon RDS database as an example. import * as cdk from 'aws-cdk-lib' ; import * as blueprints from '@aws-quickstart/eks-blueprints' ; const app = new cdk . App (); const addOn = new blueprints . addons . AckAddOn ({ id : \"ec2-ack\" , // Having this field is important if you are using multiple iterations of this Addon. createNamespace : false , //This is essential if you are using multiple iterations of this Addon to run in same namespace. serviceName : AckServiceName.EC2 // This value can be references from supported service section below, }), const blueprint = blueprints . EksBlueprint . builder () . addOns ( addOn ) . build ( app , 'my-stack-name' ); Pattern # 3 : This installs AWS Controller for Kubernetes for RDS ACK controller with user specified values. After Installing this RDS ACK Controller, the instructions in Provision ACK Resource can be used to provision Amazon RDS database using the RDS ACK controller as an example. import * as cdk from 'aws-cdk-lib' ; import * as blueprints from '@aws-quickstart/eks-blueprints' ; const app = new cdk . App (); const addOn = new blueprints . addons . AckAddOn ({ id : \"rds-ack\" , serviceName : AckServiceName.RDS , name : \"rds-chart\" , chart : \"rds-chart\" , version : \"v0.1.1\" , release : \"rds-chart\" , repository : \"oci://public.ecr.aws/aws-controllers-k8s/rds-chart\" , managedPolicyName : \"AmazonRDSFullAccess\" , createNamespace : false , saName : \"rds-chart\" }), const blueprint = blueprints . EksBlueprint . builder () . addOns ( addOn ) . build ( app , 'my-stack-name' );","title":"Usage"},{"location":"addons/ack-addon/#configuration-options","text":"id : Unique identifier of the Addon especially if you are using ACK Addon multiple times serviceName : Name of the service and this is mandatory name : Name of the ACK Chart chart : Chart Name of the ACK Chart version : Version of the ACK Chart release : Release Name of the ACK Chart repository : Repository URI of the specific ACK Chart managedPolicyName : Policy Name required to be added to the IAM role for that ACK createNamespace : (boolean) This should be false if you are using for the second time saName : Name to create the service account. values : Arbitrary values to pass to the chart Standard helm configuration options .","title":"Configuration Options"},{"location":"addons/ack-addon/#validation","text":"To validate that ack-controller-k8s is installed properly in the cluster, check if the namespace is created and pods are running in the ack-system namespace. Verify if the namespace is created correctly kubectl get all -n ack-system There should be list the following resources in the namespace NAME READY STATUS RESTARTS AGE pod/iam-chart-64c8fd7f6-wpb5k 1 /1 Running 0 34m pod/rds-chart-5f6f5b8fc7-hp55l 1 /1 Running 0 5m26s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/iam-chart 1 /1 1 1 35m deployment.apps/rds-chart 1 /1 1 1 5m36s NAME DESIRED CURRENT READY AGE replicaset.apps/iam-chart-64c8fd7f6 1 1 1 35m replicaset.apps/rds-chart-5f6f5b8fc7 1 1 1 5m36s","title":"Validation"},{"location":"addons/ack-addon/#aws-controller-8s-references","text":"Please refer to following aws-controller-8s references for more information : - ACK Workshop - ECR Gallery for ACK - ACK GitHub","title":"aws-controller-8s references"},{"location":"addons/ack-addon/#supported-aws-services-by-ack-addon","text":"You can use this ACK Addon today to provision resources for below mentioned 22 AWS services: IAM RDS EC2 S3 DYNAMODB ECR SNS APIGATEWAYV2 ELASTICACHE OPENSEARCHSERVICE MQ LAMBDA KMS MEMORYDB EKS APPLICATIONAUTOSCALING ELASTICSEARCHSERVICE PROMETHEUSSERVICE EMRCONTAINERS SFN KINESIS CLOUDTRAIL ACM ROUTE53 SQS SAGEMAKER EVENTBRIDGE We highly recommend you to contribute to this ACK Addon whenever there is a newer service or new version of supported service by this Addon is published to ECR Gallery for ACK .","title":"Supported AWS Services by ACK Addon"},{"location":"addons/adot-addon/","text":"AWS Distro for OpenTelemetry (ADOT) Add-on \u00b6 Amazon EKS supports using Amazon EKS API to install and manage the AWS Distro for OpenTelemetry (ADOT) Operator. This enables a simplified experience for instrumenting your applications running on Amazon EKS to send metric and trace data to multiple monitoring service options like Amazon CloudWatch, Prometheus, and X-Ray. This add-on is not automatically installed when you first create a cluster, it must be added to the cluster in order to manage ADOT Collectors. For more information on the add-on, please review the user guide . Prerequisites \u00b6 The ADOT Operator uses admission webhooks to mutate and validate the Collector Custom Resource (CR) requests. In Kubernetes, the webhook requires a TLS certificate that the API server is configured to trust. There are multiple ways for you to generate the required TLS certificate. However, the default method is to install the latest version of the cert-manager. You can install Certificate Manager using this user guide or you can use certificate-manager EKS blueprints addon which should be added before ADOT addon. Usage \u00b6 import * as cdk from 'aws-cdk-lib' ; import * as blueprints from '@aws-quickstart/eks-blueprints' ; const app = new cdk . App (); const addOn = new blueprints . addons . AdotCollectorAddOn (); const blueprint = blueprints . EksBlueprint . builder () . addOns ( addOn ) . build ( app , 'my-stack-name' ); Validation \u00b6 To validate that ADOT add-on is installed properly, ensure that the ADOT kubernetes resources are running in the cluster kubectl get all -n opentelemetry-operator-system Output \u00b6 NAME READY STATUS RESTARTS AGE pod/opentelemetry-operator-controller-manager-845cbd7bf7-b5s9l 2 /2 Running 0 140m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE service/opentelemetry-operator-controller-manager-metrics-service ClusterIP 172 .20.210.200 <none> 8443 /TCP 140m service/opentelemetry-operator-webhook-service ClusterIP 172 .20.56.72 <none> 443 /TCP 140m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/opentelemetry-operator-controller-manager 1 /1 1 1 140m NAME DESIRED CURRENT READY AGE replicaset.apps/opentelemetry-operator-controller-manager-845cbd7bf7 1 1 1 140m Testing \u00b6 Additionally, the aws cli can be used to determine which version of the add-on is installed in the cluster. # Assuming cluster-name is my-cluster, below command shows the version of coredns installed. Check if it is same as the version installed via EKS add-on aws eks describe-addon \\ --cluster-name my-cluster \\ --addon-name adot \\ --query \"addon.addonVersion\" \\ --output text # Output v0.51.0-eksbuild.1 Functionality \u00b6 Applies the ADOT add-on to an Amazon EKS cluster.","title":"ADOT"},{"location":"addons/adot-addon/#aws-distro-for-opentelemetry-adot-add-on","text":"Amazon EKS supports using Amazon EKS API to install and manage the AWS Distro for OpenTelemetry (ADOT) Operator. This enables a simplified experience for instrumenting your applications running on Amazon EKS to send metric and trace data to multiple monitoring service options like Amazon CloudWatch, Prometheus, and X-Ray. This add-on is not automatically installed when you first create a cluster, it must be added to the cluster in order to manage ADOT Collectors. For more information on the add-on, please review the user guide .","title":"AWS Distro for OpenTelemetry (ADOT) Add-on"},{"location":"addons/adot-addon/#prerequisites","text":"The ADOT Operator uses admission webhooks to mutate and validate the Collector Custom Resource (CR) requests. In Kubernetes, the webhook requires a TLS certificate that the API server is configured to trust. There are multiple ways for you to generate the required TLS certificate. However, the default method is to install the latest version of the cert-manager. You can install Certificate Manager using this user guide or you can use certificate-manager EKS blueprints addon which should be added before ADOT addon.","title":"Prerequisites"},{"location":"addons/adot-addon/#usage","text":"import * as cdk from 'aws-cdk-lib' ; import * as blueprints from '@aws-quickstart/eks-blueprints' ; const app = new cdk . App (); const addOn = new blueprints . addons . AdotCollectorAddOn (); const blueprint = blueprints . EksBlueprint . builder () . addOns ( addOn ) . build ( app , 'my-stack-name' );","title":"Usage"},{"location":"addons/adot-addon/#validation","text":"To validate that ADOT add-on is installed properly, ensure that the ADOT kubernetes resources are running in the cluster kubectl get all -n opentelemetry-operator-system","title":"Validation"},{"location":"addons/adot-addon/#output","text":"NAME READY STATUS RESTARTS AGE pod/opentelemetry-operator-controller-manager-845cbd7bf7-b5s9l 2 /2 Running 0 140m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE service/opentelemetry-operator-controller-manager-metrics-service ClusterIP 172 .20.210.200 <none> 8443 /TCP 140m service/opentelemetry-operator-webhook-service ClusterIP 172 .20.56.72 <none> 443 /TCP 140m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/opentelemetry-operator-controller-manager 1 /1 1 1 140m NAME DESIRED CURRENT READY AGE replicaset.apps/opentelemetry-operator-controller-manager-845cbd7bf7 1 1 1 140m","title":"Output"},{"location":"addons/adot-addon/#testing","text":"Additionally, the aws cli can be used to determine which version of the add-on is installed in the cluster. # Assuming cluster-name is my-cluster, below command shows the version of coredns installed. Check if it is same as the version installed via EKS add-on aws eks describe-addon \\ --cluster-name my-cluster \\ --addon-name adot \\ --query \"addon.addonVersion\" \\ --output text # Output v0.51.0-eksbuild.1","title":"Testing"},{"location":"addons/adot-addon/#functionality","text":"Applies the ADOT add-on to an Amazon EKS cluster.","title":"Functionality"},{"location":"addons/amp-addon/","text":"Amazon Managed Service for Prometheus (AMP) ADOT Add-on \u00b6 Amazon Managed Service for Prometheus is a Prometheus-compatible monitoring and alerting service that makes it easy to monitor containerized applications and infrastructure at scale. This add-on deploys an AWS Distro for OpenTelemetry (ADOT) Collector for Amazon Managed Service for Prometheus (AMP) which receives OTLP metrics from the application and Prometheus metrics scraped from pods on the cluster and remote writes the metrics to AMP remote write endpoint. You can change the mode to Daemonset, StatefulSet, and Sidecar depending upon your deployment strategy. This add-on is not automatically installed when you first create a cluster, it must be added to the cluster in order to setup AMP for remote write of metrics. For more information on the add-on, please review the user guide . please review the Amazon Managed Service for Prometheus supported regions documentation page for more information. Prerequisites \u00b6 adot EKS Blueprints add-on. kube-state-metrics EKS Blueprints add-on. prometheus-node-explorter EKS Blueprints add-on. Usage \u00b6 This add-on can used with four different patterns : Pattern # 1 : Simple and Easy - Using all default property values. This pattern creates a new AMP workspace with default property values such as workspaceName , namespace with no tags on the AMP workspace and deploys an ADOT collector in the default namespace with deployment as the mode to remote write metrics to AMP workspace. import * as cdk from 'aws-cdk-lib' ; import * as blueprints from '@aws-quickstart/eks-blueprints' ; const app = new cdk . App (); const addOn = new blueprints . addons . AmpAddOn (); const blueprint = blueprints . EksBlueprint . builder () . addOns ( addOn ) . build ( app , 'my-stack-name' ); Pattern # 2 : Overriding property values for Name and Tags for a custom AMP Workspace name and tags. This pattern creates a new AMP workspace with property values passed on such as workspaceName , tags and deploys an ADOT collector on the namespace specified in namespace with name in name and deployment as the mode to remote write metrics to AMP workspace. import * as cdk from 'aws-cdk-lib' ; import * as blueprints from '@aws-quickstart/eks-blueprints' ; const app = new cdk . App (); const addOn = new blueprints . addons . AmpAddOn ({ workspaceName : 'sample-AMP-Workspace' , workspaceTag : [{ key : 'Name' , value : 'Sample-AMP-Workspace' , }, { key : 'Environment' , value : 'Development' , }, { key : 'Department' , value : 'Operations' , }], deploymentMode : DeploymentMode.DEPLOYMENT , namespace : 'default' , name : 'adot-collector-amp' }) const blueprint = blueprints . EksBlueprint . builder () . addOns ( addOn ) . build ( app , 'my-stack-name' ); Pattern # 3 : Passing on AMP Remote Write Endpoint URL of an existing AMP workspace to be used to remote write metrics. This pattern does not create an AMP workspace. Deploys an ADOT collector on the namespace specified in namespace with name in name and deployment as the mode to remote write metrics to AMP workspace of the URL passed as input. This pattern ignores any other property values passed if prometheusRemoteWriteURL is present. import * as cdk from 'aws-cdk-lib' ; import * as blueprints from '@aws-quickstart/eks-blueprints' ; const app = new cdk . App (); const addOn = new blueprints . addons . AmpAddOn ({ prometheusRemoteWriteURL : 'https://aps-workspaces.us-west-2.amazonaws.com/workspaces/ws-e859f589-7eed-43c1-a82b-58f44119f17d/api/v1/remote_write' , deploymentMode : DeploymentMode.DEPLOYMENT , namespace : 'default' , name : 'adot-collector-amp' }) const blueprint = blueprints . EksBlueprint . builder () . addOns ( addOn ) . build ( app , 'my-stack-name' ); Pattern # 4 : Overriding property values for different deployment Modes. This pattern creates a new AMP workspace with property values passed on such as workspaceName , tags and deploys an ADOT collector on the namespace specified in namespace with name in name and daemonset as the mode to remote write metrics to AMP workspace. Deployment modes can be overridden to any of these values - deployment , daemonset , statefulset , sidecar . import * as cdk from 'aws-cdk-lib' ; import * as blueprints from '@aws-quickstart/eks-blueprints' ; const app = new cdk . App (); const addOn = new blueprints . addons . AmpAddOn ({ workspaceName : 'sample-AMP-Workspace' , workspaceTag : [{ key : 'Name' , value : 'Sample-AMP-Workspace' , }, { key : 'Environment' , value : 'Development' , }, { key : 'Department' , value : 'Operations' , }], deploymentMode : DeploymentMode.DAEMONSET , namespace : 'default' , name : 'adot-collector-amp' // deploymentMode: DeploymentMode.DEPLOYMENT // deploymentMode: DeploymentMode.STATEFULSET // deploymentMode: DeploymentMode.SIDECAR }) const blueprint = blueprints . EksBlueprint . builder () . addOns ( addOn ) . build ( app , 'my-stack-name' ); Validation \u00b6 To validate that AMP add-on is installed properly, ensure that the required kubernetes resources are running in the cluster: kubectl get all -n default NAME READY STATUS RESTARTS AGE pod/otel-collector-amp-collector-7877b86dd4-z9ds5 1 /1 Running 0 31m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE service/kubernetes ClusterIP 172 .20.0.1 <none> 443 /TCP 4h35m service/otel-collector-amp-collector-monitoring ClusterIP 172 .20.216.242 <none> 8888 /TCP 31m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/otel-collector-amp-collector 1 /1 1 1 31m NAME DESIRED CURRENT READY AGE replicaset.apps/otel-collector-amp-collector-7877b86dd4 1 1 1 31m Testing \u00b6 To test whether Amazon Managed Service for Prometheus received the metrics, Please use the following commands: For instructions on installing awscurl, see awscurl . AMP_WORKSPACE_NAME = \"sample-AMP-Workspace\" # The above should be replaced with your AMP workspace name if you are passing remote write URL specified in Pattern #3. WORKSPACE_ID = $( aws amp list-workspaces \\ --alias $AMP_WORKSPACE_NAME --region = ${ AWS_REGION } --query 'workspaces[0].[workspaceId]' --output text ) AMP_ENDPOINT_QUERY = https://aps-workspaces. $AWS_REGION .amazonaws.com/workspaces/ $WORKSPACE_ID /api/v1/query \\? query = awscurl --service = \"aps\" --region = ${ AWS_REGION } ${ AMP_ENDPOINT_QUERY } up Functionality \u00b6 Applies the Amazon Managed Service for Prometheus (AMP) add-on to an Amazon EKS cluster.","title":"AMP ADOT"},{"location":"addons/amp-addon/#amazon-managed-service-for-prometheus-amp-adot-add-on","text":"Amazon Managed Service for Prometheus is a Prometheus-compatible monitoring and alerting service that makes it easy to monitor containerized applications and infrastructure at scale. This add-on deploys an AWS Distro for OpenTelemetry (ADOT) Collector for Amazon Managed Service for Prometheus (AMP) which receives OTLP metrics from the application and Prometheus metrics scraped from pods on the cluster and remote writes the metrics to AMP remote write endpoint. You can change the mode to Daemonset, StatefulSet, and Sidecar depending upon your deployment strategy. This add-on is not automatically installed when you first create a cluster, it must be added to the cluster in order to setup AMP for remote write of metrics. For more information on the add-on, please review the user guide . please review the Amazon Managed Service for Prometheus supported regions documentation page for more information.","title":"Amazon Managed Service for Prometheus (AMP) ADOT Add-on"},{"location":"addons/amp-addon/#prerequisites","text":"adot EKS Blueprints add-on. kube-state-metrics EKS Blueprints add-on. prometheus-node-explorter EKS Blueprints add-on.","title":"Prerequisites"},{"location":"addons/amp-addon/#usage","text":"This add-on can used with four different patterns : Pattern # 1 : Simple and Easy - Using all default property values. This pattern creates a new AMP workspace with default property values such as workspaceName , namespace with no tags on the AMP workspace and deploys an ADOT collector in the default namespace with deployment as the mode to remote write metrics to AMP workspace. import * as cdk from 'aws-cdk-lib' ; import * as blueprints from '@aws-quickstart/eks-blueprints' ; const app = new cdk . App (); const addOn = new blueprints . addons . AmpAddOn (); const blueprint = blueprints . EksBlueprint . builder () . addOns ( addOn ) . build ( app , 'my-stack-name' ); Pattern # 2 : Overriding property values for Name and Tags for a custom AMP Workspace name and tags. This pattern creates a new AMP workspace with property values passed on such as workspaceName , tags and deploys an ADOT collector on the namespace specified in namespace with name in name and deployment as the mode to remote write metrics to AMP workspace. import * as cdk from 'aws-cdk-lib' ; import * as blueprints from '@aws-quickstart/eks-blueprints' ; const app = new cdk . App (); const addOn = new blueprints . addons . AmpAddOn ({ workspaceName : 'sample-AMP-Workspace' , workspaceTag : [{ key : 'Name' , value : 'Sample-AMP-Workspace' , }, { key : 'Environment' , value : 'Development' , }, { key : 'Department' , value : 'Operations' , }], deploymentMode : DeploymentMode.DEPLOYMENT , namespace : 'default' , name : 'adot-collector-amp' }) const blueprint = blueprints . EksBlueprint . builder () . addOns ( addOn ) . build ( app , 'my-stack-name' ); Pattern # 3 : Passing on AMP Remote Write Endpoint URL of an existing AMP workspace to be used to remote write metrics. This pattern does not create an AMP workspace. Deploys an ADOT collector on the namespace specified in namespace with name in name and deployment as the mode to remote write metrics to AMP workspace of the URL passed as input. This pattern ignores any other property values passed if prometheusRemoteWriteURL is present. import * as cdk from 'aws-cdk-lib' ; import * as blueprints from '@aws-quickstart/eks-blueprints' ; const app = new cdk . App (); const addOn = new blueprints . addons . AmpAddOn ({ prometheusRemoteWriteURL : 'https://aps-workspaces.us-west-2.amazonaws.com/workspaces/ws-e859f589-7eed-43c1-a82b-58f44119f17d/api/v1/remote_write' , deploymentMode : DeploymentMode.DEPLOYMENT , namespace : 'default' , name : 'adot-collector-amp' }) const blueprint = blueprints . EksBlueprint . builder () . addOns ( addOn ) . build ( app , 'my-stack-name' ); Pattern # 4 : Overriding property values for different deployment Modes. This pattern creates a new AMP workspace with property values passed on such as workspaceName , tags and deploys an ADOT collector on the namespace specified in namespace with name in name and daemonset as the mode to remote write metrics to AMP workspace. Deployment modes can be overridden to any of these values - deployment , daemonset , statefulset , sidecar . import * as cdk from 'aws-cdk-lib' ; import * as blueprints from '@aws-quickstart/eks-blueprints' ; const app = new cdk . App (); const addOn = new blueprints . addons . AmpAddOn ({ workspaceName : 'sample-AMP-Workspace' , workspaceTag : [{ key : 'Name' , value : 'Sample-AMP-Workspace' , }, { key : 'Environment' , value : 'Development' , }, { key : 'Department' , value : 'Operations' , }], deploymentMode : DeploymentMode.DAEMONSET , namespace : 'default' , name : 'adot-collector-amp' // deploymentMode: DeploymentMode.DEPLOYMENT // deploymentMode: DeploymentMode.STATEFULSET // deploymentMode: DeploymentMode.SIDECAR }) const blueprint = blueprints . EksBlueprint . builder () . addOns ( addOn ) . build ( app , 'my-stack-name' );","title":"Usage"},{"location":"addons/amp-addon/#validation","text":"To validate that AMP add-on is installed properly, ensure that the required kubernetes resources are running in the cluster: kubectl get all -n default NAME READY STATUS RESTARTS AGE pod/otel-collector-amp-collector-7877b86dd4-z9ds5 1 /1 Running 0 31m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE service/kubernetes ClusterIP 172 .20.0.1 <none> 443 /TCP 4h35m service/otel-collector-amp-collector-monitoring ClusterIP 172 .20.216.242 <none> 8888 /TCP 31m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/otel-collector-amp-collector 1 /1 1 1 31m NAME DESIRED CURRENT READY AGE replicaset.apps/otel-collector-amp-collector-7877b86dd4 1 1 1 31m","title":"Validation"},{"location":"addons/amp-addon/#testing","text":"To test whether Amazon Managed Service for Prometheus received the metrics, Please use the following commands: For instructions on installing awscurl, see awscurl . AMP_WORKSPACE_NAME = \"sample-AMP-Workspace\" # The above should be replaced with your AMP workspace name if you are passing remote write URL specified in Pattern #3. WORKSPACE_ID = $( aws amp list-workspaces \\ --alias $AMP_WORKSPACE_NAME --region = ${ AWS_REGION } --query 'workspaces[0].[workspaceId]' --output text ) AMP_ENDPOINT_QUERY = https://aps-workspaces. $AWS_REGION .amazonaws.com/workspaces/ $WORKSPACE_ID /api/v1/query \\? query = awscurl --service = \"aps\" --region = ${ AWS_REGION } ${ AMP_ENDPOINT_QUERY } up","title":"Testing"},{"location":"addons/amp-addon/#functionality","text":"Applies the Amazon Managed Service for Prometheus (AMP) add-on to an Amazon EKS cluster.","title":"Functionality"},{"location":"addons/app-mesh/","text":"AWS App Mesh Add-on \u00b6 AWS App Mesh is a service mesh that provides application-level networking to make it easy for your services to communicate with each other across multiple types of compute infrastructure. The App Mesh add-on provisions the necessary AWS resources and Helm charts into an EKS cluster that are needed to support App Mesh for EKS workloads. Full documentation on using App Mesh with EKS can be found here . Usage \u00b6 import 'source-map-support/register' ; import * as cdk from 'aws-cdk-lib' ; import * as blueprints from '@aws-quickstart/eks-blueprints' ; const app = new cdk . App (); const addOn = new blueprints . addons . AppMeshAddOn (); const blueprint = blueprints . EksBlueprint . builder () . addOns ( addOn ) . build ( app , 'my-stack-name' ); Functionality \u00b6 Creates an App Mesh IAM service account. Adds both AWSCloudMapFullAccess and AWSAppMeshFullAccess roles to the service account. Adds AWSXRayDaemonWriteAccess to the instance role if XRay integration is enabled. Creates the appmesh-system namespace. Deploys the appmesh-controller Helm chart into the cluster. Supports standard helm configuration options . App Mesh Sidecar Injection \u00b6 You can configure certain namespaces for automatic injection of App Mesh sidecar (Envoy) proxy. This will enable handling cross-cutting aspects such as service to service communication, resiliency patterns (circuit breaker/retries) as well handle ingress and egress for the workloads running in the namespace. Here is an example of a team with a namespace configured for automatic sidecar injection: export class TeamBurnham extends ApplicationTeam { constructor ( scope : Construct ) { super ({ name : \"burnham\" , users : getUserArns ( scope , \"team-burnham.users\" ), namespaceAnnotations : { \"appmesh.k8s.aws/sidecarInjectorWebhook\" : \"enabled\" } }); } } Tracing Integration \u00b6 App Mesh integrates with a number of tracing providers for distributed tracing support. At the moment it supports AWS X-Ray, Jaeger, and Datadog providers. The X-Ray integration at present requires either a managed node group or a self-managed auto-scaling group backed by EC2. Fargate is not supported. Enabling integration: const appMeshAddOn = new blueprints . AppMeshAddOn ({ enableTracing : true , tracingProvider : \"x-ray\" }), When configured, App Mesh will automatically inject an XRay sidecar to handle tracing which enables troubleshooting latency issues. App Mesh and XRay Integration Example \u00b6 team-burnham sample workload repository is configured with an example workload that demonstrates a \"meshified\" workload. After the workload is deployed with ArgoCD or applied directly to the cluster, a DJ application will be created in the team-burnham namespace, similar to the one used for the EKS Workshop . It was adapted for GitOps integration with Blueprints and relies on automatic sidecar injection as well as tracing integration with App Mesh. After the workload is deployed you can generate some traffic to populated traces: $ export DJ_POD_NAME = $( kubectl get pods -n team-burnham -l app = dj -o jsonpath = '{.items[].metadata.name}' ) $ kubectl -n team-burnham exec -it ${ DJ_POD_NAME } -c dj bash $ while true ; do curl http://jazz.team-burnham.svc.cluster.local:9080/ echo curl http://metal.team-burnham.svc.cluster.local:9080/ echo done The above script will start producing load which will generate traces with XRay. Once traces are produced (for a minute or more) you can navigate to the AWS XRay console and click on Service Map. You will see a screenshot similar to this:","title":"AWS App Mesh"},{"location":"addons/app-mesh/#aws-app-mesh-add-on","text":"AWS App Mesh is a service mesh that provides application-level networking to make it easy for your services to communicate with each other across multiple types of compute infrastructure. The App Mesh add-on provisions the necessary AWS resources and Helm charts into an EKS cluster that are needed to support App Mesh for EKS workloads. Full documentation on using App Mesh with EKS can be found here .","title":"AWS App Mesh Add-on"},{"location":"addons/app-mesh/#usage","text":"import 'source-map-support/register' ; import * as cdk from 'aws-cdk-lib' ; import * as blueprints from '@aws-quickstart/eks-blueprints' ; const app = new cdk . App (); const addOn = new blueprints . addons . AppMeshAddOn (); const blueprint = blueprints . EksBlueprint . builder () . addOns ( addOn ) . build ( app , 'my-stack-name' );","title":"Usage"},{"location":"addons/app-mesh/#functionality","text":"Creates an App Mesh IAM service account. Adds both AWSCloudMapFullAccess and AWSAppMeshFullAccess roles to the service account. Adds AWSXRayDaemonWriteAccess to the instance role if XRay integration is enabled. Creates the appmesh-system namespace. Deploys the appmesh-controller Helm chart into the cluster. Supports standard helm configuration options .","title":"Functionality"},{"location":"addons/app-mesh/#app-mesh-sidecar-injection","text":"You can configure certain namespaces for automatic injection of App Mesh sidecar (Envoy) proxy. This will enable handling cross-cutting aspects such as service to service communication, resiliency patterns (circuit breaker/retries) as well handle ingress and egress for the workloads running in the namespace. Here is an example of a team with a namespace configured for automatic sidecar injection: export class TeamBurnham extends ApplicationTeam { constructor ( scope : Construct ) { super ({ name : \"burnham\" , users : getUserArns ( scope , \"team-burnham.users\" ), namespaceAnnotations : { \"appmesh.k8s.aws/sidecarInjectorWebhook\" : \"enabled\" } }); } }","title":"App Mesh Sidecar Injection"},{"location":"addons/app-mesh/#tracing-integration","text":"App Mesh integrates with a number of tracing providers for distributed tracing support. At the moment it supports AWS X-Ray, Jaeger, and Datadog providers. The X-Ray integration at present requires either a managed node group or a self-managed auto-scaling group backed by EC2. Fargate is not supported. Enabling integration: const appMeshAddOn = new blueprints . AppMeshAddOn ({ enableTracing : true , tracingProvider : \"x-ray\" }), When configured, App Mesh will automatically inject an XRay sidecar to handle tracing which enables troubleshooting latency issues.","title":"Tracing Integration"},{"location":"addons/app-mesh/#app-mesh-and-xray-integration-example","text":"team-burnham sample workload repository is configured with an example workload that demonstrates a \"meshified\" workload. After the workload is deployed with ArgoCD or applied directly to the cluster, a DJ application will be created in the team-burnham namespace, similar to the one used for the EKS Workshop . It was adapted for GitOps integration with Blueprints and relies on automatic sidecar injection as well as tracing integration with App Mesh. After the workload is deployed you can generate some traffic to populated traces: $ export DJ_POD_NAME = $( kubectl get pods -n team-burnham -l app = dj -o jsonpath = '{.items[].metadata.name}' ) $ kubectl -n team-burnham exec -it ${ DJ_POD_NAME } -c dj bash $ while true ; do curl http://jazz.team-burnham.svc.cluster.local:9080/ echo curl http://metal.team-burnham.svc.cluster.local:9080/ echo done The above script will start producing load which will generate traces with XRay. Once traces are produced (for a minute or more) you can navigate to the AWS XRay console and click on Service Map. You will see a screenshot similar to this:","title":"App Mesh and XRay Integration Example"},{"location":"addons/argo-cd/","text":"Argo CD Add-on \u00b6 Argo CD is a declarative, GitOps continuous delivery tool for Kubernetes. The Argo CD add-on provisions Argo CD into an EKS cluster, and can optionally bootstrap your workloads from public and private Git repositories. The Argo CD add-on allows platform administrators to combine cluster provisioning and workload bootstrapping in a single step and enables use cases such as replicating an existing running production cluster in a different region in a matter of minutes. This is important for business continuity and disaster recovery cases as well as for cross-regional availability and geographical expansion. Please see the documentation below for details on automatic boostrapping with ArgoCD add-on. If you prefer manual bootstrapping (once your cluster is deployed with this add-on included), you can find instructions on getting started with Argo CD in our Getting Started guide. Full Argo CD project documentation can be found here . Usage \u00b6 To provision and maintain ArgoCD components without any bootstrapping, the add-on provides a no-argument constructor to get started. import 'source-map-support/register' ; import * as cdk from 'aws-cdk-lib' ; import * as blueprints from '@aws-quickstart/eks-blueprints' ; const app = new cdk . App (); const addOn = new blueprints . addons . ArgoCDAddOn (); const blueprint = blueprints . EksBlueprint . builder () . addOns ( addOn ) . build ( app , 'my-stack-name' ); The above will create an argocd namespace and install all Argo CD components. In order to bootstrap workloads you will need to change the default ArgoCD admin password and add repositories as specified in the Getting Started documentation. Functionality \u00b6 Creates the namespace specified in the construction parameter ( argocd by default). Deploys the argo-cd Helm chart into the cluster. Allows to specify ApplicationRepository selecting the required authentication method as SSH Key, username/password or username/token. Credentials are expected to be set in AWS Secrets Manager and replicated to the desired region. If bootstrap repository is specified, creates the initial bootstrap application which may be leveraged to bootstrap workloads and/or other add-ons through GitOps. Allows setting the initial admin password through AWS Secrets Manager, replicating to the desired region. Supports standard helm configuration options . Setting an Admin Password \u00b6 By default, the Argo CD add-on will create a new admin password for you. To specify your own, you can leverage the AWS Secrets Manager. const argoCDAddOn = new ArgoCDAddOn ({ adminPasswordSecretName : `your-secret-name` }); const addOns : Array < ClusterAddOn > = [ argoCDAddOn ]; The attribute adminPasswordSecretName is the logical name of the secret in AWS Secret Manager . Note, that when deploying to multiple regions, the secret is expected to be replicated to each region. Inside ArgoCD, the admin password is stored as a bcrypt hash. This step will be performed by the framework and stored in the ArgoCD admin secret . You can change the admin password through the Secrets Manager, but it will require rerunning the provisioning pipeline to apply the change. Bootstrapping \u00b6 The Blueprints framework provides an approach to bootstrap workloads and/or additional add-ons from a customer GitOps repository. In a general case, the bootstrap GitOps repository may contains an App of Apps that points to all workloads and add-ons. In order to enable bootstrapping, the add-on allows passing an ApplicationRepository at construction time. The following repository types are supported at present: Public HTTP/HTTPS repositories (e.g., GitHub) Private HTTPS accessible git repositories requiring username/password authentication. Private git repositories with SSH access requiring an SSH key for authentication. Private HTTPS accessible GitHub repositories accessible with GitHub token. An example is provided below, along with an approach that could use a separate app of apps to bootstrap workloads in different stages, which is important for a software delivery platform as it allows segregating workloads specific to each stage of the SDLC and defines clear promotion processes through GitOps. import 'source-map-support/register' ; import * as cdk from 'aws-cdk-lib' ; import { ArgoCDAddOn , ClusterAddOn , EksBlueprint , ApplicationRepository } from '@aws-quickstart/eks-blueprints' ; const secretStoreAddOn = new SecretsStoreAddOn (); const repoUrl = 'git@github.com:aws-samples/eks-blueprints-workloads.git' const bootstrapRepo : ApplicationRepository = { repoUrl , credentialsSecretName : 'github-ssh-test' , credentialsType : 'SSH' } const devBootstrapArgo = new ArgoCDAddOn ({ bootstrapRepo : { ... bootstrapRepo , path : 'envs/dev' } }); const testBootstrapArgo = new ArgoCDAddOn ({ bootstrapRepo : { ... bootstrapRepo , path : 'envs/test' , }, }); const prodBootstrapArgo = new ArgoCDAddOn ({ bootstrapRepo : { ... bootstrapRepo , path : 'envs/prod' , }, adminPasswordSecretName : 'ArgoCDAdmin' , }); const blueprint = EksBlueprint . builder () . addOns ( secretStoreAddOn ) . account ( account ); blueprint . clone ( 'us-east-1' ) . addOns ( devBootstrapArgo ) . build ( app , 'argo-us-east-1' ); blueprint . clone ( 'us-east-2' ) . addOns ( testBootstrapArgo ) . build ( app , 'argo-us-east-2' ); blueprint . clone ( 'us-west-2' ) . addOns ( prodBootstrapArgo ) . build ( app , 'argo-us-west-1' ); The application promotion process in the above example is handled entirely through GitOps. Each stage specific App of Apps contains references to respective application GitOps repository for each stage (e.g referencing the release vs work branches or path-based within individual app GitOps repository). GitOps for EKS Blueprints AddOns \u00b6 By default all AddOns defined in a blueprint are deployed to the cluster via CDK. You can opt-in to deploy them following the GitOps model via ArgoCD. You will need a repository contains all the AddOns you would like to deploy via ArgoCD, such as, eks-blueprints-add-ons . You then configure ArgoCD bootstrapping with this repository as shown above. There are two types of GitOps deployments via ArgoCD depending on whether you would like to adopt the App of Apps strategy: CDK deploys the Application resource for each AddOn enabled, and ArgoCD deploys the actual AddOn via GitOps based on the Application resource. Example: import * as blueprints from '@aws-quickstart/eks-blueprints' ; // enable gitops bootstrapping with argocd const prodBootstrapArgo = new blueprints . addons . ArgoCDAddOn ({ bootstrapRepo : { repoUrl : 'https://github.com/aws-samples/eks-blueprints-add-ons' , path : 'add-ons' , targetRevision : \"main\" , }, }); // addons const addOns : Array < blueprints . ClusterAddOn > = [ prodBootstrapArgo , new blueprints . addons . AppMeshAddOn (), new blueprints . addons . MetricsServerAddOn (), new blueprints . addons . AwsLoadBalancerControllerAddOn (), ]; blueprints . EksBlueprint . builder () . addOns (... addOns ) . enableGitOps ( blueprints . GitOpsMode . APPLICATION ) . build ( scope , stackID ); CDK deploys the only Application resource for the App of Apps, aka bootstrap-apps , and ArgoCD deploys all the AddOns based on the bootstrap-apps . This requires the naming pattern between the AddOns and App of Apps matches. Example: import * as blueprints from '@aws-quickstart/eks-blueprints' ; // enable gitops bootstrapping with argocd app of apps const prodBootstrapArgo = new blueprints . addons . ArgoCDAddOn ({ bootstrapRepo : { repoUrl : 'https://github.com/aws-samples/eks-blueprints-add-ons' , path : 'chart' , targetRevision : \"main\" , }, }); // addons const addOns : Array < blueprints . ClusterAddOn > = [ prodBootstrapArgo , new blueprints . addons . AppMeshAddOn (), new blueprints . addons . MetricsServerAddOn (), new blueprints . addons . AwsLoadBalancerControllerAddOn (), ]; blueprints . EksBlueprint . builder () . addOns (... addOns ) . enableGitOps ( blueprints . GitOpsMode . APP_OF_APPS ) . build ( scope , stackID ); Secrets Support \u00b6 The framework provides support to supply repository and administrator secrets in AWS Secrets Manager. This support is evolving and will be improved over time as ArgoCD itself matures. Private Repositories \u00b6 SSH Key Authentication Set credentialsType to SSH when defining bootstrap repository in the ArgoCD add-on configuration. . addOns ( new blueprints . addons . ArgoCDAddOn ({ bootstrapRepo : { repoUrl : 'git@github.com:aws-samples/eks-blueprints-workloads.git' , path : 'envs/dev' , credentialsSecretName : 'github-ssh-json' , credentialsType : 'SSH' } })) Note: In this case the configuration assumes that there is a secret github-ssh-json define in the target account and all the regions where the blueprint will be deployed. Define the secret in AWS Secret Manager as \"Plain Text\" that contains a JSON structure (as of ArgoCD 2.x) with the fields sshPrivateKey and url defined. Note, that JSON does not allow line break characters, so all new line characters must be escaped with \\n . Example Structure: { \"sshPrivateKey\" : \"-----BEGIN THIS IS NOT A REAL PRIVATE KEY-----\\nb3BlbnNzaC1rtdjEAAAAABG5vbmUAAAAEbm9uZQAAAAAAAAABAAACFwAAAAdzc2gtcn\\nNhAAAAAwEAAQAAAgEAy82zTTDStK+s0dnaYzE7vLSAcwsiHM8gN\\nhq2p5TfcjCcYUWetyu6e/xx5Rh+AwbVvDV5h9QyMw4NJobwuj5PBnhkc3QfwJAO5wOnl7R\\nGbehIleWWZLs9qq`DufViQsa0fDwP6JCrqD14aIozg6sJ0Oqi7vQkV+jR0ht/\\nuFO1ANXBn2ih0ZpXeHSbPDLeZQjlOBrbGytnCbdvLtfGEsV0WO2oIieWVXJj/zzpKuMmrr\\nebPsfwr36nLprOQV6IhDDo\\n-----END NOT A REAL PRIVATE KEY-----\\n\" , \"url\" : \"git@github\" } Note: You can notice explicit \\n characters in the sshPrivateKey . url attribute is required and must specify full or partial URL for credentials template. For example git@github will set the credentials for all GitHub repositories when SSH authentication is used. For more information see Repository Credentials and SSH Repositories from official ArgoCD documentation. To escape your SSH private key for storing it as a secret you can use the following command on Mac/Linux: awk 'NF {sub(/\\r/, \"\"); printf \"%s\\\\n\",$0;}' <path-to-your-cert> A convenience script to create the JSON structure for SSH private key can be found here . You will need to set the PEM_FILE (full path to the ssh private key file) and URL_TEMPLATE (part of the URL for credentials template) variables inside the script. ( important ) Replicate the secret to all the desired regions. Please see instructions for GitHub oou n details on setting up SSH access. Username Password and Token Authentication Set credentialsType to USERNAME or TOKEN when defining ApplicationRepository in the ArgoCD add-on configuration. Define the secret in the AWS Secret Manager as \"Key Value\" and set fields url , username and password to the desired values (clear text). For TOKEN username could be set to any username and password field set to the GitHub token. Replicate to the desired regions. Make sure that for this type of authentication your repository URL is set as https , e.g. https://github.com/aws-samples/eks-blueprints-workloads.git. Example Structure for USERNAME and TOKEN authentication type: { \"username\" : \"YOUR_GIT_USERNAME\" , \"password\" : \"YOUR PASSWORD OR TOKEN\" , \"url\" : \"https://github.com/aws-samples\" } Note: url value can be a path to the org, rather than an actual repository. Admin Secret Create a secret in the AWS Secrets Manager as \"Plain Text\" and set the value to the desired ArgoCD admin password. Replicate the secret to all the desired regions. Set the secret name in adminPasswordSecretName in ArgoCD add-on configuration. You can change the secret value through AWS Secrets Manager, however, it will require to rerun cdk deploy with the minimal changeset to apply the change. Alternatively to get started, the admin password hash can be set bypassing the AWS Secret by setting the following structure in the values properties of the add-on parameters: import * as bcrypt from \"bcrypt\" ; . addOns ( new blueprints . addons . ArgoCDAddOn ({ ... // other settings values : { \"configs\" : { \"secret\" : { \"argocdServerAdminPassword\" : bcrypt . hash ( < your password plain text > , 10 ) // or just supply <your bcrypt hash> directly } } } })) For more information, please refer to the ArgoCD official documentation . Known Issues \u00b6 Destruction of the cluster with provisioned applications may cause cloud formation to get stuck on deleting ArgoCD namespace. This happens because the server component that handles Application CRD resource is destroyed before it has a chance to clean up applications that were provisioned through GitOps (of which CFN is unaware). To address this issue at the moment, App of Apps application should be destroyed manually before destroying the stack. Changing the administrator password in the AWS Secrets Manager and rerunning the stack causes login error on ArgoCD UI. This happens due to the fact that Argo Helm rewrites the secret containing the Dex server API Key (OIDC component of ArgoCD). The workaround at present is to restart the argocd-server pod, which repopulates the token. Secret management aspect of ArgoCD will be improved in the future to not require this step after password change. Troubleshooting \u00b6 Dex Server crashing on startup with server.secretkey is missing . It may be a byproduct of another failure. As a rule (unless ArgoCD secret is configured separately) the initial start of the ArgoCD server should populate a few fields in the in ArgoCD secret . If ArgoCD server fails to start or is waiting on some condition to become ready, these fields are not populated, causing cascading failures. Make sure that all the secrets are mounted properly onto the ArgoCD server pod. It can be caused by an incorrect shape of the secret for private repositories (see \"Private Repositories\" section above). SSH secret is expected to have two fields ( url and sshPrivateKey ) and USERNAME/TOKEN is expected to have three fields ( username , password , url ). Make sure your secret name (as defined in AWS Secrets Manager) does not conflict with ArgoCD reserved secret names, such as argocd-secret .","title":"ArgoCD"},{"location":"addons/argo-cd/#argo-cd-add-on","text":"Argo CD is a declarative, GitOps continuous delivery tool for Kubernetes. The Argo CD add-on provisions Argo CD into an EKS cluster, and can optionally bootstrap your workloads from public and private Git repositories. The Argo CD add-on allows platform administrators to combine cluster provisioning and workload bootstrapping in a single step and enables use cases such as replicating an existing running production cluster in a different region in a matter of minutes. This is important for business continuity and disaster recovery cases as well as for cross-regional availability and geographical expansion. Please see the documentation below for details on automatic boostrapping with ArgoCD add-on. If you prefer manual bootstrapping (once your cluster is deployed with this add-on included), you can find instructions on getting started with Argo CD in our Getting Started guide. Full Argo CD project documentation can be found here .","title":"Argo CD Add-on"},{"location":"addons/argo-cd/#usage","text":"To provision and maintain ArgoCD components without any bootstrapping, the add-on provides a no-argument constructor to get started. import 'source-map-support/register' ; import * as cdk from 'aws-cdk-lib' ; import * as blueprints from '@aws-quickstart/eks-blueprints' ; const app = new cdk . App (); const addOn = new blueprints . addons . ArgoCDAddOn (); const blueprint = blueprints . EksBlueprint . builder () . addOns ( addOn ) . build ( app , 'my-stack-name' ); The above will create an argocd namespace and install all Argo CD components. In order to bootstrap workloads you will need to change the default ArgoCD admin password and add repositories as specified in the Getting Started documentation.","title":"Usage"},{"location":"addons/argo-cd/#functionality","text":"Creates the namespace specified in the construction parameter ( argocd by default). Deploys the argo-cd Helm chart into the cluster. Allows to specify ApplicationRepository selecting the required authentication method as SSH Key, username/password or username/token. Credentials are expected to be set in AWS Secrets Manager and replicated to the desired region. If bootstrap repository is specified, creates the initial bootstrap application which may be leveraged to bootstrap workloads and/or other add-ons through GitOps. Allows setting the initial admin password through AWS Secrets Manager, replicating to the desired region. Supports standard helm configuration options .","title":"Functionality"},{"location":"addons/argo-cd/#setting-an-admin-password","text":"By default, the Argo CD add-on will create a new admin password for you. To specify your own, you can leverage the AWS Secrets Manager. const argoCDAddOn = new ArgoCDAddOn ({ adminPasswordSecretName : `your-secret-name` }); const addOns : Array < ClusterAddOn > = [ argoCDAddOn ]; The attribute adminPasswordSecretName is the logical name of the secret in AWS Secret Manager . Note, that when deploying to multiple regions, the secret is expected to be replicated to each region. Inside ArgoCD, the admin password is stored as a bcrypt hash. This step will be performed by the framework and stored in the ArgoCD admin secret . You can change the admin password through the Secrets Manager, but it will require rerunning the provisioning pipeline to apply the change.","title":"Setting an Admin Password"},{"location":"addons/argo-cd/#bootstrapping","text":"The Blueprints framework provides an approach to bootstrap workloads and/or additional add-ons from a customer GitOps repository. In a general case, the bootstrap GitOps repository may contains an App of Apps that points to all workloads and add-ons. In order to enable bootstrapping, the add-on allows passing an ApplicationRepository at construction time. The following repository types are supported at present: Public HTTP/HTTPS repositories (e.g., GitHub) Private HTTPS accessible git repositories requiring username/password authentication. Private git repositories with SSH access requiring an SSH key for authentication. Private HTTPS accessible GitHub repositories accessible with GitHub token. An example is provided below, along with an approach that could use a separate app of apps to bootstrap workloads in different stages, which is important for a software delivery platform as it allows segregating workloads specific to each stage of the SDLC and defines clear promotion processes through GitOps. import 'source-map-support/register' ; import * as cdk from 'aws-cdk-lib' ; import { ArgoCDAddOn , ClusterAddOn , EksBlueprint , ApplicationRepository } from '@aws-quickstart/eks-blueprints' ; const secretStoreAddOn = new SecretsStoreAddOn (); const repoUrl = 'git@github.com:aws-samples/eks-blueprints-workloads.git' const bootstrapRepo : ApplicationRepository = { repoUrl , credentialsSecretName : 'github-ssh-test' , credentialsType : 'SSH' } const devBootstrapArgo = new ArgoCDAddOn ({ bootstrapRepo : { ... bootstrapRepo , path : 'envs/dev' } }); const testBootstrapArgo = new ArgoCDAddOn ({ bootstrapRepo : { ... bootstrapRepo , path : 'envs/test' , }, }); const prodBootstrapArgo = new ArgoCDAddOn ({ bootstrapRepo : { ... bootstrapRepo , path : 'envs/prod' , }, adminPasswordSecretName : 'ArgoCDAdmin' , }); const blueprint = EksBlueprint . builder () . addOns ( secretStoreAddOn ) . account ( account ); blueprint . clone ( 'us-east-1' ) . addOns ( devBootstrapArgo ) . build ( app , 'argo-us-east-1' ); blueprint . clone ( 'us-east-2' ) . addOns ( testBootstrapArgo ) . build ( app , 'argo-us-east-2' ); blueprint . clone ( 'us-west-2' ) . addOns ( prodBootstrapArgo ) . build ( app , 'argo-us-west-1' ); The application promotion process in the above example is handled entirely through GitOps. Each stage specific App of Apps contains references to respective application GitOps repository for each stage (e.g referencing the release vs work branches or path-based within individual app GitOps repository).","title":"Bootstrapping"},{"location":"addons/argo-cd/#gitops-for-eks-blueprints-addons","text":"By default all AddOns defined in a blueprint are deployed to the cluster via CDK. You can opt-in to deploy them following the GitOps model via ArgoCD. You will need a repository contains all the AddOns you would like to deploy via ArgoCD, such as, eks-blueprints-add-ons . You then configure ArgoCD bootstrapping with this repository as shown above. There are two types of GitOps deployments via ArgoCD depending on whether you would like to adopt the App of Apps strategy: CDK deploys the Application resource for each AddOn enabled, and ArgoCD deploys the actual AddOn via GitOps based on the Application resource. Example: import * as blueprints from '@aws-quickstart/eks-blueprints' ; // enable gitops bootstrapping with argocd const prodBootstrapArgo = new blueprints . addons . ArgoCDAddOn ({ bootstrapRepo : { repoUrl : 'https://github.com/aws-samples/eks-blueprints-add-ons' , path : 'add-ons' , targetRevision : \"main\" , }, }); // addons const addOns : Array < blueprints . ClusterAddOn > = [ prodBootstrapArgo , new blueprints . addons . AppMeshAddOn (), new blueprints . addons . MetricsServerAddOn (), new blueprints . addons . AwsLoadBalancerControllerAddOn (), ]; blueprints . EksBlueprint . builder () . addOns (... addOns ) . enableGitOps ( blueprints . GitOpsMode . APPLICATION ) . build ( scope , stackID ); CDK deploys the only Application resource for the App of Apps, aka bootstrap-apps , and ArgoCD deploys all the AddOns based on the bootstrap-apps . This requires the naming pattern between the AddOns and App of Apps matches. Example: import * as blueprints from '@aws-quickstart/eks-blueprints' ; // enable gitops bootstrapping with argocd app of apps const prodBootstrapArgo = new blueprints . addons . ArgoCDAddOn ({ bootstrapRepo : { repoUrl : 'https://github.com/aws-samples/eks-blueprints-add-ons' , path : 'chart' , targetRevision : \"main\" , }, }); // addons const addOns : Array < blueprints . ClusterAddOn > = [ prodBootstrapArgo , new blueprints . addons . AppMeshAddOn (), new blueprints . addons . MetricsServerAddOn (), new blueprints . addons . AwsLoadBalancerControllerAddOn (), ]; blueprints . EksBlueprint . builder () . addOns (... addOns ) . enableGitOps ( blueprints . GitOpsMode . APP_OF_APPS ) . build ( scope , stackID );","title":"GitOps for EKS Blueprints AddOns"},{"location":"addons/argo-cd/#secrets-support","text":"The framework provides support to supply repository and administrator secrets in AWS Secrets Manager. This support is evolving and will be improved over time as ArgoCD itself matures.","title":"Secrets Support"},{"location":"addons/argo-cd/#private-repositories","text":"SSH Key Authentication Set credentialsType to SSH when defining bootstrap repository in the ArgoCD add-on configuration. . addOns ( new blueprints . addons . ArgoCDAddOn ({ bootstrapRepo : { repoUrl : 'git@github.com:aws-samples/eks-blueprints-workloads.git' , path : 'envs/dev' , credentialsSecretName : 'github-ssh-json' , credentialsType : 'SSH' } })) Note: In this case the configuration assumes that there is a secret github-ssh-json define in the target account and all the regions where the blueprint will be deployed. Define the secret in AWS Secret Manager as \"Plain Text\" that contains a JSON structure (as of ArgoCD 2.x) with the fields sshPrivateKey and url defined. Note, that JSON does not allow line break characters, so all new line characters must be escaped with \\n . Example Structure: { \"sshPrivateKey\" : \"-----BEGIN THIS IS NOT A REAL PRIVATE KEY-----\\nb3BlbnNzaC1rtdjEAAAAABG5vbmUAAAAEbm9uZQAAAAAAAAABAAACFwAAAAdzc2gtcn\\nNhAAAAAwEAAQAAAgEAy82zTTDStK+s0dnaYzE7vLSAcwsiHM8gN\\nhq2p5TfcjCcYUWetyu6e/xx5Rh+AwbVvDV5h9QyMw4NJobwuj5PBnhkc3QfwJAO5wOnl7R\\nGbehIleWWZLs9qq`DufViQsa0fDwP6JCrqD14aIozg6sJ0Oqi7vQkV+jR0ht/\\nuFO1ANXBn2ih0ZpXeHSbPDLeZQjlOBrbGytnCbdvLtfGEsV0WO2oIieWVXJj/zzpKuMmrr\\nebPsfwr36nLprOQV6IhDDo\\n-----END NOT A REAL PRIVATE KEY-----\\n\" , \"url\" : \"git@github\" } Note: You can notice explicit \\n characters in the sshPrivateKey . url attribute is required and must specify full or partial URL for credentials template. For example git@github will set the credentials for all GitHub repositories when SSH authentication is used. For more information see Repository Credentials and SSH Repositories from official ArgoCD documentation. To escape your SSH private key for storing it as a secret you can use the following command on Mac/Linux: awk 'NF {sub(/\\r/, \"\"); printf \"%s\\\\n\",$0;}' <path-to-your-cert> A convenience script to create the JSON structure for SSH private key can be found here . You will need to set the PEM_FILE (full path to the ssh private key file) and URL_TEMPLATE (part of the URL for credentials template) variables inside the script. ( important ) Replicate the secret to all the desired regions. Please see instructions for GitHub oou n details on setting up SSH access. Username Password and Token Authentication Set credentialsType to USERNAME or TOKEN when defining ApplicationRepository in the ArgoCD add-on configuration. Define the secret in the AWS Secret Manager as \"Key Value\" and set fields url , username and password to the desired values (clear text). For TOKEN username could be set to any username and password field set to the GitHub token. Replicate to the desired regions. Make sure that for this type of authentication your repository URL is set as https , e.g. https://github.com/aws-samples/eks-blueprints-workloads.git. Example Structure for USERNAME and TOKEN authentication type: { \"username\" : \"YOUR_GIT_USERNAME\" , \"password\" : \"YOUR PASSWORD OR TOKEN\" , \"url\" : \"https://github.com/aws-samples\" } Note: url value can be a path to the org, rather than an actual repository. Admin Secret Create a secret in the AWS Secrets Manager as \"Plain Text\" and set the value to the desired ArgoCD admin password. Replicate the secret to all the desired regions. Set the secret name in adminPasswordSecretName in ArgoCD add-on configuration. You can change the secret value through AWS Secrets Manager, however, it will require to rerun cdk deploy with the minimal changeset to apply the change. Alternatively to get started, the admin password hash can be set bypassing the AWS Secret by setting the following structure in the values properties of the add-on parameters: import * as bcrypt from \"bcrypt\" ; . addOns ( new blueprints . addons . ArgoCDAddOn ({ ... // other settings values : { \"configs\" : { \"secret\" : { \"argocdServerAdminPassword\" : bcrypt . hash ( < your password plain text > , 10 ) // or just supply <your bcrypt hash> directly } } } })) For more information, please refer to the ArgoCD official documentation .","title":"Private Repositories"},{"location":"addons/argo-cd/#known-issues","text":"Destruction of the cluster with provisioned applications may cause cloud formation to get stuck on deleting ArgoCD namespace. This happens because the server component that handles Application CRD resource is destroyed before it has a chance to clean up applications that were provisioned through GitOps (of which CFN is unaware). To address this issue at the moment, App of Apps application should be destroyed manually before destroying the stack. Changing the administrator password in the AWS Secrets Manager and rerunning the stack causes login error on ArgoCD UI. This happens due to the fact that Argo Helm rewrites the secret containing the Dex server API Key (OIDC component of ArgoCD). The workaround at present is to restart the argocd-server pod, which repopulates the token. Secret management aspect of ArgoCD will be improved in the future to not require this step after password change.","title":"Known Issues"},{"location":"addons/argo-cd/#troubleshooting","text":"Dex Server crashing on startup with server.secretkey is missing . It may be a byproduct of another failure. As a rule (unless ArgoCD secret is configured separately) the initial start of the ArgoCD server should populate a few fields in the in ArgoCD secret . If ArgoCD server fails to start or is waiting on some condition to become ready, these fields are not populated, causing cascading failures. Make sure that all the secrets are mounted properly onto the ArgoCD server pod. It can be caused by an incorrect shape of the secret for private repositories (see \"Private Repositories\" section above). SSH secret is expected to have two fields ( url and sshPrivateKey ) and USERNAME/TOKEN is expected to have three fields ( username , password , url ). Make sure your secret name (as defined in AWS Secrets Manager) does not conflict with ArgoCD reserved secret names, such as argocd-secret .","title":"Troubleshooting"},{"location":"addons/aws-batch-on-eks/","text":"AWS Batch on EKS \u00b6 AWS Batch is a managed service that orchestrates batch workloads in your Kubernetes clusters that are managed by Amazon Elastic Kubernetes Service (Amazon EKS). Since AWS Batch is a managed service, there are no Kubernetes components (for example, Operators or Custom Resources) to install or manage in your cluster. AWS Batch only needs your cluster to be configured with Role-Based Access Controls (RBAC) that allow AWS Batch to communicate with the Kubernetes API server. AWS Batch calls Kubernetes APIs to create, monitor, and delete Kubernetes pods and nodes. For more information, consult our official documentations . This Add-on MUST be used with AWS Batch on EKS Team . Usage \u00b6 import 'source-map-support/register' ; import * as cdk from 'aws-cdk-lib' ; import * as blueprints from '@aws-quickstart/eks-blueprints' ; const app = new cdk . App (); const addOn = new blueprints . addons . AwsBatchAddOn (); const blueprint = blueprints . EksBlueprint . builder () . addOns ( addOn ) . build ( app , 'my-stack-name' ); Once the AddOn is deployed you can execute the following command: kubectl describe -n kube-system configmap/aws-auth The output of the command would show a list of IAM role and mapping to Kubernetes users, one fo the mapping would be for AWS Batch on EKS role and would be similar to the following: mapRoles: | - rolearn: arn:aws:iam::<your-account-id>:role/AWSServiceRoleForBatch username: aws-batch","title":"AWS Batch for EKS"},{"location":"addons/aws-batch-on-eks/#aws-batch-on-eks","text":"AWS Batch is a managed service that orchestrates batch workloads in your Kubernetes clusters that are managed by Amazon Elastic Kubernetes Service (Amazon EKS). Since AWS Batch is a managed service, there are no Kubernetes components (for example, Operators or Custom Resources) to install or manage in your cluster. AWS Batch only needs your cluster to be configured with Role-Based Access Controls (RBAC) that allow AWS Batch to communicate with the Kubernetes API server. AWS Batch calls Kubernetes APIs to create, monitor, and delete Kubernetes pods and nodes. For more information, consult our official documentations . This Add-on MUST be used with AWS Batch on EKS Team .","title":"AWS Batch on EKS"},{"location":"addons/aws-batch-on-eks/#usage","text":"import 'source-map-support/register' ; import * as cdk from 'aws-cdk-lib' ; import * as blueprints from '@aws-quickstart/eks-blueprints' ; const app = new cdk . App (); const addOn = new blueprints . addons . AwsBatchAddOn (); const blueprint = blueprints . EksBlueprint . builder () . addOns ( addOn ) . build ( app , 'my-stack-name' ); Once the AddOn is deployed you can execute the following command: kubectl describe -n kube-system configmap/aws-auth The output of the command would show a list of IAM role and mapping to Kubernetes users, one fo the mapping would be for AWS Batch on EKS role and would be similar to the following: mapRoles: | - rolearn: arn:aws:iam::<your-account-id>:role/AWSServiceRoleForBatch username: aws-batch","title":"Usage"},{"location":"addons/aws-for-fluent-bit/","text":"AWS for Fluent Bit \u00b6 Fluent Bit is an open source log processor and forwarder which allows you to collect data like metrics and logs from different sources, enrich them with filters and send them to multiple destinations. AWS provides a Fluent Bit image with plugins for both CloudWatch Logs and Kinesis Data Firehose. The AWS for Fluent Bit image is available on the Amazon ECR Public Gallery. For more details, see AWS for Fluent Bit GitHub repository . Usage \u00b6 import 'source-map-support/register' ; import * as cdk from 'aws-cdk-lib' ; import * as blueprints from '@aws-quickstart/eks-blueprints' ; const app = new cdk . App (); const addOn = new blueprints . addons . AwsForFluentBitAddOn (); const blueprint = blueprints . EksBlueprint . builder () . addOns ( addOn ) . build ( app , 'my-stack-name' ); Configuration \u00b6 AWS for FluentBit can be configured to forward logs to multiple AWS destinations including CloudWatch, Kinesis, and Elasticsearch (now AWS OpenSearch Search). Sample configuration can be found below: import * as blueprints from '@aws-quickstart/eks-blueprints' ; const awsForFluentBit = new blueprints . addons . AwsForFluentBitAddOn ({ values : { cloudWatch : { enabled : true , region : \"<aws_region\" , logGroupName : \"<log_groups_name>\" }, kinesis : { enabled : true , region : \"<aws_region\" , deliveryStream : \"<delivery_stream>\" }, elasticSearch : { enabled : true , region : \"<aws_region\" , host : \"<elastic_search_host>\" } } }); IAM Policies \u00b6 When leveraging AWS for FluentBit to forward logs to various AWS destinations, you will need to supply an IAM role that grants privileges to the namespace in which FluentBit runs. For example, in order to forward logs to Amazon Elasticsearch Service, you would supply the following IAMPolicyStatement. import * as blueprints from '@aws-quickstart/eks-blueprints' ; const domain = es . Domain () const domainWritePolicy = new iam . PolicyStatement ({ actions : [ 'es:ESHttpDelete' , 'es:ESHttpPost' , 'es:ESHttpPut' , 'es:ESHttpPatch' ], resources : [ domain . arn ], }) const awsForFluentBit = new blueprints . addons . AwsForFluentBitAddOn ({ iamPolicies : [ domainWritePolicy ] values : { elasticSearch : { enabled : true , awsRegion : \"<aws_region\" , host : \"<elastic_search_host>\" } } });","title":"AWS For Fluent Bit"},{"location":"addons/aws-for-fluent-bit/#aws-for-fluent-bit","text":"Fluent Bit is an open source log processor and forwarder which allows you to collect data like metrics and logs from different sources, enrich them with filters and send them to multiple destinations. AWS provides a Fluent Bit image with plugins for both CloudWatch Logs and Kinesis Data Firehose. The AWS for Fluent Bit image is available on the Amazon ECR Public Gallery. For more details, see AWS for Fluent Bit GitHub repository .","title":"AWS for Fluent Bit"},{"location":"addons/aws-for-fluent-bit/#usage","text":"import 'source-map-support/register' ; import * as cdk from 'aws-cdk-lib' ; import * as blueprints from '@aws-quickstart/eks-blueprints' ; const app = new cdk . App (); const addOn = new blueprints . addons . AwsForFluentBitAddOn (); const blueprint = blueprints . EksBlueprint . builder () . addOns ( addOn ) . build ( app , 'my-stack-name' );","title":"Usage"},{"location":"addons/aws-for-fluent-bit/#configuration","text":"AWS for FluentBit can be configured to forward logs to multiple AWS destinations including CloudWatch, Kinesis, and Elasticsearch (now AWS OpenSearch Search). Sample configuration can be found below: import * as blueprints from '@aws-quickstart/eks-blueprints' ; const awsForFluentBit = new blueprints . addons . AwsForFluentBitAddOn ({ values : { cloudWatch : { enabled : true , region : \"<aws_region\" , logGroupName : \"<log_groups_name>\" }, kinesis : { enabled : true , region : \"<aws_region\" , deliveryStream : \"<delivery_stream>\" }, elasticSearch : { enabled : true , region : \"<aws_region\" , host : \"<elastic_search_host>\" } } });","title":"Configuration"},{"location":"addons/aws-for-fluent-bit/#iam-policies","text":"When leveraging AWS for FluentBit to forward logs to various AWS destinations, you will need to supply an IAM role that grants privileges to the namespace in which FluentBit runs. For example, in order to forward logs to Amazon Elasticsearch Service, you would supply the following IAMPolicyStatement. import * as blueprints from '@aws-quickstart/eks-blueprints' ; const domain = es . Domain () const domainWritePolicy = new iam . PolicyStatement ({ actions : [ 'es:ESHttpDelete' , 'es:ESHttpPost' , 'es:ESHttpPut' , 'es:ESHttpPatch' ], resources : [ domain . arn ], }) const awsForFluentBit = new blueprints . addons . AwsForFluentBitAddOn ({ iamPolicies : [ domainWritePolicy ] values : { elasticSearch : { enabled : true , awsRegion : \"<aws_region\" , host : \"<elastic_search_host>\" } } });","title":"IAM Policies"},{"location":"addons/aws-load-balancer-controller/","text":"AWS Load Balancer Controller Add-on \u00b6 The AWS Load Balancer Controller manages AWS Elastic Load Balancers for a Kubernetes cluster. The controller provisions the following resources: An AWS Application Load Balancer (ALB) when you create a Kubernetes Ingress. An AWS Network Load Balancer (NLB) when you create a Kubernetes Service of type LoadBalancer. In the past, you used the Kubernetes in-tree load balancer for instance targets, but used the AWS Load balancer Controller for IP targets. With the AWS Load Balancer Controller version 2.2.0 or later, you can create Network Load Balancers using either target type. For more information about NLB target types, see Target type in the User Guide for Network Load Balancers. For more information about AWS Load Balancer Controller please see the official documentation . This controller is a required for proper configuration of other ingress controllers such as NGINX. Usage \u00b6 import 'source-map-support/register' ; import * as cdk from 'aws-cdk-lib' ; import * as blueprints from '@aws-quickstart/eks-blueprints' ; const app = new cdk . App (); const addOn = new blueprints . addons . AwsLoadBalancerControllerAddOn (); const blueprint = blueprints . EksBlueprint . builder () . addOns ( addOn ) . build ( app , 'my-stack-name' ); To validate that controller is running, ensure that controller deployment is in RUNNING state: # Assuming controller is installed in kube-system namespace $ kubectl get deployments -n kube-system NAME READY UP-TO-DATE AVAILABLE AGE aws-load-balancer-controller 2 /2 2 2 3m58s Functionality \u00b6 Adds proper IAM permissions and creates a Kubernetes service account with IRSA integration. Allows configuration options such as enabling WAF and Shield. Allows to replace the helm chart version if a specific version of the controller is needed. Creates an IngressClass associated with the AWS Load Balance Controller when the createIngressClassResource prop is set to true Supports standard helm configuration options . Note : An ingressClass must be created in the cluster, either using the createIngressClassResource prop or externally, to be able to create Ingresses associated with the AWS ALB. Creating a Load Balanced Service \u00b6 Once the AWS Load Balancer Controller add-on is installed in your cluster, it is able to provision both Network Load Balancers and Application Load Balancers on your behalf. For example, when the following manifest is applied to your cluster, it will create an NLB. apiVersion : v1 kind : Service metadata : annotations : service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout : '60' service.beta.kubernetes.io/aws-load-balancer-type : nlb name : udp-test1 spec : type : LoadBalancer ports : - port : 5005 protocol : UDP targetPort : 5005 selector : name : your-app","title":"AWS Load Balancer Controller"},{"location":"addons/aws-load-balancer-controller/#aws-load-balancer-controller-add-on","text":"The AWS Load Balancer Controller manages AWS Elastic Load Balancers for a Kubernetes cluster. The controller provisions the following resources: An AWS Application Load Balancer (ALB) when you create a Kubernetes Ingress. An AWS Network Load Balancer (NLB) when you create a Kubernetes Service of type LoadBalancer. In the past, you used the Kubernetes in-tree load balancer for instance targets, but used the AWS Load balancer Controller for IP targets. With the AWS Load Balancer Controller version 2.2.0 or later, you can create Network Load Balancers using either target type. For more information about NLB target types, see Target type in the User Guide for Network Load Balancers. For more information about AWS Load Balancer Controller please see the official documentation . This controller is a required for proper configuration of other ingress controllers such as NGINX.","title":"AWS Load Balancer Controller Add-on"},{"location":"addons/aws-load-balancer-controller/#usage","text":"import 'source-map-support/register' ; import * as cdk from 'aws-cdk-lib' ; import * as blueprints from '@aws-quickstart/eks-blueprints' ; const app = new cdk . App (); const addOn = new blueprints . addons . AwsLoadBalancerControllerAddOn (); const blueprint = blueprints . EksBlueprint . builder () . addOns ( addOn ) . build ( app , 'my-stack-name' ); To validate that controller is running, ensure that controller deployment is in RUNNING state: # Assuming controller is installed in kube-system namespace $ kubectl get deployments -n kube-system NAME READY UP-TO-DATE AVAILABLE AGE aws-load-balancer-controller 2 /2 2 2 3m58s","title":"Usage"},{"location":"addons/aws-load-balancer-controller/#functionality","text":"Adds proper IAM permissions and creates a Kubernetes service account with IRSA integration. Allows configuration options such as enabling WAF and Shield. Allows to replace the helm chart version if a specific version of the controller is needed. Creates an IngressClass associated with the AWS Load Balance Controller when the createIngressClassResource prop is set to true Supports standard helm configuration options . Note : An ingressClass must be created in the cluster, either using the createIngressClassResource prop or externally, to be able to create Ingresses associated with the AWS ALB.","title":"Functionality"},{"location":"addons/aws-load-balancer-controller/#creating-a-load-balanced-service","text":"Once the AWS Load Balancer Controller add-on is installed in your cluster, it is able to provision both Network Load Balancers and Application Load Balancers on your behalf. For example, when the following manifest is applied to your cluster, it will create an NLB. apiVersion : v1 kind : Service metadata : annotations : service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout : '60' service.beta.kubernetes.io/aws-load-balancer-type : nlb name : udp-test1 spec : type : LoadBalancer ports : - port : 5005 protocol : UDP targetPort : 5005 selector : name : your-app","title":"Creating a Load Balanced Service"},{"location":"addons/aws-node-termination-handler/","text":"AWS Node Termination Handler \u00b6 The AWS Node Termination Handler (NTH) project ensures that the Kubernetes control plane responds appropriately to events that can cause your EC2 instance to become unavailable, such as EC2 maintenance events, EC2 Spot interruptions, ASG Scale-In, ASG AZ Rebalance, and EC2 Instance Termination via the API or Console. If not handled, your application code may not stop gracefully, take longer to recover full availability, or accidentally schedule work to nodes that are going down. For more information see [README.md][https://github.com/aws/aws-node-termination-handler#readme]. NTH can operate in two different modes: Instance Metadata Service (IMDS) or the Queue Processor. To choose the operating mode refer to this table . Best Practice NTH should only be used when you are using self-managed node groups and self-managed node groups with Spot instances. For more information on why you do not need NTH on managed node groups see this issue and EKS Workshop for detailed explanation. Best Practice Use NTH in Queue Processor option to add every AWS Node Termination Handler feature to the self-managed node group. Note With AWS Fargate, you no longer have to provision, configure, or scale clusters of virtual machines to run containers. This removes the need to use AWS Node Termination Handler. Usage \u00b6 import 'source-map-support/register' ; import * as cdk from 'aws-cdk-lib' ; import * as blueprints from '@aws-quickstart/eks-blueprints' ; const app = new cdk . App (); const addOn = new blueprints . addons . AwsNodeTerminationHandlerAddOn (); const clusterProvider = new blueprints . AsgClusterProvider ({ version : eks.KubernetesVersion.V1_24 , machineImageType : eks.MachineImageType.BOTTLEROCKET }); const blueprint = blueprints . EksBlueprint . builder () . clusterProvider ( clusterProvider ) . addOns ( addOn ) . build ( app , 'my-stack-name' ); To validate that controller is running, ensure that controller deployment is in RUNNING state: # Assuming handler is installed in kube-system namespace $ kubectl get deployments -n kube-system NAME READY UP-TO-DATE AVAILABLE AGE aws-node-termination-handler 1 /1 1 1 23m Functionality \u00b6 IMDS Mode (default) \u00b6 Node group ASG tagged with key=aws-node-termination-handler/managed Deploy the AWS Node Termination Handler helm chart Queue Mode \u00b6 Node group ASG tagged with key=aws-node-termination-handler/managed AutoScaling Group Termination Lifecycle Hook Amazon Simple Queue Service (SQS) Queue Amazon EventBridge Rule IAM Role for the aws-node-termination-handler Queue Processing Pods Deploy the AWS Node Termination Handler helm chart","title":"AWS Node Termination Handler"},{"location":"addons/aws-node-termination-handler/#aws-node-termination-handler","text":"The AWS Node Termination Handler (NTH) project ensures that the Kubernetes control plane responds appropriately to events that can cause your EC2 instance to become unavailable, such as EC2 maintenance events, EC2 Spot interruptions, ASG Scale-In, ASG AZ Rebalance, and EC2 Instance Termination via the API or Console. If not handled, your application code may not stop gracefully, take longer to recover full availability, or accidentally schedule work to nodes that are going down. For more information see [README.md][https://github.com/aws/aws-node-termination-handler#readme]. NTH can operate in two different modes: Instance Metadata Service (IMDS) or the Queue Processor. To choose the operating mode refer to this table . Best Practice NTH should only be used when you are using self-managed node groups and self-managed node groups with Spot instances. For more information on why you do not need NTH on managed node groups see this issue and EKS Workshop for detailed explanation. Best Practice Use NTH in Queue Processor option to add every AWS Node Termination Handler feature to the self-managed node group. Note With AWS Fargate, you no longer have to provision, configure, or scale clusters of virtual machines to run containers. This removes the need to use AWS Node Termination Handler.","title":"AWS Node Termination Handler"},{"location":"addons/aws-node-termination-handler/#usage","text":"import 'source-map-support/register' ; import * as cdk from 'aws-cdk-lib' ; import * as blueprints from '@aws-quickstart/eks-blueprints' ; const app = new cdk . App (); const addOn = new blueprints . addons . AwsNodeTerminationHandlerAddOn (); const clusterProvider = new blueprints . AsgClusterProvider ({ version : eks.KubernetesVersion.V1_24 , machineImageType : eks.MachineImageType.BOTTLEROCKET }); const blueprint = blueprints . EksBlueprint . builder () . clusterProvider ( clusterProvider ) . addOns ( addOn ) . build ( app , 'my-stack-name' ); To validate that controller is running, ensure that controller deployment is in RUNNING state: # Assuming handler is installed in kube-system namespace $ kubectl get deployments -n kube-system NAME READY UP-TO-DATE AVAILABLE AGE aws-node-termination-handler 1 /1 1 1 23m","title":"Usage"},{"location":"addons/aws-node-termination-handler/#functionality","text":"","title":"Functionality"},{"location":"addons/aws-node-termination-handler/#imds-mode-default","text":"Node group ASG tagged with key=aws-node-termination-handler/managed Deploy the AWS Node Termination Handler helm chart","title":"IMDS Mode (default)"},{"location":"addons/aws-node-termination-handler/#queue-mode","text":"Node group ASG tagged with key=aws-node-termination-handler/managed AutoScaling Group Termination Lifecycle Hook Amazon Simple Queue Service (SQS) Queue Amazon EventBridge Rule IAM Role for the aws-node-termination-handler Queue Processing Pods Deploy the AWS Node Termination Handler helm chart","title":"Queue Mode"},{"location":"addons/aws-privateca-issuer/","text":"AWS Private CA Issuer Add-on \u00b6 This addon will install aws-privateca-issuer AWS ACM Private CA is a module of the AWS Certificate Manager that can setup and manage private CAs. The AWS PrivateCA Issuer plugin acts as an addon to cert-manager that signs certificate requests using ACM Private CA. Since its an addon to cert-manager, for Installing AWS ACM Private CA Addon, You must install cert-manager Addon first cert-manager is a Kubernetes add-on to automate the management and issuance of TLS certificates from various issuing sources. It will ensure certificates are valid and up to date periodically, and attempt to renew certificates at an appropriate time before expiry. Usage \u00b6 Please ensure that cert-manager addon is already installed import 'source-map-support/register' ; import * as cdk from 'aws-cdk-lib' ; import * as blueprints from '@aws-quickstart/eks-blueprints' ; const app = new cdk . App (); const awsPcaParams = { iamPolicies : [ \"AWSCertificateManagerPrivateCAFullAccess\" ] } const addOn = new blueprints . addons . AWSPrivateCAIssuerAddon ( awsPcaParams ) const blueprint = blueprints . EksBlueprint . builder () . addOns ( addOn ) . build ( app , 'my-stack-name' ); Configuration Options \u00b6 serviceAccountName : (string) User provided name for service account. The default value is aws-pca-issuer iamPolicies - An array of Managed IAM Policies which Service Account needs for IRSA Eg: irsaRoles:[\"AWSCertificateManagerPrivateCAFullAccess\"]. If not empty the Service Account will be created by the CDK with IAM Roles Mapped (IRSA). In case if its empty, Service Account will be created with out default IAM Policy - \"AWSCertificateManagerPrivateCAFullAccess\" values : Arbitrary values to pass to the chart. Refer to the aws-pca-issuer Helm Chart Values for additional details. It also supports all standard helm configuration options ( for Eg: https://github.com/aws-quickstart/cdk-eks-blueprints/blob/main/docs/addons/index.md#standard-helm-add-on-configuration-options) cert-manager compatibility with EKS and Fargate \u00b6 Please refer to the cert-manager compatibility and open issues with EKS and Fargate [cert-manager compatibility with EKS](https://cert-manager.io/docs/installation/compatibility/#aws-eks_ Validation \u00b6 To validate that aws-pca-issuer is installed properly in the cluster, check if the namespace aws-pca-issuer is created Verify if the namespace is created correctly kubectl get ns | grep \"aws-pca-issuer\" There should be list the pca namespace aws-pca-issuer Active 31m Verify the objects under namespace aws-pca-issuer kubectl get all -n aws-pca-issuer It should give results as below For Eg: NAME READY STATUS RESTARTS AGE pod/aws-pca-issuer-aws-privateca-issuer-7b9df7c7cc-vz8hw 1 /1 Running 0 3m2s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE service/aws-pca-issuer-aws-privateca-issuer ClusterIP 172 .20.17.134 <none> 8080 /TCP 3m3s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/aws-pca-issuer-aws-privateca-issuer 1 /1 1 1 3m2s Testing \u00b6 1) Create an ACM Private CA For this testing create a private certificate authority in ACM Private CA with RSA 2048 selected as the key algorithm. You can create a CA using the AWS console Once your private CA is active note down the ARN 2) Create a K8s namespace for testing purpose kubectl create ns acm-pca-demo 3) Change the current context to namespace acm-pca-demo kubectl config set-context --current --namespace = acm-pca-demo 4) Create CRD AWSPCAIssuer with name demo-awspcs-issuer >> AWSPCAIssuer.yaml AWSPCAIssuer This is a regular namespaced issuer that can be used as a reference in your Certificate CRs. AWSPCAClusterIssuer This CR is identical to the AWSPCAIssuer. The only difference being that it\u2019s not namespaced and can be referenced from anywhere. In thi example we will use AWSPCAIssuer Replace the arn Replace ${AWS_REGION} with your target region and ${ARN} with the ARN of CM Private CA recieved from step 1 --- apiVersion : awspca.cert-manager.io/v1beta1 kind : AWSPCAIssuer metadata : name : demo-awspcs-issuer namespace : acm-pca-demo spec : arn : ${ARN} region : ${AWS_REGION} --- Apply the yaml file kubectl apply -f AWSPCAIssuer.yaml Verify AWSPCAIssuer installed correctly kubectl describe AWSPCAIssuer Check the Events section and you must see the message Issuer verified if everything goes correct Normal Verified 46s ( x2 over 46s ) awspcaissuer-controller Issuer verified 4) Create CRD Certificate with name rsa-cert-2048 for dns name rsa-2048.example.com >> Certificate.yaml For th formats other than 2048 check the examples --- kind : Certificate apiVersion : cert-manager.io/v1 metadata : name : rsa-cert-2048 spec : commonName : www.rsa-2048.example.com dnsNames : - www.rsa-2048.example.com - rsa-2048.example.com duration : 2160h0m0s issuerRef : group : awspca.cert-manager.io kind : AWSPCAIssuer name : demo-awspcs-issuer renewBefore : 360h0m0s secretName : rsa-example-cert-2048 usages : - server auth - client auth privateKey : algorithm : \"RSA\" size : 2048 --- Apply the yaml file kubectl apply -f Certificate.yaml Verify Certificate is installed correctly kubectl get Certificates It should output Ready as True as shown below NAME READY SECRET AGE rsa-cert-2048 True rsa-example-cert-2048 31s The actual certificate file is stored as a secret. To see the details of secret get the secret name k describe certificate | grep Secret Output Secret Name: rsa-example-cert-2048 Describe the secret to get the value kubectl describe secret rsa-example-cert-2048 Troubleshooting \u00b6 Please use kubectl get events for debugging. kubectl get events Sample Output for Successfull Certificate Request 5s Normal cert-manager.io certificaterequest/rsa-cert-2048.io-zqftp Certificate request has been approved by cert-manager.io 2s Normal Issued certificaterequest/rsa-cert-2048.io-zqftp certificate issued 5s Normal Issuing certificate/rsa-cert-2048 Issuing certificate as Secret does not exist 5s Normal Generated certificate/rsa-cert-2048 Stored new private key in temporary Secret resource \"rsa-cert-2048-k7zxv\" 5s Normal Requested certificate/rsa-cert-2048 Created new CertificateRequest resource \"rsa-cert-2048-zqftp\" 2s Normal Issuing certificate/rsa-cert-2048 The certificate has been successfully issued 8m22s Normal Verified awspcaissuer/rsa-cert-2048 Issuer verified 85s Normal Verified awspcaissuer/rsa-cert-2048 Issuer verified","title":"AWS Private CA Issuer"},{"location":"addons/aws-privateca-issuer/#aws-private-ca-issuer-add-on","text":"This addon will install aws-privateca-issuer AWS ACM Private CA is a module of the AWS Certificate Manager that can setup and manage private CAs. The AWS PrivateCA Issuer plugin acts as an addon to cert-manager that signs certificate requests using ACM Private CA. Since its an addon to cert-manager, for Installing AWS ACM Private CA Addon, You must install cert-manager Addon first cert-manager is a Kubernetes add-on to automate the management and issuance of TLS certificates from various issuing sources. It will ensure certificates are valid and up to date periodically, and attempt to renew certificates at an appropriate time before expiry.","title":"AWS Private CA Issuer Add-on"},{"location":"addons/aws-privateca-issuer/#usage","text":"Please ensure that cert-manager addon is already installed import 'source-map-support/register' ; import * as cdk from 'aws-cdk-lib' ; import * as blueprints from '@aws-quickstart/eks-blueprints' ; const app = new cdk . App (); const awsPcaParams = { iamPolicies : [ \"AWSCertificateManagerPrivateCAFullAccess\" ] } const addOn = new blueprints . addons . AWSPrivateCAIssuerAddon ( awsPcaParams ) const blueprint = blueprints . EksBlueprint . builder () . addOns ( addOn ) . build ( app , 'my-stack-name' );","title":"Usage"},{"location":"addons/aws-privateca-issuer/#configuration-options","text":"serviceAccountName : (string) User provided name for service account. The default value is aws-pca-issuer iamPolicies - An array of Managed IAM Policies which Service Account needs for IRSA Eg: irsaRoles:[\"AWSCertificateManagerPrivateCAFullAccess\"]. If not empty the Service Account will be created by the CDK with IAM Roles Mapped (IRSA). In case if its empty, Service Account will be created with out default IAM Policy - \"AWSCertificateManagerPrivateCAFullAccess\" values : Arbitrary values to pass to the chart. Refer to the aws-pca-issuer Helm Chart Values for additional details. It also supports all standard helm configuration options ( for Eg: https://github.com/aws-quickstart/cdk-eks-blueprints/blob/main/docs/addons/index.md#standard-helm-add-on-configuration-options)","title":"Configuration Options"},{"location":"addons/aws-privateca-issuer/#cert-manager-compatibility-with-eks-and-fargate","text":"Please refer to the cert-manager compatibility and open issues with EKS and Fargate [cert-manager compatibility with EKS](https://cert-manager.io/docs/installation/compatibility/#aws-eks_","title":"cert-manager compatibility with EKS and Fargate"},{"location":"addons/aws-privateca-issuer/#validation","text":"To validate that aws-pca-issuer is installed properly in the cluster, check if the namespace aws-pca-issuer is created Verify if the namespace is created correctly kubectl get ns | grep \"aws-pca-issuer\" There should be list the pca namespace aws-pca-issuer Active 31m Verify the objects under namespace aws-pca-issuer kubectl get all -n aws-pca-issuer It should give results as below For Eg: NAME READY STATUS RESTARTS AGE pod/aws-pca-issuer-aws-privateca-issuer-7b9df7c7cc-vz8hw 1 /1 Running 0 3m2s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE service/aws-pca-issuer-aws-privateca-issuer ClusterIP 172 .20.17.134 <none> 8080 /TCP 3m3s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/aws-pca-issuer-aws-privateca-issuer 1 /1 1 1 3m2s","title":"Validation"},{"location":"addons/aws-privateca-issuer/#testing","text":"1) Create an ACM Private CA For this testing create a private certificate authority in ACM Private CA with RSA 2048 selected as the key algorithm. You can create a CA using the AWS console Once your private CA is active note down the ARN 2) Create a K8s namespace for testing purpose kubectl create ns acm-pca-demo 3) Change the current context to namespace acm-pca-demo kubectl config set-context --current --namespace = acm-pca-demo 4) Create CRD AWSPCAIssuer with name demo-awspcs-issuer >> AWSPCAIssuer.yaml AWSPCAIssuer This is a regular namespaced issuer that can be used as a reference in your Certificate CRs. AWSPCAClusterIssuer This CR is identical to the AWSPCAIssuer. The only difference being that it\u2019s not namespaced and can be referenced from anywhere. In thi example we will use AWSPCAIssuer Replace the arn Replace ${AWS_REGION} with your target region and ${ARN} with the ARN of CM Private CA recieved from step 1 --- apiVersion : awspca.cert-manager.io/v1beta1 kind : AWSPCAIssuer metadata : name : demo-awspcs-issuer namespace : acm-pca-demo spec : arn : ${ARN} region : ${AWS_REGION} --- Apply the yaml file kubectl apply -f AWSPCAIssuer.yaml Verify AWSPCAIssuer installed correctly kubectl describe AWSPCAIssuer Check the Events section and you must see the message Issuer verified if everything goes correct Normal Verified 46s ( x2 over 46s ) awspcaissuer-controller Issuer verified 4) Create CRD Certificate with name rsa-cert-2048 for dns name rsa-2048.example.com >> Certificate.yaml For th formats other than 2048 check the examples --- kind : Certificate apiVersion : cert-manager.io/v1 metadata : name : rsa-cert-2048 spec : commonName : www.rsa-2048.example.com dnsNames : - www.rsa-2048.example.com - rsa-2048.example.com duration : 2160h0m0s issuerRef : group : awspca.cert-manager.io kind : AWSPCAIssuer name : demo-awspcs-issuer renewBefore : 360h0m0s secretName : rsa-example-cert-2048 usages : - server auth - client auth privateKey : algorithm : \"RSA\" size : 2048 --- Apply the yaml file kubectl apply -f Certificate.yaml Verify Certificate is installed correctly kubectl get Certificates It should output Ready as True as shown below NAME READY SECRET AGE rsa-cert-2048 True rsa-example-cert-2048 31s The actual certificate file is stored as a secret. To see the details of secret get the secret name k describe certificate | grep Secret Output Secret Name: rsa-example-cert-2048 Describe the secret to get the value kubectl describe secret rsa-example-cert-2048","title":"Testing"},{"location":"addons/aws-privateca-issuer/#troubleshooting","text":"Please use kubectl get events for debugging. kubectl get events Sample Output for Successfull Certificate Request 5s Normal cert-manager.io certificaterequest/rsa-cert-2048.io-zqftp Certificate request has been approved by cert-manager.io 2s Normal Issued certificaterequest/rsa-cert-2048.io-zqftp certificate issued 5s Normal Issuing certificate/rsa-cert-2048 Issuing certificate as Secret does not exist 5s Normal Generated certificate/rsa-cert-2048 Stored new private key in temporary Secret resource \"rsa-cert-2048-k7zxv\" 5s Normal Requested certificate/rsa-cert-2048 Created new CertificateRequest resource \"rsa-cert-2048-zqftp\" 2s Normal Issuing certificate/rsa-cert-2048 The certificate has been successfully issued 8m22s Normal Verified awspcaissuer/rsa-cert-2048 Issuer verified 85s Normal Verified awspcaissuer/rsa-cert-2048 Issuer verified","title":"Troubleshooting"},{"location":"addons/calico-operator/","text":"Calico Operator Add-on \u00b6 Project Calico is an open source networking and network security solution for containers, virtual machines, and native host-based workloads. To secure workloads in Kubernetes, Calico utilizes Network Policies. The Calico Operator add-on adds support for Calico to an EKS cluster by deploying Tigera Operator . By default, the native VPC-CNI plugin for Kubernetes on EKS does not support Kubernetes Network Policies. Installing Calico (or alternate CNI provider) will enable customers to define and apply standard Kubernetes Network Policies to their EKS cluster. Calico add-on supports standard helm configuration options . Usage \u00b6 import 'source-map-support/register' ; import * as cdk from 'aws-cdk-lib' ; import * as blueprints from '@aws-quickstart/eks-blueprints' ; const app = new cdk . App (); const addOn = new blueprints . addons . CalicoOperatorAddOn (); const blueprint = blueprints . EksBlueprint . builder () . addOns ( addOn ) . build ( app , 'my-stack-name' ); Upgrading from Calico Add-on \u00b6 To upgrade from Calico add-on the following two step process has been validated to work: Remove CalicoAddOn from the blueprint and deploy (via cdk deploy or via pipeline support). Add CalicoOperatorAddOn to the blueprint and deploy again. Applying Network Policies \u00b6 In the Getting Started guide, we bootstrapped an EKS cluster with the workloads contained in the eks-blueprints-workloads repository . Below, we will demonstrate how we can apply network policies to govern traffic between the workloads once Calico is installed. To start, we can verify that there are no network policies in place in your EKS cluster. kubectl get networkpolicy -A This means that all resources within the cluster should be able to make ingress and egress connections with other resources within and outside the cluster. You can verify, for example, that you are able to ping a team-burnham pod from a team-riker pod. To do so, first retrieve the podIP from the team-burnham namespace. BURNHAM_POD = $( kubectl get pod -n team-burnham -o jsonpath = '{.items[0].metadata.name}' ) BURNHAM_POD_IP = $( kubectl get pod -n team-burnham $BURNHAM_POD -o jsonpath = '{.status.podIP}' ) Now you can start a shell from the pod in the team-riker namespace and ping the pod from team-burnham namespace: RIKER_POD = $( kubectl -n team-riker get pod -o jsonpath = '{.items[0].metadata.name}' ) kubectl exec -ti -n team-riker $RIKER_POD -- sh Note: since this opens a shell inside the pod, it will not have the environment variables saved above. You should retrieve the actual podIP from the environment variable BURNHAM_POD_IP . With those actual values, curl the IP and port 80 of the pod from team-burnham : # curl -s <Team Burnham Pod IP>:80>/dev/null && echo Success. || echo Fail. You should see Success. Applying Kubernetes Network Policy to block traffic \u00b6 Let's apply the following Network Policy: kind : NetworkPolicy apiVersion : networking.k8s.io/v1 metadata : name : default-deny spec : podSelector : matchLabels : {} Save it as deny-all.yaml . Run the following commands to apply the policy to both team-riker and team-burnham namespaces: kubectl -n team-riker apply -f deny-all.yaml kubectl -n team-burnham apply -f deny-all.yaml This will prevent access to all resources within both namespaces. Try curl commands from above to verify that it fails. Applying additional policy to re-open pod to pod communications \u00b6 You can apply a new Kubernetes Network Policy on top of the previous to \u201cpoke holes\u201d for egress and ingress needs. For example, if you want to be able to curl from the team-riker pod to the team-burnham pod, the following Kubernetes NetworkPolicy should be applied. kind : NetworkPolicy apiVersion : networking.k8s.io/v1 metadata : namespace : team-burnham name : allow-riker-to-burnham spec : podSelector : matchLabels : app : guestbook-ui policyTypes : - Ingress - Egress ingress : - from : - podSelector : matchLabels : app : guestbook-ui namespaceSelector : matchLabels : name : team-riker ports : - protocol : TCP port : 80 egress : - to : - podSelector : matchLabels : app : guestbook-ui namespaceSelector : matchLabels : name : team-riker ports : - protocol : TCP port : 80 Save as allow-burnham-riker.yaml and apply the new NetworkPolicy: kubectl apply -f allow-burnham-riker.yaml Once the policy is applied, once again try the curl command from above. You should now see Success. once again. Securing your environment with Kubernetes Network Policies \u00b6 Calico also allows Custom Resource Definitions (CRD) which provides the ability to add features not in the standard Kubernetes Network Policies, such as: Explicit Deny rules Layer 7 rule support (i.e. Http Request types) Endpoint support other than standard pods: OpenShift, VMs, interfaces, etc. In order to use CRDs (in particular defined within the projectcalico.org/v3 Calico API), you must install the Calico CLI ( calicoctl ). You can find more information about Calico Network Policy and using calicoctl here .","title":"Calico Operator"},{"location":"addons/calico-operator/#calico-operator-add-on","text":"Project Calico is an open source networking and network security solution for containers, virtual machines, and native host-based workloads. To secure workloads in Kubernetes, Calico utilizes Network Policies. The Calico Operator add-on adds support for Calico to an EKS cluster by deploying Tigera Operator . By default, the native VPC-CNI plugin for Kubernetes on EKS does not support Kubernetes Network Policies. Installing Calico (or alternate CNI provider) will enable customers to define and apply standard Kubernetes Network Policies to their EKS cluster. Calico add-on supports standard helm configuration options .","title":"Calico Operator Add-on"},{"location":"addons/calico-operator/#usage","text":"import 'source-map-support/register' ; import * as cdk from 'aws-cdk-lib' ; import * as blueprints from '@aws-quickstart/eks-blueprints' ; const app = new cdk . App (); const addOn = new blueprints . addons . CalicoOperatorAddOn (); const blueprint = blueprints . EksBlueprint . builder () . addOns ( addOn ) . build ( app , 'my-stack-name' );","title":"Usage"},{"location":"addons/calico-operator/#upgrading-from-calico-add-on","text":"To upgrade from Calico add-on the following two step process has been validated to work: Remove CalicoAddOn from the blueprint and deploy (via cdk deploy or via pipeline support). Add CalicoOperatorAddOn to the blueprint and deploy again.","title":"Upgrading from Calico Add-on"},{"location":"addons/calico-operator/#applying-network-policies","text":"In the Getting Started guide, we bootstrapped an EKS cluster with the workloads contained in the eks-blueprints-workloads repository . Below, we will demonstrate how we can apply network policies to govern traffic between the workloads once Calico is installed. To start, we can verify that there are no network policies in place in your EKS cluster. kubectl get networkpolicy -A This means that all resources within the cluster should be able to make ingress and egress connections with other resources within and outside the cluster. You can verify, for example, that you are able to ping a team-burnham pod from a team-riker pod. To do so, first retrieve the podIP from the team-burnham namespace. BURNHAM_POD = $( kubectl get pod -n team-burnham -o jsonpath = '{.items[0].metadata.name}' ) BURNHAM_POD_IP = $( kubectl get pod -n team-burnham $BURNHAM_POD -o jsonpath = '{.status.podIP}' ) Now you can start a shell from the pod in the team-riker namespace and ping the pod from team-burnham namespace: RIKER_POD = $( kubectl -n team-riker get pod -o jsonpath = '{.items[0].metadata.name}' ) kubectl exec -ti -n team-riker $RIKER_POD -- sh Note: since this opens a shell inside the pod, it will not have the environment variables saved above. You should retrieve the actual podIP from the environment variable BURNHAM_POD_IP . With those actual values, curl the IP and port 80 of the pod from team-burnham : # curl -s <Team Burnham Pod IP>:80>/dev/null && echo Success. || echo Fail. You should see Success.","title":"Applying Network Policies"},{"location":"addons/calico-operator/#applying-kubernetes-network-policy-to-block-traffic","text":"Let's apply the following Network Policy: kind : NetworkPolicy apiVersion : networking.k8s.io/v1 metadata : name : default-deny spec : podSelector : matchLabels : {} Save it as deny-all.yaml . Run the following commands to apply the policy to both team-riker and team-burnham namespaces: kubectl -n team-riker apply -f deny-all.yaml kubectl -n team-burnham apply -f deny-all.yaml This will prevent access to all resources within both namespaces. Try curl commands from above to verify that it fails.","title":"Applying Kubernetes Network Policy to block traffic"},{"location":"addons/calico-operator/#applying-additional-policy-to-re-open-pod-to-pod-communications","text":"You can apply a new Kubernetes Network Policy on top of the previous to \u201cpoke holes\u201d for egress and ingress needs. For example, if you want to be able to curl from the team-riker pod to the team-burnham pod, the following Kubernetes NetworkPolicy should be applied. kind : NetworkPolicy apiVersion : networking.k8s.io/v1 metadata : namespace : team-burnham name : allow-riker-to-burnham spec : podSelector : matchLabels : app : guestbook-ui policyTypes : - Ingress - Egress ingress : - from : - podSelector : matchLabels : app : guestbook-ui namespaceSelector : matchLabels : name : team-riker ports : - protocol : TCP port : 80 egress : - to : - podSelector : matchLabels : app : guestbook-ui namespaceSelector : matchLabels : name : team-riker ports : - protocol : TCP port : 80 Save as allow-burnham-riker.yaml and apply the new NetworkPolicy: kubectl apply -f allow-burnham-riker.yaml Once the policy is applied, once again try the curl command from above. You should now see Success. once again.","title":"Applying additional policy to re-open pod to pod communications"},{"location":"addons/calico-operator/#securing-your-environment-with-kubernetes-network-policies","text":"Calico also allows Custom Resource Definitions (CRD) which provides the ability to add features not in the standard Kubernetes Network Policies, such as: Explicit Deny rules Layer 7 rule support (i.e. Http Request types) Endpoint support other than standard pods: OpenShift, VMs, interfaces, etc. In order to use CRDs (in particular defined within the projectcalico.org/v3 Calico API), you must install the Calico CLI ( calicoctl ). You can find more information about Calico Network Policy and using calicoctl here .","title":"Securing your environment with Kubernetes Network Policies"},{"location":"addons/calico/","text":"Calico Add-on \u00b6 This add-on is deprecated as AWS/Tigera stopped support for the Calico helm chart published on EKS charts. Please use Calico Operator Add-on instead, including instructions for migration from Calico add-on.","title":"Calico"},{"location":"addons/calico/#calico-add-on","text":"This add-on is deprecated as AWS/Tigera stopped support for the Calico helm chart published on EKS charts. Please use Calico Operator Add-on instead, including instructions for migration from Calico add-on.","title":"Calico Add-on"},{"location":"addons/cert-manager/","text":"Certificate Manager Add-on \u00b6 This add-on installs cert-manager . cert-manager adds certificates and certificate issuers as resource types in Kubernetes clusters, and simplifies the process of obtaining, renewing and using those certificates. Add-on can issue certificates from a variety of supported sources, including Let's Encrypt, HashiCorp Vault, and Venafi as well as private PKI. It will ensure certificates are valid and up to date, and attempt to renew certificates at a configured time before expiry. Usage \u00b6 import * as cdk from 'aws-cdk-lib' ; import * as blueprints from '@aws-quickstart/eks-blueprints' ; const app = new cdk . App (); const addOn = new blueprints . addons . CertManagerAddOn () const blueprint = blueprints . EksBlueprint . builder () . addOns ( addOn ) . build ( app , 'my-stack-name' ); Configuration Options \u00b6 installCRDs : (boolean) To automatically install and manage the CRDs as part of your Helm release, createNamespace : (boolean) If you want CDK to create the namespace for you values : Arbitrary values to pass to the chart. Refer to the cert-manager Helm Chart documentation for additional details. It also supports all standard helm configuration options ( for Eg: https://github.com/aws-quickstart/cdk-eks-blueprints/blob/main/docs/addons/index.md#standard-helm-add-on-configuration-options) cert-manager compatibility with EKS and Fargate \u00b6 Please refer to the cert-manager compatibility and open issues with EKS and Fargate [cert-manager compatibility with EKS](https://cert-manager.io/docs/installation/compatibility/#aws-eks_ Validation \u00b6 To validate that cert-manager is installed properly in the cluster, check if the namespace is created and cert-manger pods are running. Verify if the namespace is created correctly kubectl get ns | grep \"cert-manager\" There should be list the cert-manager namespace cert-manager Active 31m Verify if the pods are running correctly in cert-manager namespace kubectl get pods -n cert-manager There should list 3 pods starting with name cert-manager- For Eg: NAME READY STATUS RESTARTS AGE cert-manager-5bb7949947-vxf76 1 /1 Running 0 2m56s cert-manager-cainjector-5ff98c66d-g4kpv 1 /1 Running 0 2m56s cert-manager-webhook-fb48856b5-bpsbl 1 /1 Running 0 2m56s Testing \u00b6 Cert-Manager Kubectl Plugin Cert-Manager has a Kubectl plugin that simplifies some common management tasks. It also lets you check whether Cert-Manager is up and ready to serve requests. Install Cert-Manager Kubectl Plugin curl -L -o kubectl-cert-manager.tar.gz https://github.com/jetstack/cert-manager/releases/latest/download/kubectl-cert_manager-linux-amd64.tar.gz tar xzf kubectl-cert-manager.tar.gz sudo mv kubectl-cert_manager /usr/local/bin Run the follwing command to check if the plugin was correctly installed: kubectl cert-manager check api This should print following message \"The cert-manager API is ready\"","title":"Certificate Manager"},{"location":"addons/cert-manager/#certificate-manager-add-on","text":"This add-on installs cert-manager . cert-manager adds certificates and certificate issuers as resource types in Kubernetes clusters, and simplifies the process of obtaining, renewing and using those certificates. Add-on can issue certificates from a variety of supported sources, including Let's Encrypt, HashiCorp Vault, and Venafi as well as private PKI. It will ensure certificates are valid and up to date, and attempt to renew certificates at a configured time before expiry.","title":"Certificate Manager Add-on"},{"location":"addons/cert-manager/#usage","text":"import * as cdk from 'aws-cdk-lib' ; import * as blueprints from '@aws-quickstart/eks-blueprints' ; const app = new cdk . App (); const addOn = new blueprints . addons . CertManagerAddOn () const blueprint = blueprints . EksBlueprint . builder () . addOns ( addOn ) . build ( app , 'my-stack-name' );","title":"Usage"},{"location":"addons/cert-manager/#configuration-options","text":"installCRDs : (boolean) To automatically install and manage the CRDs as part of your Helm release, createNamespace : (boolean) If you want CDK to create the namespace for you values : Arbitrary values to pass to the chart. Refer to the cert-manager Helm Chart documentation for additional details. It also supports all standard helm configuration options ( for Eg: https://github.com/aws-quickstart/cdk-eks-blueprints/blob/main/docs/addons/index.md#standard-helm-add-on-configuration-options)","title":"Configuration Options"},{"location":"addons/cert-manager/#cert-manager-compatibility-with-eks-and-fargate","text":"Please refer to the cert-manager compatibility and open issues with EKS and Fargate [cert-manager compatibility with EKS](https://cert-manager.io/docs/installation/compatibility/#aws-eks_","title":"cert-manager compatibility with EKS and Fargate"},{"location":"addons/cert-manager/#validation","text":"To validate that cert-manager is installed properly in the cluster, check if the namespace is created and cert-manger pods are running. Verify if the namespace is created correctly kubectl get ns | grep \"cert-manager\" There should be list the cert-manager namespace cert-manager Active 31m Verify if the pods are running correctly in cert-manager namespace kubectl get pods -n cert-manager There should list 3 pods starting with name cert-manager- For Eg: NAME READY STATUS RESTARTS AGE cert-manager-5bb7949947-vxf76 1 /1 Running 0 2m56s cert-manager-cainjector-5ff98c66d-g4kpv 1 /1 Running 0 2m56s cert-manager-webhook-fb48856b5-bpsbl 1 /1 Running 0 2m56s","title":"Validation"},{"location":"addons/cert-manager/#testing","text":"Cert-Manager Kubectl Plugin Cert-Manager has a Kubectl plugin that simplifies some common management tasks. It also lets you check whether Cert-Manager is up and ready to serve requests. Install Cert-Manager Kubectl Plugin curl -L -o kubectl-cert-manager.tar.gz https://github.com/jetstack/cert-manager/releases/latest/download/kubectl-cert_manager-linux-amd64.tar.gz tar xzf kubectl-cert-manager.tar.gz sudo mv kubectl-cert_manager /usr/local/bin Run the follwing command to check if the plugin was correctly installed: kubectl cert-manager check api This should print following message \"The cert-manager API is ready\"","title":"Testing"},{"location":"addons/cloudwatch-adot-addon/","text":"Amazon CloudWatch ADOT Add-on \u00b6 Amazon CloudWatch collects monitoring and operational data in the form of logs, metrics, and events. You get a unified view of operational health and gain complete visibility of your AWS resources, applications, and services running on AWS and on-premises. This add-on deploys an AWS Distro for OpenTelemetry (ADOT) Collector for CloudWatch which receives metrics and logs from the application and sends the same to CloudWatch console. You can change the mode to Daemonset, StatefulSet, and Sidecar depending upon your deployment strategy. This add-on is not automatically installed when you first create a cluster, it must be added to the cluster in order to setup CloudWatch for remote write metrics. For more information on the add-on, please review the user guide . Note: Due to lack of helm chart support and lack of \u201cserverside apply\u201d in the current version of EKS CW add-on cannot be used together with AMP add-on. Check this Github Issue for more information. Prerequisites \u00b6 adot EKS Blueprints add-on. Usage \u00b6 This add-on can used with two different patterns : Pattern # 1 : Simple and Easy - Using all default property values. This pattern deploys an ADOT collector with deployment as the mode to write traces to CloudWatch console. import * as cdk from 'aws-cdk-lib' ; import * as blueprints from '@aws-quickstart/eks-blueprints' ; const app = new cdk . App (); const addOn = new blueprints . addons . CloudWatchAdotAddOn (); const blueprint = blueprints . EksBlueprint . builder () . addOns ( addOn ) . build ( app , 'my-stack-name' ); Pattern # 2 : Overriding property value for different deployment Modes. This pattern deploys an ADOT collector on the namespace specified in namespace , name specified in name with daemonset as the mode to visualize metrics in CloudWatch console. Deployment mode can be overridden to any of these values - deployment , daemonset , statefulset , sidecar . Mode sidecar is to support Fargate profile. You can pass required metrics including custom metrics and required pod labels of application pods emitting custom metrics to visualize using metricsNameSelectors , podLabelRegex as parameters as shown below. import * as cdk from 'aws-cdk-lib' ; import * as blueprints from '@aws-quickstart/eks-blueprints' ; const app = new cdk . App (); const addOn = new blueprints . addons . CloudWatchAdotAddOn ({ deploymentMode : cloudWatchDeploymentMode.DEPLOYMENT , namespace : 'default' , name : 'adot-collector-cloudwatch' , metricsNameSelectors : [ 'apiserver_request_.*' , 'container_memory_.*' , 'container_threads' , 'otelcol_process_.*' ], podLabelRegex : 'frontend|downstream(.*)' }); const blueprint = blueprints . EksBlueprint . builder () . addOns ( addOn ) . build ( app , 'my-stack-name' ); Validation \u00b6 To validate whether CloudWatch add-on is installed properly, ensure that the required kubernetes resources are running in the cluster. kubectl get all -n default Output \u00b6 NAME READY STATUS RESTARTS AGE pod/otel-collector-cloudwatch-collector-7565f958c6-r485f 1 /1 Running 0 2m41s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE service/kubernetes ClusterIP 172 .20.0.1 <none> 443 /TCP 18h service/otel-collector-cloudwatch-collector-monitoring ClusterIP 172 .20.254.103 <none> 8888 /TCP 2m43s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/otel-collector-cloudwatch-collector 1 /1 1 1 2m42s NAME DESIRED CURRENT READY AGE replicaset.apps/otel-collector-cloudwatch-collector-7565f958c6 1 1 1 2m42s Functionality \u00b6 Applies the CloudWatch ADOT add-on to an Amazon EKS cluster.","title":"CloudWatch ADOT"},{"location":"addons/cloudwatch-adot-addon/#amazon-cloudwatch-adot-add-on","text":"Amazon CloudWatch collects monitoring and operational data in the form of logs, metrics, and events. You get a unified view of operational health and gain complete visibility of your AWS resources, applications, and services running on AWS and on-premises. This add-on deploys an AWS Distro for OpenTelemetry (ADOT) Collector for CloudWatch which receives metrics and logs from the application and sends the same to CloudWatch console. You can change the mode to Daemonset, StatefulSet, and Sidecar depending upon your deployment strategy. This add-on is not automatically installed when you first create a cluster, it must be added to the cluster in order to setup CloudWatch for remote write metrics. For more information on the add-on, please review the user guide . Note: Due to lack of helm chart support and lack of \u201cserverside apply\u201d in the current version of EKS CW add-on cannot be used together with AMP add-on. Check this Github Issue for more information.","title":"Amazon CloudWatch ADOT Add-on"},{"location":"addons/cloudwatch-adot-addon/#prerequisites","text":"adot EKS Blueprints add-on.","title":"Prerequisites"},{"location":"addons/cloudwatch-adot-addon/#usage","text":"This add-on can used with two different patterns : Pattern # 1 : Simple and Easy - Using all default property values. This pattern deploys an ADOT collector with deployment as the mode to write traces to CloudWatch console. import * as cdk from 'aws-cdk-lib' ; import * as blueprints from '@aws-quickstart/eks-blueprints' ; const app = new cdk . App (); const addOn = new blueprints . addons . CloudWatchAdotAddOn (); const blueprint = blueprints . EksBlueprint . builder () . addOns ( addOn ) . build ( app , 'my-stack-name' ); Pattern # 2 : Overriding property value for different deployment Modes. This pattern deploys an ADOT collector on the namespace specified in namespace , name specified in name with daemonset as the mode to visualize metrics in CloudWatch console. Deployment mode can be overridden to any of these values - deployment , daemonset , statefulset , sidecar . Mode sidecar is to support Fargate profile. You can pass required metrics including custom metrics and required pod labels of application pods emitting custom metrics to visualize using metricsNameSelectors , podLabelRegex as parameters as shown below. import * as cdk from 'aws-cdk-lib' ; import * as blueprints from '@aws-quickstart/eks-blueprints' ; const app = new cdk . App (); const addOn = new blueprints . addons . CloudWatchAdotAddOn ({ deploymentMode : cloudWatchDeploymentMode.DEPLOYMENT , namespace : 'default' , name : 'adot-collector-cloudwatch' , metricsNameSelectors : [ 'apiserver_request_.*' , 'container_memory_.*' , 'container_threads' , 'otelcol_process_.*' ], podLabelRegex : 'frontend|downstream(.*)' }); const blueprint = blueprints . EksBlueprint . builder () . addOns ( addOn ) . build ( app , 'my-stack-name' );","title":"Usage"},{"location":"addons/cloudwatch-adot-addon/#validation","text":"To validate whether CloudWatch add-on is installed properly, ensure that the required kubernetes resources are running in the cluster. kubectl get all -n default","title":"Validation"},{"location":"addons/cloudwatch-adot-addon/#output","text":"NAME READY STATUS RESTARTS AGE pod/otel-collector-cloudwatch-collector-7565f958c6-r485f 1 /1 Running 0 2m41s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE service/kubernetes ClusterIP 172 .20.0.1 <none> 443 /TCP 18h service/otel-collector-cloudwatch-collector-monitoring ClusterIP 172 .20.254.103 <none> 8888 /TCP 2m43s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/otel-collector-cloudwatch-collector 1 /1 1 1 2m42s NAME DESIRED CURRENT READY AGE replicaset.apps/otel-collector-cloudwatch-collector-7565f958c6 1 1 1 2m42s","title":"Output"},{"location":"addons/cloudwatch-adot-addon/#functionality","text":"Applies the CloudWatch ADOT add-on to an Amazon EKS cluster.","title":"Functionality"},{"location":"addons/cluster-autoscaler/","text":"Cluster Autoscaler Add-on \u00b6 The Cluster Autoscaler add-on adds support for Cluster Autoscaler to an EKS cluster. Cluster Autoscaler is a tool that automatically adjusts the number of nodes in your cluster when: pods fail due to insufficient resources, or pods are rescheduled onto other nodes due to being in nodes that are underutilized for an extended period of time. Usage \u00b6 import 'source-map-support/register' ; import * as cdk from 'aws-cdk-lib' ; import * as blueprints from '@aws-quickstart/eks-blueprints' ; const app = new cdk . App (); const addOn = new blueprints . addons . ClusterAutoscalerAddOn (); const blueprint = blueprints . EksBlueprint . builder () . addOns ( addOn ) . build ( app , 'my-stack-name' ); Functionality \u00b6 Configure proper IAM permissions (such as modify autoscaling groups, terminate instances, etc.) as a Policy. Configures IAM Role for Service Account (IRSA) with the generated policy. Resolves proper CA image to pull based on the Kubernetes version. Applies proper tags for discoverability to the EC2 instances. Supports standard helm configuration options . The add-on automatically sets the following Helm Chart values , and it is highly recommended not to pass these values in (as it will result in a failed deployment): - cloudProvider - autoDiscovery.clusterName - awsRegion - rbac.serviceAccount.create - rbac.serviceAccount.name Testing the scaling functionality \u00b6 The following steps will help test and validate Cluster Autoscaler functionality in your cluster. Deploy a sample app as a deployment. Create a Horizontal Pod Autoscaler (HPA) resource. Generate load to trigger scaling. Deploy a sample app \u00b6 Take a note of the number of nodes available: kubectl get nodes NAME STATUS ROLES AGE VERSION ip-10-0-189-107.us-west-2.compute.internal Ready <none> 80m v1.19.6-eks-49a6c0 The first step is to create a sample application via deployment and request 20m of CPU: kubectl create deployment php-apache --image = us.gcr.io/k8s-artifacts-prod/hpa-example kubectl set resources deploy php-apache --requests = cpu = 20m kubectl expose deployment php-apache --port 80 You can see that there's 1 pod currently running: kubectl get pod -l app = php-apache NAME READY STATUS RESTARTS AGE php-apache-55c4584468-vsbl7 1/1 Running 0 63s Create HPA resource \u00b6 Now we can create Horizontal Pod Autoscaler resource with 50% CPU target utilization, and the minimum number of pods at 1 and max at 20: kubectl autoscale deployment php-apache \\ --cpu-percent = 50 \\ --min = 1 \\ --max = 50 You can verify by looking at the hpa resource: kubectl get hpa NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE php-apache Deployment/php-apache 10%/50% 1 50 2 52s Generate load \u00b6 With the resources created, you can generate load on the apache server with a busybox container: kubectl --generator = run-pod/v1 run -i --tty load-generator --image = busybox /bin/sh You can generate the actual load on the shell by running a while loop: while true ; do wget -q -O - http://php-apache ; done Verify that Cluster Autoscaler works \u00b6 While the load is being generated, access another terminal to verify that HPA is working. The following command should return a list of many nods created (as many as 10): kubectl get pods -l app = php-apache -o wide --watch With more pods being created, you would expect more nodes to be created; you can access the Cluster Autoscaler logs to confirm: kubectl -n kube-system logs -f deployment/blueprints-addon-cluster-autoscaler-aws-cluster-autoscaler Lastly, you can list all the nodes and see that there are now multiple nodes: kubectl get nodes NAME STATUS ROLES AGE VERSION ip-10-0-187-70.us-west-2.compute.internal Ready <none> 73s v1.19.6-eks-49a6c0 ip-10-0-189-107.us-west-2.compute.internal Ready <none> 84m v1.19.6-eks-49a6c0 ip-10-0-224-226.us-west-2.compute.internal Ready <none> 46s v1.19.6-eks-49a6c0 ip-10-0-233-105.us-west-2.compute.internal Ready <none> 90s v1.19.6-eks-49a6c0","title":"Cluster Autoscaler"},{"location":"addons/cluster-autoscaler/#cluster-autoscaler-add-on","text":"The Cluster Autoscaler add-on adds support for Cluster Autoscaler to an EKS cluster. Cluster Autoscaler is a tool that automatically adjusts the number of nodes in your cluster when: pods fail due to insufficient resources, or pods are rescheduled onto other nodes due to being in nodes that are underutilized for an extended period of time.","title":"Cluster Autoscaler Add-on"},{"location":"addons/cluster-autoscaler/#usage","text":"import 'source-map-support/register' ; import * as cdk from 'aws-cdk-lib' ; import * as blueprints from '@aws-quickstart/eks-blueprints' ; const app = new cdk . App (); const addOn = new blueprints . addons . ClusterAutoscalerAddOn (); const blueprint = blueprints . EksBlueprint . builder () . addOns ( addOn ) . build ( app , 'my-stack-name' );","title":"Usage"},{"location":"addons/cluster-autoscaler/#functionality","text":"Configure proper IAM permissions (such as modify autoscaling groups, terminate instances, etc.) as a Policy. Configures IAM Role for Service Account (IRSA) with the generated policy. Resolves proper CA image to pull based on the Kubernetes version. Applies proper tags for discoverability to the EC2 instances. Supports standard helm configuration options . The add-on automatically sets the following Helm Chart values , and it is highly recommended not to pass these values in (as it will result in a failed deployment): - cloudProvider - autoDiscovery.clusterName - awsRegion - rbac.serviceAccount.create - rbac.serviceAccount.name","title":"Functionality"},{"location":"addons/cluster-autoscaler/#testing-the-scaling-functionality","text":"The following steps will help test and validate Cluster Autoscaler functionality in your cluster. Deploy a sample app as a deployment. Create a Horizontal Pod Autoscaler (HPA) resource. Generate load to trigger scaling.","title":"Testing the scaling functionality"},{"location":"addons/cluster-autoscaler/#deploy-a-sample-app","text":"Take a note of the number of nodes available: kubectl get nodes NAME STATUS ROLES AGE VERSION ip-10-0-189-107.us-west-2.compute.internal Ready <none> 80m v1.19.6-eks-49a6c0 The first step is to create a sample application via deployment and request 20m of CPU: kubectl create deployment php-apache --image = us.gcr.io/k8s-artifacts-prod/hpa-example kubectl set resources deploy php-apache --requests = cpu = 20m kubectl expose deployment php-apache --port 80 You can see that there's 1 pod currently running: kubectl get pod -l app = php-apache NAME READY STATUS RESTARTS AGE php-apache-55c4584468-vsbl7 1/1 Running 0 63s","title":"Deploy a sample app"},{"location":"addons/cluster-autoscaler/#create-hpa-resource","text":"Now we can create Horizontal Pod Autoscaler resource with 50% CPU target utilization, and the minimum number of pods at 1 and max at 20: kubectl autoscale deployment php-apache \\ --cpu-percent = 50 \\ --min = 1 \\ --max = 50 You can verify by looking at the hpa resource: kubectl get hpa NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE php-apache Deployment/php-apache 10%/50% 1 50 2 52s","title":"Create HPA resource"},{"location":"addons/cluster-autoscaler/#generate-load","text":"With the resources created, you can generate load on the apache server with a busybox container: kubectl --generator = run-pod/v1 run -i --tty load-generator --image = busybox /bin/sh You can generate the actual load on the shell by running a while loop: while true ; do wget -q -O - http://php-apache ; done","title":"Generate load"},{"location":"addons/cluster-autoscaler/#verify-that-cluster-autoscaler-works","text":"While the load is being generated, access another terminal to verify that HPA is working. The following command should return a list of many nods created (as many as 10): kubectl get pods -l app = php-apache -o wide --watch With more pods being created, you would expect more nodes to be created; you can access the Cluster Autoscaler logs to confirm: kubectl -n kube-system logs -f deployment/blueprints-addon-cluster-autoscaler-aws-cluster-autoscaler Lastly, you can list all the nodes and see that there are now multiple nodes: kubectl get nodes NAME STATUS ROLES AGE VERSION ip-10-0-187-70.us-west-2.compute.internal Ready <none> 73s v1.19.6-eks-49a6c0 ip-10-0-189-107.us-west-2.compute.internal Ready <none> 84m v1.19.6-eks-49a6c0 ip-10-0-224-226.us-west-2.compute.internal Ready <none> 46s v1.19.6-eks-49a6c0 ip-10-0-233-105.us-west-2.compute.internal Ready <none> 90s v1.19.6-eks-49a6c0","title":"Verify that Cluster Autoscaler works"},{"location":"addons/container-insights/","text":"Container Insights Add-on \u00b6 The Container Insights add-on adds support for Container Insights to an EKS cluster. Customers can use Container Insights to collect, aggregate, and summarize metrics and logs from your containerized applications and microservices. Container Insights collects data as performance log events using an embedded metric format. These performance log events are entries that use a structured JSON schema that enables high-cardinality data to be ingested and stored at scale. From this data, CloudWatch creates aggregated metrics at the cluster, node, pod, task, and service level as CloudWatch metrics. The metrics that Container Insights collects are available in CloudWatch automatic dashboards, and also viewable in the Metrics section of the CloudWatch console. IMPORTANT CloudWatch does not automatically create all possible metrics from the log data, to help you manage your Container Insights costs. However, you can view additional metrics and additional levels of granularity by using CloudWatch Logs Insights to analyze the raw performance log events. Metrics collected by Container Insights are charged as custom metrics. For more information about CloudWatch pricing , see Amazon CloudWatch Pricing. Usage \u00b6 Add the following as an add-on to your main.ts file to add Containers Insights to your cluster import 'source-map-support/register' ; import * as cdk from 'aws-cdk-lib' ; import * as blueprints from '@aws-quickstart/eks-blueprints' ; const app = new cdk . App (); const addOn = new blueprints . addons . ContainerInsightsAddOn (); const blueprint = blueprints . EksBlueprint . builder () . addOns ( addOn ) . build ( app , 'my-stack-name' ); Prerequisites \u00b6 Once the Container Insights add-on has been installed in your cluster, validate that the AWS Distro for Open Telemetry (ADOT) and the FluentBit daemons are running. kubectl get all -n amazon-cloudwatch kubectl get all -n amzn-cloudwatch-metrics You should see output similar to the following respectively (assuming two node cluster): NAME READY STATUS RESTARTS AGE pod/fluent-bit-5chvg 1/1 Running 2 100s pod/fluent-bit-px7r6 1/1 Running 0 101s NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE daemonset.apps/fluent-bit 2 2 2 2 2 <none> 100s NAME READY STATUS RESTARTS AGE pod/adot-collector-daemonset-b2rpc 1/1 Running 0 106s pod/adot-collector-daemonset-k6tfw 1/1 Running 2 106s NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE daemonset.apps/adot-collector-daemonset 2 2 2 2 2 <none> 106s To enable or disable control plane logs with the console, run the following command in your terminal. aws eks update-cluster-config \\ --region us-east-2 \\ --name east-dev \\ --logging '{\"clusterLogging\":[{\"types\":[\"api\",\"audit\",\"authenticator\",\"controllerManager\",\"scheduler\"],\"enabled\":true}]}' You should see a similar output as the following. { \"update\" : { \"id\" : \"<883405c8-65c6-4758-8cee-2a7c1340a6d9>\" , \"status\" : \"InProgress\" , \"type\" : \"LoggingUpdate\" , \"params\" : [ { \"type\" : \"ClusterLogging\" , \"value\" : \"{\\\"clusterLogging\\\":[{\\\"types\\\":[\\\"api\\\",\\\"audit\\\",\\\"authenticator\\\",\\\"controllerManager\\\",\\\"scheduler\\\"],\\\"enabled\\\":true}]}\" } ], \"createdAt\" : 1553271814.684 , \"errors\" : [] } } You can also monitor the status of your log configuration update to your cluster by running the following command. aws eks describe-update \\ --region <region-code> \\ --name <prod> \\ --update-id <883405c8-65c6-4758-8cee-2a7c1340a6d9> Once the update is complete, you should see a similar output. { \"update\" : { \"id\" : \"<883405c8-65c6-4758-8cee-2a7c1340a6d9>\" , \"status\" : \"Successful\" , \"type\" : \"LoggingUpdate\" , \"params\" : [ { \"type\" : \"ClusterLogging\" , \"value\" : \"{\\\"clusterLogging\\\":[{\\\"types\\\":[\\\"api\\\",\\\"audit\\\",\\\"authenticator\\\",\\\"controllerManager\\\",\\\"scheduler\\\"],\\\"enabled\\\":true}]}\" } ], \"createdAt\" : 1553271814.684 , \"errors\" : [] } } View metrics for cluster and workloads \u00b6 Under Performance Monitoring, the Container Insights dashboard allows you to hone in on both cluster and workload metrics. After selecting EKS Pods and Clusters, you will see that the dashboard provides CPU and memory utilization along with other important metrics such as network performance. View cluster level logs \u00b6 After you have enabled any of the control plane log types for your Amazon EKS cluster, you can view them on the CloudWatch console. To view these logs on the CloudWatch console follow these steps: Open the CloudWatch console and choose the cluster that you want to view logs for. The log group name format is /aws/eks/ /cluster. Choose the log stream to view. The following list describes the log stream name format for each log type. Kubernetes API server component logs (api) \u2013 kube-apiserver- Audit (audit) \u2013 kube-apiserver-audit- Authenticator (authenticator) \u2013 authenticator- Controller manager (controllerManager) \u2013 kube-controller-manager- Scheduler (scheduler) \u2013 kube-scheduler- Next in the console, click on Log groups under Logs. You will see under log streams all the log streams from your Amazon EKS control plane. View workload level logs \u00b6 In order to view workload level logs follow these steps after browsing to the CloudWatch Logs Insights console In the navigation pane, choose Insights. Near the top of the screen is the query editor. When you first open CloudWatch Logs Insights, this box contains a default query that returns the 20 most recent log events. In the box above the query editor, select one of the Container Insights log groups to query. For the following example queries to work, the log group name must end with performance. We will look at /aws/containerinsights/east-dev/performance When you select a log group, CloudWatch Logs Insights automatically detects fields in the data in the log group and displays them in Discovered fields in the right pane. It also displays a bar graph of log events in this log group over time. This bar graph shows the distribution of events in the log group that matches your query and time range, not only the events displayed in the table. In the query editor, replace the default query with the following query and choose Run query. STATS avg(node_cpu_utilization) as avg_node_cpu_utilization by NodeName SORT avg_node_cpu_utilization DESC This query shows a list of nodes, sorted by average node CPU utilization. Below is an example of what the visualization should look like. To try another example, replace that query with another query and choose Run query. More sample queries are listed later on this page. STATS avg(number_of_container_restarts) as avg_number_of_container_restarts by PodName SORT avg_number_of_container_restarts DESC This query displays a list of your pods, sorted by average number of container restarts as shown below If you want to try another query, you can use include fields in the list at the right of the screen. For more information about query syntax, see CloudWatch Logs Insights Query Syntax. View containers via that container map in container insights. \u00b6 In order to view a map of all of your containers running inside your cluster, click on View your container map in the Container Insights tab. You will then see a map of all of your namespaces and their associated pods and services.","title":"Container Insights"},{"location":"addons/container-insights/#container-insights-add-on","text":"The Container Insights add-on adds support for Container Insights to an EKS cluster. Customers can use Container Insights to collect, aggregate, and summarize metrics and logs from your containerized applications and microservices. Container Insights collects data as performance log events using an embedded metric format. These performance log events are entries that use a structured JSON schema that enables high-cardinality data to be ingested and stored at scale. From this data, CloudWatch creates aggregated metrics at the cluster, node, pod, task, and service level as CloudWatch metrics. The metrics that Container Insights collects are available in CloudWatch automatic dashboards, and also viewable in the Metrics section of the CloudWatch console. IMPORTANT CloudWatch does not automatically create all possible metrics from the log data, to help you manage your Container Insights costs. However, you can view additional metrics and additional levels of granularity by using CloudWatch Logs Insights to analyze the raw performance log events. Metrics collected by Container Insights are charged as custom metrics. For more information about CloudWatch pricing , see Amazon CloudWatch Pricing.","title":"Container Insights Add-on"},{"location":"addons/container-insights/#usage","text":"Add the following as an add-on to your main.ts file to add Containers Insights to your cluster import 'source-map-support/register' ; import * as cdk from 'aws-cdk-lib' ; import * as blueprints from '@aws-quickstart/eks-blueprints' ; const app = new cdk . App (); const addOn = new blueprints . addons . ContainerInsightsAddOn (); const blueprint = blueprints . EksBlueprint . builder () . addOns ( addOn ) . build ( app , 'my-stack-name' );","title":"Usage"},{"location":"addons/container-insights/#prerequisites","text":"Once the Container Insights add-on has been installed in your cluster, validate that the AWS Distro for Open Telemetry (ADOT) and the FluentBit daemons are running. kubectl get all -n amazon-cloudwatch kubectl get all -n amzn-cloudwatch-metrics You should see output similar to the following respectively (assuming two node cluster): NAME READY STATUS RESTARTS AGE pod/fluent-bit-5chvg 1/1 Running 2 100s pod/fluent-bit-px7r6 1/1 Running 0 101s NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE daemonset.apps/fluent-bit 2 2 2 2 2 <none> 100s NAME READY STATUS RESTARTS AGE pod/adot-collector-daemonset-b2rpc 1/1 Running 0 106s pod/adot-collector-daemonset-k6tfw 1/1 Running 2 106s NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE daemonset.apps/adot-collector-daemonset 2 2 2 2 2 <none> 106s To enable or disable control plane logs with the console, run the following command in your terminal. aws eks update-cluster-config \\ --region us-east-2 \\ --name east-dev \\ --logging '{\"clusterLogging\":[{\"types\":[\"api\",\"audit\",\"authenticator\",\"controllerManager\",\"scheduler\"],\"enabled\":true}]}' You should see a similar output as the following. { \"update\" : { \"id\" : \"<883405c8-65c6-4758-8cee-2a7c1340a6d9>\" , \"status\" : \"InProgress\" , \"type\" : \"LoggingUpdate\" , \"params\" : [ { \"type\" : \"ClusterLogging\" , \"value\" : \"{\\\"clusterLogging\\\":[{\\\"types\\\":[\\\"api\\\",\\\"audit\\\",\\\"authenticator\\\",\\\"controllerManager\\\",\\\"scheduler\\\"],\\\"enabled\\\":true}]}\" } ], \"createdAt\" : 1553271814.684 , \"errors\" : [] } } You can also monitor the status of your log configuration update to your cluster by running the following command. aws eks describe-update \\ --region <region-code> \\ --name <prod> \\ --update-id <883405c8-65c6-4758-8cee-2a7c1340a6d9> Once the update is complete, you should see a similar output. { \"update\" : { \"id\" : \"<883405c8-65c6-4758-8cee-2a7c1340a6d9>\" , \"status\" : \"Successful\" , \"type\" : \"LoggingUpdate\" , \"params\" : [ { \"type\" : \"ClusterLogging\" , \"value\" : \"{\\\"clusterLogging\\\":[{\\\"types\\\":[\\\"api\\\",\\\"audit\\\",\\\"authenticator\\\",\\\"controllerManager\\\",\\\"scheduler\\\"],\\\"enabled\\\":true}]}\" } ], \"createdAt\" : 1553271814.684 , \"errors\" : [] } }","title":"Prerequisites"},{"location":"addons/container-insights/#view-metrics-for-cluster-and-workloads","text":"Under Performance Monitoring, the Container Insights dashboard allows you to hone in on both cluster and workload metrics. After selecting EKS Pods and Clusters, you will see that the dashboard provides CPU and memory utilization along with other important metrics such as network performance.","title":"View metrics for cluster and workloads"},{"location":"addons/container-insights/#view-cluster-level-logs","text":"After you have enabled any of the control plane log types for your Amazon EKS cluster, you can view them on the CloudWatch console. To view these logs on the CloudWatch console follow these steps: Open the CloudWatch console and choose the cluster that you want to view logs for. The log group name format is /aws/eks/ /cluster. Choose the log stream to view. The following list describes the log stream name format for each log type. Kubernetes API server component logs (api) \u2013 kube-apiserver- Audit (audit) \u2013 kube-apiserver-audit- Authenticator (authenticator) \u2013 authenticator- Controller manager (controllerManager) \u2013 kube-controller-manager- Scheduler (scheduler) \u2013 kube-scheduler- Next in the console, click on Log groups under Logs. You will see under log streams all the log streams from your Amazon EKS control plane.","title":"View cluster level logs"},{"location":"addons/container-insights/#view-workload-level-logs","text":"In order to view workload level logs follow these steps after browsing to the CloudWatch Logs Insights console In the navigation pane, choose Insights. Near the top of the screen is the query editor. When you first open CloudWatch Logs Insights, this box contains a default query that returns the 20 most recent log events. In the box above the query editor, select one of the Container Insights log groups to query. For the following example queries to work, the log group name must end with performance. We will look at /aws/containerinsights/east-dev/performance When you select a log group, CloudWatch Logs Insights automatically detects fields in the data in the log group and displays them in Discovered fields in the right pane. It also displays a bar graph of log events in this log group over time. This bar graph shows the distribution of events in the log group that matches your query and time range, not only the events displayed in the table. In the query editor, replace the default query with the following query and choose Run query. STATS avg(node_cpu_utilization) as avg_node_cpu_utilization by NodeName SORT avg_node_cpu_utilization DESC This query shows a list of nodes, sorted by average node CPU utilization. Below is an example of what the visualization should look like. To try another example, replace that query with another query and choose Run query. More sample queries are listed later on this page. STATS avg(number_of_container_restarts) as avg_number_of_container_restarts by PodName SORT avg_number_of_container_restarts DESC This query displays a list of your pods, sorted by average number of container restarts as shown below If you want to try another query, you can use include fields in the list at the right of the screen. For more information about query syntax, see CloudWatch Logs Insights Query Syntax.","title":"View workload level logs"},{"location":"addons/container-insights/#view-containers-via-that-container-map-in-container-insights","text":"In order to view a map of all of your containers running inside your cluster, click on View your container map in the Container Insights tab. You will then see a map of all of your namespaces and their associated pods and services.","title":"View containers via that container map in container insights."},{"location":"addons/coredns/","text":"CoreDNS Amazon EKS Add-on \u00b6 The CoreDNS Amazon EKS Add-on adds support for CoreDNS . CoreDNS is a flexible, extensible DNS server that can serve as the Kubernetes cluster DNS. The CoreDNS Pods provide name resolution for all Pods in the cluster. For more information about CoreDNS, see Using CoreDNS for Service Discovery in the Kubernetes documentation. Installing CoreDNS as Amazon EKS add-on will reduce the amount of work that is needed to do in order to install, configure, and update CoreDNS. It includes the latest security patches, bug fixes and is validated by AWS to work with Amazon EKS. This ensures that Amazon EKS clusters are secure and stable. Amazon EKS automatically installs CoreDNS as self-managed add-on for every cluster. So if it is already running on your cluster, you can still install it as Amazon EKS add-on to start benefiting from the capabilities of Amazon EKS add-ons. Prerequisite \u00b6 Amazon EKS add-ons are only available with Amazon EKS clusters running Kubernetes version 1.18 and later. Usage \u00b6 import 'source-map-support/register' ; import * as cdk from 'aws-cdk-lib' ; import * as blueprints from '@aws-quickstart/eks-blueprints' ; const app = new cdk . App (); const addOn = new blueprints . addons . CoreDnsAddOn ( \"v1.8.0-eksbuild.1\" ); // optionally specify image version to pull or empty constructor const blueprint = blueprints . EksBlueprint . builder () . addOns ( addOn ) . build ( app , 'my-stack-name' ); Configuration Options \u00b6 version : Pass in the core-dns plugin version compatible with kubernetes-cluster version as shown below # Assuming cluster version is 1.19, below command shows versions of the CoreDNS add-on available for the specified cluster's version. aws eks describe-addon-versions \\ --addon-name coredns \\ --kubernetes-version 1 .19 \\ --query \"addons[].addonVersions[].[addonVersion, compatibilities[].defaultVersion]\" --output text # Output v1.8.3-eksbuild.1 False v1.8.0-eksbuild.1 True v1.7.0-eksbuild.1 False Validation \u00b6 To validate that coredns add-on is running, ensure that both the coredns pods are in Running state. $ kubectl get pods -n kube-system | grep coredns NAME READY STATUS RESTARTS AGE coredns-644944ff4-2hjkj 1 /1 Running 0 34d coredns-644944ff4-fz6p5 1 /1 Running 0 34d # Assuming cluster-name is my-cluster, below command shows the version of coredns installed. Check if it is same as the version installed via EKS add-on aws eks describe-addon \\ --cluster-name my-cluster \\ --addon-name coredns \\ --query \"addon.addonVersion\" \\ --output text # Output v1.8.0-eksbuild.1 Functionality \u00b6 Applies CoreDNS add-on to an Amazon EKS cluster.","title":"CoreDns"},{"location":"addons/coredns/#coredns-amazon-eks-add-on","text":"The CoreDNS Amazon EKS Add-on adds support for CoreDNS . CoreDNS is a flexible, extensible DNS server that can serve as the Kubernetes cluster DNS. The CoreDNS Pods provide name resolution for all Pods in the cluster. For more information about CoreDNS, see Using CoreDNS for Service Discovery in the Kubernetes documentation. Installing CoreDNS as Amazon EKS add-on will reduce the amount of work that is needed to do in order to install, configure, and update CoreDNS. It includes the latest security patches, bug fixes and is validated by AWS to work with Amazon EKS. This ensures that Amazon EKS clusters are secure and stable. Amazon EKS automatically installs CoreDNS as self-managed add-on for every cluster. So if it is already running on your cluster, you can still install it as Amazon EKS add-on to start benefiting from the capabilities of Amazon EKS add-ons.","title":"CoreDNS Amazon EKS Add-on"},{"location":"addons/coredns/#prerequisite","text":"Amazon EKS add-ons are only available with Amazon EKS clusters running Kubernetes version 1.18 and later.","title":"Prerequisite"},{"location":"addons/coredns/#usage","text":"import 'source-map-support/register' ; import * as cdk from 'aws-cdk-lib' ; import * as blueprints from '@aws-quickstart/eks-blueprints' ; const app = new cdk . App (); const addOn = new blueprints . addons . CoreDnsAddOn ( \"v1.8.0-eksbuild.1\" ); // optionally specify image version to pull or empty constructor const blueprint = blueprints . EksBlueprint . builder () . addOns ( addOn ) . build ( app , 'my-stack-name' );","title":"Usage"},{"location":"addons/coredns/#configuration-options","text":"version : Pass in the core-dns plugin version compatible with kubernetes-cluster version as shown below # Assuming cluster version is 1.19, below command shows versions of the CoreDNS add-on available for the specified cluster's version. aws eks describe-addon-versions \\ --addon-name coredns \\ --kubernetes-version 1 .19 \\ --query \"addons[].addonVersions[].[addonVersion, compatibilities[].defaultVersion]\" --output text # Output v1.8.3-eksbuild.1 False v1.8.0-eksbuild.1 True v1.7.0-eksbuild.1 False","title":"Configuration Options"},{"location":"addons/coredns/#validation","text":"To validate that coredns add-on is running, ensure that both the coredns pods are in Running state. $ kubectl get pods -n kube-system | grep coredns NAME READY STATUS RESTARTS AGE coredns-644944ff4-2hjkj 1 /1 Running 0 34d coredns-644944ff4-fz6p5 1 /1 Running 0 34d # Assuming cluster-name is my-cluster, below command shows the version of coredns installed. Check if it is same as the version installed via EKS add-on aws eks describe-addon \\ --cluster-name my-cluster \\ --addon-name coredns \\ --query \"addon.addonVersion\" \\ --output text # Output v1.8.0-eksbuild.1","title":"Validation"},{"location":"addons/coredns/#functionality","text":"Applies CoreDNS add-on to an Amazon EKS cluster.","title":"Functionality"},{"location":"addons/datadog/","text":"Datadog Amazon EKS Blueprints AddOn \u00b6 The Datadog Blueprints AddOn deploys the Datadog Agent on Amazon EKS using the eks-blueprints CDK construct. Installation \u00b6 npm install @datadog/datadog-eks-blueprints-addon Usage \u00b6 Using an existing Kubernetes secret \u00b6 import * as cdk from 'aws-cdk-lib' ; import * as blueprints from '@aws-quickstart/eks-blueprints' ; import { DatadogAddOn } from '@datadog/datadog-eks-blueprints-addon' ; const app = new cdk . App (); const addOns : Array < blueprints . ClusterAddOn > = [ new DatadogAddOn ({ // Kubernetes secret holding Datadog API key // The value should be set with the `api-key` key in the secret object. apiKeyExistingSecret : '<secret name>' }) ]; const account = '<aws account id>' const region = '<aws region>' const props = { env : { account , region } } new blueprints . EksBlueprint ( app , { id : '<eks cluster name>' , addOns }, props ) Using AWS Secrets Manager \u00b6 Store the API key using AWS Secrets Manager: aws secretsmanager create-secret --name <secret name> --secret-string <api_key> --region <aws region> Refer to the previously created secret with apiKeyAWSSecret : import * as cdk from 'aws-cdk-lib' ; import * as blueprints from '@aws-quickstart/eks-blueprints' ; import { DatadogAddOn } from '@datadog/datadog-eks-blueprints-addon' ; const app = new cdk . App (); const addOns : Array < blueprints . ClusterAddOn > = [ new DatadogAddOn ({ apiKeyAWSSecret : '<secret name>' }) ]; const account = '<aws account id>' const region = '<aws region>' const props = { env : { account , region } } new blueprints . EksBlueprint ( app , { id : '<eks cluster name>' , addOns }, props ) AddOn Options \u00b6 Option Description Default apiKey Your Datadog API key \"\" appKey Your Datadog APP key \"\" apiKeyExistingSecret Existing k8s Secret storing the API key \"\" appKeyExistingSecret Existing k8s Secret storing the APP key \"\" apiKeyAWSSecret Secret in AWS Secrets Manager storing the API key \"\" appKeyAWSSecret Secret in AWS Secrets Manager storing the APP key \"\" namespace Namespace to install the Datadog Agent \"default\" version Version of the Datadog Helm chart \"2.28.13\" release Name of the Helm release \"datadog\" repository Repository of the Helm chart \"https://helm.datadoghq.com\" values Configuration values passed to the chart. See options . {} Support \u00b6 https://www.datadoghq.com/support/","title":"Datadog"},{"location":"addons/datadog/#datadog-amazon-eks-blueprints-addon","text":"The Datadog Blueprints AddOn deploys the Datadog Agent on Amazon EKS using the eks-blueprints CDK construct.","title":"Datadog Amazon EKS Blueprints AddOn"},{"location":"addons/datadog/#installation","text":"npm install @datadog/datadog-eks-blueprints-addon","title":"Installation"},{"location":"addons/datadog/#usage","text":"","title":"Usage"},{"location":"addons/datadog/#using-an-existing-kubernetes-secret","text":"import * as cdk from 'aws-cdk-lib' ; import * as blueprints from '@aws-quickstart/eks-blueprints' ; import { DatadogAddOn } from '@datadog/datadog-eks-blueprints-addon' ; const app = new cdk . App (); const addOns : Array < blueprints . ClusterAddOn > = [ new DatadogAddOn ({ // Kubernetes secret holding Datadog API key // The value should be set with the `api-key` key in the secret object. apiKeyExistingSecret : '<secret name>' }) ]; const account = '<aws account id>' const region = '<aws region>' const props = { env : { account , region } } new blueprints . EksBlueprint ( app , { id : '<eks cluster name>' , addOns }, props )","title":"Using an existing Kubernetes secret"},{"location":"addons/datadog/#using-aws-secrets-manager","text":"Store the API key using AWS Secrets Manager: aws secretsmanager create-secret --name <secret name> --secret-string <api_key> --region <aws region> Refer to the previously created secret with apiKeyAWSSecret : import * as cdk from 'aws-cdk-lib' ; import * as blueprints from '@aws-quickstart/eks-blueprints' ; import { DatadogAddOn } from '@datadog/datadog-eks-blueprints-addon' ; const app = new cdk . App (); const addOns : Array < blueprints . ClusterAddOn > = [ new DatadogAddOn ({ apiKeyAWSSecret : '<secret name>' }) ]; const account = '<aws account id>' const region = '<aws region>' const props = { env : { account , region } } new blueprints . EksBlueprint ( app , { id : '<eks cluster name>' , addOns }, props )","title":"Using AWS Secrets Manager"},{"location":"addons/datadog/#addon-options","text":"Option Description Default apiKey Your Datadog API key \"\" appKey Your Datadog APP key \"\" apiKeyExistingSecret Existing k8s Secret storing the API key \"\" appKeyExistingSecret Existing k8s Secret storing the APP key \"\" apiKeyAWSSecret Secret in AWS Secrets Manager storing the API key \"\" appKeyAWSSecret Secret in AWS Secrets Manager storing the APP key \"\" namespace Namespace to install the Datadog Agent \"default\" version Version of the Datadog Helm chart \"2.28.13\" release Name of the Helm release \"datadog\" repository Repository of the Helm chart \"https://helm.datadoghq.com\" values Configuration values passed to the chart. See options . {}","title":"AddOn Options"},{"location":"addons/datadog/#support","text":"https://www.datadoghq.com/support/","title":"Support"},{"location":"addons/ebs-csi-driver/","text":"EBS CSI Driver Amazon EKS Add-on \u00b6 The EBS CSI Driver Amazon EKS Add-on allows Amazon Elastic Kubernetes Service (Amazon EKS) clusters to manage the lifecycle of Amazon EBS volumes for persistent volumes. The driver allows you to use AWS KMS keys to encrypt EBS volumes (optionally). This driver is not automatically installed when you first create a cluster, it must be added to the cluster in order to manage EBS volumes. For more information on the driver, please review the user guide . Prerequisites \u00b6 Amazon EKS EBS CSI Driver add-on is only available on Amazon EKS clusters running Kubernetes version 1.20 and later. Note that the version of the driver that can be used on an EKS cluster depends on the version of Kubernetes running in the cluster. See the configuration options section below for more details Usage \u00b6 import 'source-map-support/register' ; import * as cdk from 'aws-cdk-lib' ; import * as blueprints from '@aws-quickstart/eks-blueprints' ; const app = new cdk . App (); const addOn = new blueprints . addons . EbsCsiDriverAddOn ({ kmsKeys : [ blueprints . getResource ( ( context ) => new kms . Key ( context . scope , \"ebs-csi-driver-key\" , { alias : \"ebs-csi-driver-key\" , }) ), ], }), const blueprint = blueprints . EksBlueprint . builder () . addOns ( addOn ) . build ( app , 'my-stack-name' ); Configuration Options \u00b6 version : Version of the EBS CSI Driver add-on to be installed. The version must be compatible with kubernetes cluster version. # Command to show versions of the EBS CSI Driver add-on available for cluster version is 1.20 aws eks describe-addon-versions \\ --addon-name aws-ebs-csi-driver \\ --kubernetes-version 1 .20 \\ --query \"addons[].addonVersions[].[addonVersion, compatibilities[].defaultVersion]\" --output text # Output v1.4.0-eksbuild.preview Validation \u00b6 To validate that EBS CSI Driver add-on is installed properly, ensure that the ebs pods are running in the cluster kubectl get pods -n kube-system | grep ebs # Output ebs-csi-controller-95848f4d9-hlrzs 4 /4 Running 0 5m8s ebs-csi-controller-95848f4d9-m4f54 4 /4 Running 0 4m38s ebs-csi-node-c9xdf 3 /3 Running 0 5m8s Additionally, the aws cli can be used to determine which version of the add-on is installed in the cluster # Assuming cluster-name is my-cluster, below command shows the version of coredns installed. Check if it is same as the version installed via EKS add-on aws eks describe-addon \\ --cluster-name my-cluster \\ --addon-name aws-ebs-csi-driver \\ --query \"addon.addonVersion\" \\ --output text # Output v1.4.0-eksbuild.preview Functionality \u00b6 Applies the EBS CSI Driver add-on to an Amazon EKS cluster.","title":"EBS CSI Driver"},{"location":"addons/ebs-csi-driver/#ebs-csi-driver-amazon-eks-add-on","text":"The EBS CSI Driver Amazon EKS Add-on allows Amazon Elastic Kubernetes Service (Amazon EKS) clusters to manage the lifecycle of Amazon EBS volumes for persistent volumes. The driver allows you to use AWS KMS keys to encrypt EBS volumes (optionally). This driver is not automatically installed when you first create a cluster, it must be added to the cluster in order to manage EBS volumes. For more information on the driver, please review the user guide .","title":"EBS CSI Driver Amazon EKS Add-on"},{"location":"addons/ebs-csi-driver/#prerequisites","text":"Amazon EKS EBS CSI Driver add-on is only available on Amazon EKS clusters running Kubernetes version 1.20 and later. Note that the version of the driver that can be used on an EKS cluster depends on the version of Kubernetes running in the cluster. See the configuration options section below for more details","title":"Prerequisites"},{"location":"addons/ebs-csi-driver/#usage","text":"import 'source-map-support/register' ; import * as cdk from 'aws-cdk-lib' ; import * as blueprints from '@aws-quickstart/eks-blueprints' ; const app = new cdk . App (); const addOn = new blueprints . addons . EbsCsiDriverAddOn ({ kmsKeys : [ blueprints . getResource ( ( context ) => new kms . Key ( context . scope , \"ebs-csi-driver-key\" , { alias : \"ebs-csi-driver-key\" , }) ), ], }), const blueprint = blueprints . EksBlueprint . builder () . addOns ( addOn ) . build ( app , 'my-stack-name' );","title":"Usage"},{"location":"addons/ebs-csi-driver/#configuration-options","text":"version : Version of the EBS CSI Driver add-on to be installed. The version must be compatible with kubernetes cluster version. # Command to show versions of the EBS CSI Driver add-on available for cluster version is 1.20 aws eks describe-addon-versions \\ --addon-name aws-ebs-csi-driver \\ --kubernetes-version 1 .20 \\ --query \"addons[].addonVersions[].[addonVersion, compatibilities[].defaultVersion]\" --output text # Output v1.4.0-eksbuild.preview","title":"Configuration Options"},{"location":"addons/ebs-csi-driver/#validation","text":"To validate that EBS CSI Driver add-on is installed properly, ensure that the ebs pods are running in the cluster kubectl get pods -n kube-system | grep ebs # Output ebs-csi-controller-95848f4d9-hlrzs 4 /4 Running 0 5m8s ebs-csi-controller-95848f4d9-m4f54 4 /4 Running 0 4m38s ebs-csi-node-c9xdf 3 /3 Running 0 5m8s Additionally, the aws cli can be used to determine which version of the add-on is installed in the cluster # Assuming cluster-name is my-cluster, below command shows the version of coredns installed. Check if it is same as the version installed via EKS add-on aws eks describe-addon \\ --cluster-name my-cluster \\ --addon-name aws-ebs-csi-driver \\ --query \"addon.addonVersion\" \\ --output text # Output v1.4.0-eksbuild.preview","title":"Validation"},{"location":"addons/ebs-csi-driver/#functionality","text":"Applies the EBS CSI Driver add-on to an Amazon EKS cluster.","title":"Functionality"},{"location":"addons/efs-csi-driver/","text":"EFS CSI Driver Amazon EKS Add-on \u00b6 The EFS CSI Driver Amazon EKS Add-on provides a CSI interface that allows Kubernetes clusters running on AWS to manage the lifecycle of Amazon EFS volumes for persistent storage. EFS CSI driver supports both dynamic and static provisioning of storage. A couple of things to note: Driver is not compatible with Windows-based container images The number of replicas to be deployed must be less or equal to the number of nodes in the cluster For more information on the driver, please review the user guide . Prerequisites \u00b6 The EFS file system itself must be created in AWS separately as the driver uses the EFS for storage, but it does not create it. You can create an EFS file system using the CreateEfsFileSystemProvider , e.g.: .resourceProvider(\"efs-file-system\", new CreateEfsFileSystemProvider('efs-file-system')) Usage \u00b6 import 'source-map-support/register' ; import * as cdk from 'aws-cdk-lib' ; import * as blueprints from '@aws-quickstart/eks-blueprints' ; const app = new cdk . App (); const addOn = new blueprints . addons . EfsCsiDriverAddOn (); const blueprint = blueprints . EksBlueprint . builder () . addOns ( addOn ) . build ( app , 'my-stack-name' ); Configuration Options \u00b6 version : Version of the EFS CSI Driver add-on to be installed. Version 2.2.3 will be installed by default if a value is not provided replicaCount : Number of replicas to be deployed. If not provided, two replicas will be deployed. Note that the number of replicas should be less than or equal to the number of nodes in the cluster otherwise some pods will be left of pending state kmsKeys : List of KMS keys used for encryption-at-rest, so that the IAM policy can be updated to allow the EFS CSI driver to access the keys Validation \u00b6 To validate that EFS CSI Driver add-on is installed properly, ensure that the ebs pods are running in the cluster kubectl get pods -n kube-system | grep efs # Output efs-csi-controller-7c9bd5d86d-v7jtk 3 /3 Running 0 155m efs-csi-node-2c29j 3 /3 Running 0 155m Additionally, the driver documentation shows how to create an EFS file system to test the driver Functionality \u00b6 Applies the EFS CSI Driver add-on to an Amazon EKS cluster.","title":"EFS CSI Driver"},{"location":"addons/efs-csi-driver/#efs-csi-driver-amazon-eks-add-on","text":"The EFS CSI Driver Amazon EKS Add-on provides a CSI interface that allows Kubernetes clusters running on AWS to manage the lifecycle of Amazon EFS volumes for persistent storage. EFS CSI driver supports both dynamic and static provisioning of storage. A couple of things to note: Driver is not compatible with Windows-based container images The number of replicas to be deployed must be less or equal to the number of nodes in the cluster For more information on the driver, please review the user guide .","title":"EFS CSI Driver Amazon EKS Add-on"},{"location":"addons/efs-csi-driver/#prerequisites","text":"The EFS file system itself must be created in AWS separately as the driver uses the EFS for storage, but it does not create it. You can create an EFS file system using the CreateEfsFileSystemProvider , e.g.: .resourceProvider(\"efs-file-system\", new CreateEfsFileSystemProvider('efs-file-system'))","title":"Prerequisites"},{"location":"addons/efs-csi-driver/#usage","text":"import 'source-map-support/register' ; import * as cdk from 'aws-cdk-lib' ; import * as blueprints from '@aws-quickstart/eks-blueprints' ; const app = new cdk . App (); const addOn = new blueprints . addons . EfsCsiDriverAddOn (); const blueprint = blueprints . EksBlueprint . builder () . addOns ( addOn ) . build ( app , 'my-stack-name' );","title":"Usage"},{"location":"addons/efs-csi-driver/#configuration-options","text":"version : Version of the EFS CSI Driver add-on to be installed. Version 2.2.3 will be installed by default if a value is not provided replicaCount : Number of replicas to be deployed. If not provided, two replicas will be deployed. Note that the number of replicas should be less than or equal to the number of nodes in the cluster otherwise some pods will be left of pending state kmsKeys : List of KMS keys used for encryption-at-rest, so that the IAM policy can be updated to allow the EFS CSI driver to access the keys","title":"Configuration Options"},{"location":"addons/efs-csi-driver/#validation","text":"To validate that EFS CSI Driver add-on is installed properly, ensure that the ebs pods are running in the cluster kubectl get pods -n kube-system | grep efs # Output efs-csi-controller-7c9bd5d86d-v7jtk 3 /3 Running 0 155m efs-csi-node-2c29j 3 /3 Running 0 155m Additionally, the driver documentation shows how to create an EFS file system to test the driver","title":"Validation"},{"location":"addons/efs-csi-driver/#functionality","text":"Applies the EFS CSI Driver add-on to an Amazon EKS cluster.","title":"Functionality"},{"location":"addons/emr-eks/","text":"EMR on EKS Add-on \u00b6 The Amazon EMR on EKS Add-on enables EMR on EKS service to use an EKS cluster. It creates the EMR on EKS IAM Service Linked Role and adds it to awsAuth configmap. This Add-on MUST be used with EMR on EKS Team . Usage \u00b6 import 'source-map-support/register' ; import * as cdk from 'aws-cdk-lib' ; import * as blueprints from '@aws-quickstart/eks-blueprints' ; const app = new cdk . App (); const addOn = new blueprints . addons . EmrEksAddOn (); const blueprint = blueprints . EksBlueprint . builder () . addOns ( addOn ) . build ( app , 'my-stack-name' ); Verify \u00b6 Once the AddOn is deployed you can execute the following command: kubectl describe -n kube-system configmap/aws-auth The output of the command would show a list of IAM role and mapping to Kuberenets users, one fo the mapping would be for EMR on EKS role and would be similar to below. mapRoles: | - rolearn: arn:aws:iam::<your-account-id>:role/AWSServiceRoleForAmazonEMRContainers username: emr-containers","title":"EMR on EKS"},{"location":"addons/emr-eks/#emr-on-eks-add-on","text":"The Amazon EMR on EKS Add-on enables EMR on EKS service to use an EKS cluster. It creates the EMR on EKS IAM Service Linked Role and adds it to awsAuth configmap. This Add-on MUST be used with EMR on EKS Team .","title":"EMR on EKS Add-on"},{"location":"addons/emr-eks/#usage","text":"import 'source-map-support/register' ; import * as cdk from 'aws-cdk-lib' ; import * as blueprints from '@aws-quickstart/eks-blueprints' ; const app = new cdk . App (); const addOn = new blueprints . addons . EmrEksAddOn (); const blueprint = blueprints . EksBlueprint . builder () . addOns ( addOn ) . build ( app , 'my-stack-name' );","title":"Usage"},{"location":"addons/emr-eks/#verify","text":"Once the AddOn is deployed you can execute the following command: kubectl describe -n kube-system configmap/aws-auth The output of the command would show a list of IAM role and mapping to Kuberenets users, one fo the mapping would be for EMR on EKS role and would be similar to below. mapRoles: | - rolearn: arn:aws:iam::<your-account-id>:role/AWSServiceRoleForAmazonEMRContainers username: emr-containers","title":"Verify"},{"location":"addons/external-dns/","text":"External DNS Add-on \u00b6 External DNS add-on is based on the ExternalDNS open source project and allows integration of exposed Kubernetes services and Ingresses with DNS providers, in particular Amazon Route 53 . The add-on provides functionality to configure IAM policies and Kubernetes service accounts for Route 53 integration support based on AWS Tutorial for External DNS . Usage \u00b6 import 'source-map-support/register' ; import * as cdk from 'aws-cdk-lib' ; import * as blueprints from '@aws-quickstart/eks-blueprints' ; const app = new cdk . App (); const hostedZoneName = ... const addOn = new blueprints . addons . ExternalDnsAddOn ({ hostedZoneProviders : [ hostedZoneName ]; // can be multiple }); const blueprint = blueprints . EksBlueprint . builder () . addOns ( addOn ) . resourceProvider ( hostedZoneName , new blueprints . addons . LookupHostedZoneProvider ( hostedZoneName )) . addOns ( addOn ) . build ( app , 'my-stack-name' ); To validate that external DNS add-on is running ensure that the add-on deployment is in RUNNING state: # Assuming add-on is installed in the external-dns namespace. $ kubectl get po -n external-dns NAME READY STATUS RESTARTS AGE external-dns-fcf6c9c66-xd8f4 1 /1 Running 0 3d3h Using External DNS \u00b6 You can now provision external services and ingresses integrating with Route 53. For example to provision an NLB you can use the following service manifest: apiVersion : v1 kind : Service metadata : annotations : service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout : '60' service.beta.kubernetes.io/aws-load-balancer-type : nlb external-dns.alpha.kubernetes.io/hostname : <MyDomainName> name : udp-test1 spec : type : LoadBalancer ports : - port : 80 protocol : TCP targetPort : 80 selector : name : your-app Note the external-dns.alpha.kubernetes.io/hostname annotation for the service name that allows specifying its domain name. Hosted Zone Providers \u00b6 In order for external DNS to work, you need to supply one or more hosted zones. Hosted zones are expected to be supplied leveraging resource providers . To help customers handle common use cases for Route 53 provisioning the framework provides a few convenience providers that can be registered with the EKS Blueprint Stack. Name look-up and direct import provider: This provider will allow to bind to an existing hosted zone based on its name. const myDomainName = \"\" ; blueprints . EksBlueprint . builder () // Register hosted zone1 under the name of MyHostedZone1 . resourceProvider ( \"MyHostedZone1\" , new blueprints . LookupHostedZoneProvider ( myDomainName )) . addOns ( new blueprints . addons . ExternalDnsAddOn ({ hostedZoneProviders : [ \"MyHostedZone1\" ]; }) . build (...); If the hosted zone ID is known, then the recommended approach is to use a ImportHostedZoneProvider which bypasses any lookup calls. const myHostedZoneId = \"\" ; blueprints . EksBlueprint . builder () // Register hosted zone1 under the name of MyHostedZone1 . resourceProvider ( \"MyHostedZone1\" , new blueprints . addons . ImportHostedZoneProvider ( myHostedZoneId )) . addOns ( new blueprints . addons . ExternalDnsAddOn ({ hostedZoneProviders : [ \"MyHostedZone1\" ]; }) . build (...); Delegating Hosted Zone Provider In many cases, enterprises choose to decouple provisioning of root domains and subdomains. A common pattern is to have a specific DNS account (AWS account) hosting the root domain, while subdomains may be created within individual workload accounts. This implies cross-account access from child account to the parent DNS account for subdomain delegation. Prerequisites: Parent account defines an IAM role with the following managed policies: AmazonRoute53DomainsFullAccess AmazonRoute53ReadOnlyAccess AmazonRoute53AutoNamingFullAccess Create trust relationship between this role and the child account that is expected to add subdomains. For info see IAM tutorial . Example: Let's assume that parent DNS account parentAccountId has domain named myglobal-domain.com . Now when provisioned an EKS Blueprints cluster you would like to use a stage specific name like dev.myglobal-domain.com or test.myglobal-domain.com . In addition to these requirements, you would like to enable tenant specific access to those domains such as my-tenant1.dev.myglobal-domain.com or team specific access team-riker.dev.myglobal-domain.com . The setup will look the following way: In the parentAccountId account you create a role for delegation ( DomainOperatorRole in this example) and a trust relationship to the child account in which EKS Blueprints will be provisioned. In a general case, the number of child accounts can be large, so each of them will have to be listed in the trust relationship. In the parentAccountId you create a public hosted zone for myglobal-domain.com . With that the setup that may require separate automation (or a manual process) is complete. Use the following configuration of the add-on: blueprints . EksBlueprint . builder () // Register hosted zone1 under the name of MyHostedZone1 . resourceProvider ( \"MyHostedZone1\" , new blueprints . DelegatingHostedZoneProvider ({ parentDomain : 'myglobal-domain.com' , subdomain : 'dev.myglobal-domain.com' , parentAccountId : parentDnsAccountId , delegatingRoleName : 'DomainOperatorRole' , wildcardSubdomain : true }) . addOns ( new blueprints . addons . ExternalDnsAddOn ({ hostedZoneProviders : [ \"MyHostedZone1\" ]; }) The parameter wildcardSubdomain above when set to true will also create a CNAME for *.dev.myglobal-domain.com . This is at the moment used for host based routing within EKS (e.g. with NGINX ingress). Configuration Options \u00b6 namespace : Optional target namespace where add-on will be installed. Changing this value on the operating cluster is not recommended. Set to external-dns by default. version : The add-on is leveraging a Bitnami helm chart. This parameter allows overriding the helm chart version used. hostedZone : Hosted zone provider is a interface that provides one or more hosted zones that the add-on will leverage for the service and ingress configuration. Functionality \u00b6 Applies External-DNS configuration for AWS DNS provider. See AWS Tutorial for External DNS for more information. Supports standard helm configuration options .","title":"External DNS"},{"location":"addons/external-dns/#external-dns-add-on","text":"External DNS add-on is based on the ExternalDNS open source project and allows integration of exposed Kubernetes services and Ingresses with DNS providers, in particular Amazon Route 53 . The add-on provides functionality to configure IAM policies and Kubernetes service accounts for Route 53 integration support based on AWS Tutorial for External DNS .","title":"External DNS Add-on"},{"location":"addons/external-dns/#usage","text":"import 'source-map-support/register' ; import * as cdk from 'aws-cdk-lib' ; import * as blueprints from '@aws-quickstart/eks-blueprints' ; const app = new cdk . App (); const hostedZoneName = ... const addOn = new blueprints . addons . ExternalDnsAddOn ({ hostedZoneProviders : [ hostedZoneName ]; // can be multiple }); const blueprint = blueprints . EksBlueprint . builder () . addOns ( addOn ) . resourceProvider ( hostedZoneName , new blueprints . addons . LookupHostedZoneProvider ( hostedZoneName )) . addOns ( addOn ) . build ( app , 'my-stack-name' ); To validate that external DNS add-on is running ensure that the add-on deployment is in RUNNING state: # Assuming add-on is installed in the external-dns namespace. $ kubectl get po -n external-dns NAME READY STATUS RESTARTS AGE external-dns-fcf6c9c66-xd8f4 1 /1 Running 0 3d3h","title":"Usage"},{"location":"addons/external-dns/#using-external-dns","text":"You can now provision external services and ingresses integrating with Route 53. For example to provision an NLB you can use the following service manifest: apiVersion : v1 kind : Service metadata : annotations : service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout : '60' service.beta.kubernetes.io/aws-load-balancer-type : nlb external-dns.alpha.kubernetes.io/hostname : <MyDomainName> name : udp-test1 spec : type : LoadBalancer ports : - port : 80 protocol : TCP targetPort : 80 selector : name : your-app Note the external-dns.alpha.kubernetes.io/hostname annotation for the service name that allows specifying its domain name.","title":"Using External DNS"},{"location":"addons/external-dns/#hosted-zone-providers","text":"In order for external DNS to work, you need to supply one or more hosted zones. Hosted zones are expected to be supplied leveraging resource providers . To help customers handle common use cases for Route 53 provisioning the framework provides a few convenience providers that can be registered with the EKS Blueprint Stack. Name look-up and direct import provider: This provider will allow to bind to an existing hosted zone based on its name. const myDomainName = \"\" ; blueprints . EksBlueprint . builder () // Register hosted zone1 under the name of MyHostedZone1 . resourceProvider ( \"MyHostedZone1\" , new blueprints . LookupHostedZoneProvider ( myDomainName )) . addOns ( new blueprints . addons . ExternalDnsAddOn ({ hostedZoneProviders : [ \"MyHostedZone1\" ]; }) . build (...); If the hosted zone ID is known, then the recommended approach is to use a ImportHostedZoneProvider which bypasses any lookup calls. const myHostedZoneId = \"\" ; blueprints . EksBlueprint . builder () // Register hosted zone1 under the name of MyHostedZone1 . resourceProvider ( \"MyHostedZone1\" , new blueprints . addons . ImportHostedZoneProvider ( myHostedZoneId )) . addOns ( new blueprints . addons . ExternalDnsAddOn ({ hostedZoneProviders : [ \"MyHostedZone1\" ]; }) . build (...); Delegating Hosted Zone Provider In many cases, enterprises choose to decouple provisioning of root domains and subdomains. A common pattern is to have a specific DNS account (AWS account) hosting the root domain, while subdomains may be created within individual workload accounts. This implies cross-account access from child account to the parent DNS account for subdomain delegation. Prerequisites: Parent account defines an IAM role with the following managed policies: AmazonRoute53DomainsFullAccess AmazonRoute53ReadOnlyAccess AmazonRoute53AutoNamingFullAccess Create trust relationship between this role and the child account that is expected to add subdomains. For info see IAM tutorial . Example: Let's assume that parent DNS account parentAccountId has domain named myglobal-domain.com . Now when provisioned an EKS Blueprints cluster you would like to use a stage specific name like dev.myglobal-domain.com or test.myglobal-domain.com . In addition to these requirements, you would like to enable tenant specific access to those domains such as my-tenant1.dev.myglobal-domain.com or team specific access team-riker.dev.myglobal-domain.com . The setup will look the following way: In the parentAccountId account you create a role for delegation ( DomainOperatorRole in this example) and a trust relationship to the child account in which EKS Blueprints will be provisioned. In a general case, the number of child accounts can be large, so each of them will have to be listed in the trust relationship. In the parentAccountId you create a public hosted zone for myglobal-domain.com . With that the setup that may require separate automation (or a manual process) is complete. Use the following configuration of the add-on: blueprints . EksBlueprint . builder () // Register hosted zone1 under the name of MyHostedZone1 . resourceProvider ( \"MyHostedZone1\" , new blueprints . DelegatingHostedZoneProvider ({ parentDomain : 'myglobal-domain.com' , subdomain : 'dev.myglobal-domain.com' , parentAccountId : parentDnsAccountId , delegatingRoleName : 'DomainOperatorRole' , wildcardSubdomain : true }) . addOns ( new blueprints . addons . ExternalDnsAddOn ({ hostedZoneProviders : [ \"MyHostedZone1\" ]; }) The parameter wildcardSubdomain above when set to true will also create a CNAME for *.dev.myglobal-domain.com . This is at the moment used for host based routing within EKS (e.g. with NGINX ingress).","title":"Hosted Zone Providers"},{"location":"addons/external-dns/#configuration-options","text":"namespace : Optional target namespace where add-on will be installed. Changing this value on the operating cluster is not recommended. Set to external-dns by default. version : The add-on is leveraging a Bitnami helm chart. This parameter allows overriding the helm chart version used. hostedZone : Hosted zone provider is a interface that provides one or more hosted zones that the add-on will leverage for the service and ingress configuration.","title":"Configuration Options"},{"location":"addons/external-dns/#functionality","text":"Applies External-DNS configuration for AWS DNS provider. See AWS Tutorial for External DNS for more information. Supports standard helm configuration options .","title":"Functionality"},{"location":"addons/external-secrets/","text":"External Secrets Add-On \u00b6 External Secrets add-on is based on External Secrets Operator (ESO) and allows integration with third-party secret stores like AWS Secrets Manager and inject the values into the EKS cluster as Kubernetes Secrets Usage \u00b6 import * as cdk from 'aws-cdk-lib' ; import * as blueprints from '@aws-quickstart/eks-blueprints' ; const app = new cdk . App (); const addOn = new blueprints . addons . ExternalsSecretsAddOn ({}); const blueprint = blueprints . EksBlueprint . builder () . addOns ( addOn ) . build ( app , 'my-stack-name' ); Cluster Secret Store \u00b6 Create a ClusterSecretStore which can be used by all ExternalSecrets from all namespaces import * as eks from 'aws-cdk-lib/aws-eks' ; const cluster = blueprint . getClusterInfo (). cluster ; const clusterSecretStore = new eks . KubernetesManifest ( scope , \"ClusterSecretStore\" , { cluster : cluster , manifest : [ { apiVersion : \"external-secrets.io/v1beta1\" , kind : \"ClusterSecretStore\" , metadata : { name : \"default\" }, spec : { provider : { aws : { service : \"SecretsManager\" , region : region , auth : { jwt : { serviceAccountRef : { name : \"external-secrets-sa\" , namespace : \"external-secrets\" , }, }, }, }, }, }, }, ], }); External Secret \u00b6 Create an ExternalSecret which can be used to fetch, transform and inject secret data import * as eks from 'aws-cdk-lib/aws-eks' ; const cluster = blueprint . getClusterInfo (). cluster ; const keyfiles = new eks . KubernetesManifest ( scope , \"ExternalSecret\" , { cluster : cluster , manifest : [ { apiVersion : \"external-secrets.io/v1beta1\" , kind : \"ExternalSecret\" , metadata : { name : \"the-external-secret-name\" }, spec : { secretStoreRef : { name : \"default\" , kind : \"ClusterSecretStore\" , }, target : { name : \"the-kubernetes-secret-name\" , creationPolicy : \"Merge\" , }, data : [ { secretKey : \"secret-key-to-be-managed\" , remoteRef : { key : \"the-providers-secret-name\" , property : \"the-provider-secret-property\" , }, }, ], }, }, ], });","title":"External Secrets"},{"location":"addons/external-secrets/#external-secrets-add-on","text":"External Secrets add-on is based on External Secrets Operator (ESO) and allows integration with third-party secret stores like AWS Secrets Manager and inject the values into the EKS cluster as Kubernetes Secrets","title":"External Secrets Add-On"},{"location":"addons/external-secrets/#usage","text":"import * as cdk from 'aws-cdk-lib' ; import * as blueprints from '@aws-quickstart/eks-blueprints' ; const app = new cdk . App (); const addOn = new blueprints . addons . ExternalsSecretsAddOn ({}); const blueprint = blueprints . EksBlueprint . builder () . addOns ( addOn ) . build ( app , 'my-stack-name' );","title":"Usage"},{"location":"addons/external-secrets/#cluster-secret-store","text":"Create a ClusterSecretStore which can be used by all ExternalSecrets from all namespaces import * as eks from 'aws-cdk-lib/aws-eks' ; const cluster = blueprint . getClusterInfo (). cluster ; const clusterSecretStore = new eks . KubernetesManifest ( scope , \"ClusterSecretStore\" , { cluster : cluster , manifest : [ { apiVersion : \"external-secrets.io/v1beta1\" , kind : \"ClusterSecretStore\" , metadata : { name : \"default\" }, spec : { provider : { aws : { service : \"SecretsManager\" , region : region , auth : { jwt : { serviceAccountRef : { name : \"external-secrets-sa\" , namespace : \"external-secrets\" , }, }, }, }, }, }, }, ], });","title":"Cluster Secret Store"},{"location":"addons/external-secrets/#external-secret","text":"Create an ExternalSecret which can be used to fetch, transform and inject secret data import * as eks from 'aws-cdk-lib/aws-eks' ; const cluster = blueprint . getClusterInfo (). cluster ; const keyfiles = new eks . KubernetesManifest ( scope , \"ExternalSecret\" , { cluster : cluster , manifest : [ { apiVersion : \"external-secrets.io/v1beta1\" , kind : \"ExternalSecret\" , metadata : { name : \"the-external-secret-name\" }, spec : { secretStoreRef : { name : \"default\" , kind : \"ClusterSecretStore\" , }, target : { name : \"the-kubernetes-secret-name\" , creationPolicy : \"Merge\" , }, data : [ { secretKey : \"secret-key-to-be-managed\" , remoteRef : { key : \"the-providers-secret-name\" , property : \"the-provider-secret-property\" , }, }, ], }, }, ], });","title":"External Secret"},{"location":"addons/fluxcd/","text":"FluxCD Add-on \u00b6 This add-on installs fluxcd . Flux is a declarative, GitOps-based continuous delivery tool that can be integrated into any CI/CD pipeline. The ability to manage deployments to multiple remote Kubernetes clusters from a central management cluster, support for progressive delivery, and multi-tenancy are some of the notable features of Flux. Usage \u00b6 import * as cdk from 'aws-cdk-lib' ; import * as blueprints from '@aws-quickstart/eks-blueprints' ; const app = new cdk . App (); const addOn = new blueprints . addons . FluxCDAddOn ({ bootstrapRepo : { repoUrl : 'https://github.com/stefanprodan/podinfo' , name : \"podinfo\" , targetRevision : \"master\" , }, }), const blueprint = blueprints . EksBlueprint . builder () . addOns ( addOn ) . build ( app , 'my-stack-name' ); Configuration Options \u00b6 createNamespace : (boolean) If you want CDK to create the namespace for you. values : Arbitrary values to pass to the chart. Refer to the FluxCD Helm Chart documentation for additional details. It also supports all standard helm configuration options ( for Eg: https://github.com/aws-quickstart/cdk-eks-blueprints/blob/main/docs/addons/index.md#standard-helm-add-on-configuration-options). Validation \u00b6 To validate that fluxcd is installed properly in the cluster, check if the namespace is created and fluxcd pods are running. Verify if the namespace is created correctly kubectl get ns | grep \"flux-system\" There should be list the flux-system namespace flux-system Active 31m Verify if the pods are running correctly in flux-system namespace kubectl get pods -n flux-system There should list 3 pods starting with name flux-system For Eg: NAME READY STATUS RESTARTS AGE helm-controller-65cc46469f-v4hnr 1 /1 Running 0 6m13s image-automation-controller-d8f7bfcb4-v4pz8 1 /1 Running 0 6m13s image-reflector-controller-68979dfd49-t4dpj 1 /1 Running 0 6m13s kustomize-controller-767677f7f5-7j26b 1 /1 Running 0 6m13s notification-controller-55d8c759f5-zqd6f 1 /1 Running 0 6m13s source-controller-58c66d55cd-4f6vh 1 /1 Running 0 6m13s Verify if the GitRepository is created fine to bootstrap a git repo: \u276f kubectl get gitrepositories.source.toolkit.fluxcd.io -A NAMESPACE NAME URL AGE READY STATUS flux-system workloadsrepo https://github.com/aws-samples/eks-blueprints-add-ons.git 24m True stored artifact for revision 'eks-blueprints-cdk@sha1:65f1fbbb5165821f6f8bd14eba65a6d2f6cfe0fb' Testing \u00b6 The Flux CLI is available as a binary executable for all major platforms, the binaries can be downloaded form GitHub releases page. Install Fluxcd client curl -s https://fluxcd.io/install.sh | sudo bash . < ( flux completion bash ) Run the below command to check on the GitRepository setup with Flux : kubectl get gitrepository -A NAME URL AGE READY STATUS podinfo https://github.com/stefanprodan/podinfo 5s True stored artifact for revision 'master@sha1:132f4e719209eb10b9485302f8593fc0e680f4fc' Run the below command to create a Kustomization using a source : flux create kustomization podinfo \\ --namespace = flux-system \\ --source = podinfo \\ --path = \"./kustomize\" \\ --prune = true \\ --interval = 5m","title":"FluxCD"},{"location":"addons/fluxcd/#fluxcd-add-on","text":"This add-on installs fluxcd . Flux is a declarative, GitOps-based continuous delivery tool that can be integrated into any CI/CD pipeline. The ability to manage deployments to multiple remote Kubernetes clusters from a central management cluster, support for progressive delivery, and multi-tenancy are some of the notable features of Flux.","title":"FluxCD Add-on"},{"location":"addons/fluxcd/#usage","text":"import * as cdk from 'aws-cdk-lib' ; import * as blueprints from '@aws-quickstart/eks-blueprints' ; const app = new cdk . App (); const addOn = new blueprints . addons . FluxCDAddOn ({ bootstrapRepo : { repoUrl : 'https://github.com/stefanprodan/podinfo' , name : \"podinfo\" , targetRevision : \"master\" , }, }), const blueprint = blueprints . EksBlueprint . builder () . addOns ( addOn ) . build ( app , 'my-stack-name' );","title":"Usage"},{"location":"addons/fluxcd/#configuration-options","text":"createNamespace : (boolean) If you want CDK to create the namespace for you. values : Arbitrary values to pass to the chart. Refer to the FluxCD Helm Chart documentation for additional details. It also supports all standard helm configuration options ( for Eg: https://github.com/aws-quickstart/cdk-eks-blueprints/blob/main/docs/addons/index.md#standard-helm-add-on-configuration-options).","title":"Configuration Options"},{"location":"addons/fluxcd/#validation","text":"To validate that fluxcd is installed properly in the cluster, check if the namespace is created and fluxcd pods are running. Verify if the namespace is created correctly kubectl get ns | grep \"flux-system\" There should be list the flux-system namespace flux-system Active 31m Verify if the pods are running correctly in flux-system namespace kubectl get pods -n flux-system There should list 3 pods starting with name flux-system For Eg: NAME READY STATUS RESTARTS AGE helm-controller-65cc46469f-v4hnr 1 /1 Running 0 6m13s image-automation-controller-d8f7bfcb4-v4pz8 1 /1 Running 0 6m13s image-reflector-controller-68979dfd49-t4dpj 1 /1 Running 0 6m13s kustomize-controller-767677f7f5-7j26b 1 /1 Running 0 6m13s notification-controller-55d8c759f5-zqd6f 1 /1 Running 0 6m13s source-controller-58c66d55cd-4f6vh 1 /1 Running 0 6m13s Verify if the GitRepository is created fine to bootstrap a git repo: \u276f kubectl get gitrepositories.source.toolkit.fluxcd.io -A NAMESPACE NAME URL AGE READY STATUS flux-system workloadsrepo https://github.com/aws-samples/eks-blueprints-add-ons.git 24m True stored artifact for revision 'eks-blueprints-cdk@sha1:65f1fbbb5165821f6f8bd14eba65a6d2f6cfe0fb'","title":"Validation"},{"location":"addons/fluxcd/#testing","text":"The Flux CLI is available as a binary executable for all major platforms, the binaries can be downloaded form GitHub releases page. Install Fluxcd client curl -s https://fluxcd.io/install.sh | sudo bash . < ( flux completion bash ) Run the below command to check on the GitRepository setup with Flux : kubectl get gitrepository -A NAME URL AGE READY STATUS podinfo https://github.com/stefanprodan/podinfo 5s True stored artifact for revision 'master@sha1:132f4e719209eb10b9485302f8593fc0e680f4fc' Run the below command to create a Kustomization using a source : flux create kustomization podinfo \\ --namespace = flux-system \\ --source = podinfo \\ --path = \"./kustomize\" \\ --prune = true \\ --interval = 5m","title":"Testing"},{"location":"addons/grafana-operator/","text":"FluxCD Add-on \u00b6 The grafana-operator is a Kubernetes operator built to help you manage your Grafana instances inside and outside Kubernetes. Grafana Operator makes it possible for you to manage and create Grafana dashboards, datasources etc. declaratively between multiple instances in an easy and scalable way. Using grafana-operator, it will be possible to add AWS data sources such as Amazon Managed Service for Prometheus, Amazon CloudWatch, AWS X-Ray to Amazon Managed Grafana and create Grafana dashboards on Amazon Managed Grafana from your Amazon EKS cluster. This enables customers to use our Kubernetes cluster to create and manage the lifecyle of resources in Amazon Managed Grafana in a Kubernetes native way. This ultimately enables us to use GitOps mechanisms using CNCF projects such as FluxCD to create and manage the lifecyle of resources in Amazon Managed Grafana. Usage \u00b6 import * as cdk from 'aws-cdk-lib' ; import * as blueprints from '@aws-quickstart/eks-blueprints' ; const app = new cdk . App (); const addOn = new blueprints . addons . GrafanaOperatorAddon (), const blueprint = blueprints . EksBlueprint . builder () . addOns ( addOn ) . build ( app , 'my-stack-name' ); Configuration Options \u00b6 createNamespace : (boolean) If you want CDK to create the namespace for you. values : Arbitrary values to pass to the chart. Refer to the Grafana Operator Helm Chart documentation for additional details. It also supports all standard helm configuration options . Validation \u00b6 To validate that Grafana Operator is installed properly in the cluster, check if the namespace is created and pods are running. Verify if the namespace is created correctly kubectl get ns | grep \"grafana-operator\" There should be list the grafana-operator namespace grafana-operator Active 31m Verify if the pods are running correctly in flux-system namespace kubectl get pods -n grafana-operator There should list 3 pods starting with name flux-system For Eg: NAME READY STATUS RESTARTS AGE pod/grafana-operator-779956546b-q5tlf 2 /2 Running 0 3m7s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE service/grafana-operator-metrics-service ClusterIP 172 .20.255.216 <none> 8443 /TCP 3m7s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/grafana-operator 1 /1 1 1 3m7s NAME DESIRED CURRENT READY AGE replicaset.apps/grafana-operator-779956546b 1 1 1 3m7s Testing \u00b6 Please refer to the AWS Blog Using Open Source Grafana Operator on your Kubernetes cluster to manage Amazon Managed Grafana on testing the following features : Setting up Grafana Identity to Amazon Managed Grafana. Adding AWS data sources such as Amazon Managed Service For Prometheus, Amazon CloudWatch, AWS X-Ray. Creating Grafana Dashboards on Amazon Managed Grafana with Grafana Operator.","title":"Grafana Operator"},{"location":"addons/grafana-operator/#fluxcd-add-on","text":"The grafana-operator is a Kubernetes operator built to help you manage your Grafana instances inside and outside Kubernetes. Grafana Operator makes it possible for you to manage and create Grafana dashboards, datasources etc. declaratively between multiple instances in an easy and scalable way. Using grafana-operator, it will be possible to add AWS data sources such as Amazon Managed Service for Prometheus, Amazon CloudWatch, AWS X-Ray to Amazon Managed Grafana and create Grafana dashboards on Amazon Managed Grafana from your Amazon EKS cluster. This enables customers to use our Kubernetes cluster to create and manage the lifecyle of resources in Amazon Managed Grafana in a Kubernetes native way. This ultimately enables us to use GitOps mechanisms using CNCF projects such as FluxCD to create and manage the lifecyle of resources in Amazon Managed Grafana.","title":"FluxCD Add-on"},{"location":"addons/grafana-operator/#usage","text":"import * as cdk from 'aws-cdk-lib' ; import * as blueprints from '@aws-quickstart/eks-blueprints' ; const app = new cdk . App (); const addOn = new blueprints . addons . GrafanaOperatorAddon (), const blueprint = blueprints . EksBlueprint . builder () . addOns ( addOn ) . build ( app , 'my-stack-name' );","title":"Usage"},{"location":"addons/grafana-operator/#configuration-options","text":"createNamespace : (boolean) If you want CDK to create the namespace for you. values : Arbitrary values to pass to the chart. Refer to the Grafana Operator Helm Chart documentation for additional details. It also supports all standard helm configuration options .","title":"Configuration Options"},{"location":"addons/grafana-operator/#validation","text":"To validate that Grafana Operator is installed properly in the cluster, check if the namespace is created and pods are running. Verify if the namespace is created correctly kubectl get ns | grep \"grafana-operator\" There should be list the grafana-operator namespace grafana-operator Active 31m Verify if the pods are running correctly in flux-system namespace kubectl get pods -n grafana-operator There should list 3 pods starting with name flux-system For Eg: NAME READY STATUS RESTARTS AGE pod/grafana-operator-779956546b-q5tlf 2 /2 Running 0 3m7s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE service/grafana-operator-metrics-service ClusterIP 172 .20.255.216 <none> 8443 /TCP 3m7s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/grafana-operator 1 /1 1 1 3m7s NAME DESIRED CURRENT READY AGE replicaset.apps/grafana-operator-779956546b 1 1 1 3m7s","title":"Validation"},{"location":"addons/grafana-operator/#testing","text":"Please refer to the AWS Blog Using Open Source Grafana Operator on your Kubernetes cluster to manage Amazon Managed Grafana on testing the following features : Setting up Grafana Identity to Amazon Managed Grafana. Adding AWS data sources such as Amazon Managed Service For Prometheus, Amazon CloudWatch, AWS X-Ray. Creating Grafana Dashboards on Amazon Managed Grafana with Grafana Operator.","title":"Testing"},{"location":"addons/istio-base/","text":"Istio Base Add-on \u00b6 The Base add-on adds support for Istio base chart which contains cluster-wide resources and CRDs used by the Istio control plane to an EKS cluster. Usage \u00b6 Add the following as an add-on to your main.ts file to add Istio Base to your cluster import 'source-map-support/register' ; import * as cdk from 'aws-cdk-lib' ; import * as blueprints from '@aws-quickstart/eks-blueprints' ; const app = new cdk . App (); const addOn = new blueprints . addons . IstioBaseAddOn (); const blueprint = blueprints . EksBlueprint . builder () . addOns ( addOn ) . build ( app , 'my-stack-name' ); To validate that installation is successful run the following command: $ kubectl get crd -n istio-system NAME CREATED AT authorizationpolicies.security.istio.io 2022 -05-05T02:16:23Z destinationrules.networking.istio.io 2022 -05-05T02:16:23Z eniconfigs.crd.k8s.amazonaws.com 2022 -05-05T02:04:10Z envoyfilters.networking.istio.io 2022 -05-05T02:16:23Z gateways.networking.istio.io 2022 -05-05T02:16:23Z istiooperators.install.istio.io 2022 -05-05T02:16:23Z peerauthentications.security.istio.io 2022 -05-05T02:16:23Z proxyconfigs.networking.istio.io 2022 -05-05T02:16:23Z requestauthentications.security.istio.io 2022 -05-05T02:16:23Z securitygrouppolicies.vpcresources.k8s.aws 2022 -05-05T02:04:12Z serviceentries.networking.istio.io 2022 -05-05T02:16:23Z sidecars.networking.istio.io 2022 -05-05T02:16:23Z telemetries.telemetry.istio.io 2022 -05-05T02:16:23Z virtualservices.networking.istio.io 2022 -05-05T02:16:23Z wasmplugins.extensions.istio.io 2022 -05-05T02:16:23Z workloadentries.networking.istio.io 2022 -05-05T02:16:23Z workloadgroups.networking.istio.io 2022 -05-05T02:16:23Z Note that the all the CRDs are created in provided namespace (istio-system by default). Once deployed, it allows the control plane to be installed. Configuration \u00b6 enableAnalysis : Enable istioctl analysis which provides rich analysis of Istio configuration state in order to identity invalid or suboptimal configurations. Defaults to false if not specified. configValidation : Enable the istio base config validation. Defaults to true if not specified. externalIstiod : If this is set to true, one Istiod will control remote clusters including CA. Defaults to false if not specified. remotePilotAddress : The address or hostname of the remote pilot. validationURL : Validation webhook configuration url. For example: https://$remotePilotAddress:15017/validate . enableIstioConfigCRDs : For istioctl usage to disable istio config crds in base. Defaults to true if not specified. Functionality \u00b6 Installs cluster-wide resources and CRDs used by the Istio control plane Provides convenience options for istioctl commands Works as the basis for the Istio Control Plane AddOn","title":"Istio Base"},{"location":"addons/istio-base/#istio-base-add-on","text":"The Base add-on adds support for Istio base chart which contains cluster-wide resources and CRDs used by the Istio control plane to an EKS cluster.","title":"Istio Base Add-on"},{"location":"addons/istio-base/#usage","text":"Add the following as an add-on to your main.ts file to add Istio Base to your cluster import 'source-map-support/register' ; import * as cdk from 'aws-cdk-lib' ; import * as blueprints from '@aws-quickstart/eks-blueprints' ; const app = new cdk . App (); const addOn = new blueprints . addons . IstioBaseAddOn (); const blueprint = blueprints . EksBlueprint . builder () . addOns ( addOn ) . build ( app , 'my-stack-name' ); To validate that installation is successful run the following command: $ kubectl get crd -n istio-system NAME CREATED AT authorizationpolicies.security.istio.io 2022 -05-05T02:16:23Z destinationrules.networking.istio.io 2022 -05-05T02:16:23Z eniconfigs.crd.k8s.amazonaws.com 2022 -05-05T02:04:10Z envoyfilters.networking.istio.io 2022 -05-05T02:16:23Z gateways.networking.istio.io 2022 -05-05T02:16:23Z istiooperators.install.istio.io 2022 -05-05T02:16:23Z peerauthentications.security.istio.io 2022 -05-05T02:16:23Z proxyconfigs.networking.istio.io 2022 -05-05T02:16:23Z requestauthentications.security.istio.io 2022 -05-05T02:16:23Z securitygrouppolicies.vpcresources.k8s.aws 2022 -05-05T02:04:12Z serviceentries.networking.istio.io 2022 -05-05T02:16:23Z sidecars.networking.istio.io 2022 -05-05T02:16:23Z telemetries.telemetry.istio.io 2022 -05-05T02:16:23Z virtualservices.networking.istio.io 2022 -05-05T02:16:23Z wasmplugins.extensions.istio.io 2022 -05-05T02:16:23Z workloadentries.networking.istio.io 2022 -05-05T02:16:23Z workloadgroups.networking.istio.io 2022 -05-05T02:16:23Z Note that the all the CRDs are created in provided namespace (istio-system by default). Once deployed, it allows the control plane to be installed.","title":"Usage"},{"location":"addons/istio-base/#configuration","text":"enableAnalysis : Enable istioctl analysis which provides rich analysis of Istio configuration state in order to identity invalid or suboptimal configurations. Defaults to false if not specified. configValidation : Enable the istio base config validation. Defaults to true if not specified. externalIstiod : If this is set to true, one Istiod will control remote clusters including CA. Defaults to false if not specified. remotePilotAddress : The address or hostname of the remote pilot. validationURL : Validation webhook configuration url. For example: https://$remotePilotAddress:15017/validate . enableIstioConfigCRDs : For istioctl usage to disable istio config crds in base. Defaults to true if not specified.","title":"Configuration"},{"location":"addons/istio-base/#functionality","text":"Installs cluster-wide resources and CRDs used by the Istio control plane Provides convenience options for istioctl commands Works as the basis for the Istio Control Plane AddOn","title":"Functionality"},{"location":"addons/istio-control-plane/","text":"Istio Control Plane Add-on \u00b6 Istio is an open platform for providing a uniform way to integrate microservices, manage traffic flow across microservices, enforce policies and aggregate telemetry data. Istio's control plane provides an abstraction layer over the underlying cluster management platform, such as Kubernetes. IMPORTANT : This add-on depends on Istio Base Add-on for cluster-wide resources and CRDs. Istio Base add-on must be present in add-on array and must be in add-on array before the Istio Control Plane add-on for it to work, as shown in below example. Otherwise will run into error Assertion failed: Missing a dependency for IstioBaseAddOn . Usage \u00b6 Add the following as an add-on to your main.ts file to add Istio Control Plane to your cluster import 'source-map-support/register' ; import * as cdk from 'aws-cdk-lib' ; import * as blueprints from '@aws-quickstart/eks-blueprints' ; const app = new cdk . App (); const istioBase = new blueprints . addons . IstioBaseAddOn (); const istioControlPlane = new blueprints . addons . IstioControlPlaneAddOn () const addOns : Array < blueprints . ClusterAddOn > = [ istioBase , istioControlPlane ]; const blueprint = blueprints . EksBlueprint . builder () . addOns (... addOns ) . build ( app , 'my-stack-name' ); To validate that installation is successful run the following command: $ kubectl get po -n istio-system NAME READY STATUS RESTARTS AGE istiod-5797797b4b-fjrq2 1 /1 Running 0 28m Configuration \u00b6 values : Arbitrary values to pass to the chart as per https://istio.io/v1.4/docs/reference/config/installation-options/ import 'source-map-support/register' ; import * as cdk from 'aws-cdk-lib' ; import * as blueprints from '@edgarsilva948/eks-blueprints' ; const app = new cdk . App (); const istioControlPlaneAddOnProps = { values : { pilot : { autoscaleEnabled : true , autoscaleMin : 1 , autoscaleMax : 5 , replicaCount : 1 , rollingMaxSurge : \"100%\" , rollingMaxUnavailable : \"25%\" , resources : { requests : { cpu : \"500m\" , memory : \"2048Mi\" , } } } } } const istioBase = new blueprints . addons . IstioBaseAddOn (); const istioControlPlane = new blueprints . addons . IstioControlPlaneAddOn ( IstioControlPlaneAddOnProps ) const addOns : Array < blueprints . ClusterAddOn > = [ istioBase , istioControlPlane ]; const blueprint = blueprints . EksBlueprint . builder () . addOns (... addOns ) . build ( app , 'my-stack-name' ); Functionality \u00b6 Installs Istio Control Plane deployment","title":"Istio Control Plane"},{"location":"addons/istio-control-plane/#istio-control-plane-add-on","text":"Istio is an open platform for providing a uniform way to integrate microservices, manage traffic flow across microservices, enforce policies and aggregate telemetry data. Istio's control plane provides an abstraction layer over the underlying cluster management platform, such as Kubernetes. IMPORTANT : This add-on depends on Istio Base Add-on for cluster-wide resources and CRDs. Istio Base add-on must be present in add-on array and must be in add-on array before the Istio Control Plane add-on for it to work, as shown in below example. Otherwise will run into error Assertion failed: Missing a dependency for IstioBaseAddOn .","title":"Istio Control Plane Add-on"},{"location":"addons/istio-control-plane/#usage","text":"Add the following as an add-on to your main.ts file to add Istio Control Plane to your cluster import 'source-map-support/register' ; import * as cdk from 'aws-cdk-lib' ; import * as blueprints from '@aws-quickstart/eks-blueprints' ; const app = new cdk . App (); const istioBase = new blueprints . addons . IstioBaseAddOn (); const istioControlPlane = new blueprints . addons . IstioControlPlaneAddOn () const addOns : Array < blueprints . ClusterAddOn > = [ istioBase , istioControlPlane ]; const blueprint = blueprints . EksBlueprint . builder () . addOns (... addOns ) . build ( app , 'my-stack-name' ); To validate that installation is successful run the following command: $ kubectl get po -n istio-system NAME READY STATUS RESTARTS AGE istiod-5797797b4b-fjrq2 1 /1 Running 0 28m","title":"Usage"},{"location":"addons/istio-control-plane/#configuration","text":"values : Arbitrary values to pass to the chart as per https://istio.io/v1.4/docs/reference/config/installation-options/ import 'source-map-support/register' ; import * as cdk from 'aws-cdk-lib' ; import * as blueprints from '@edgarsilva948/eks-blueprints' ; const app = new cdk . App (); const istioControlPlaneAddOnProps = { values : { pilot : { autoscaleEnabled : true , autoscaleMin : 1 , autoscaleMax : 5 , replicaCount : 1 , rollingMaxSurge : \"100%\" , rollingMaxUnavailable : \"25%\" , resources : { requests : { cpu : \"500m\" , memory : \"2048Mi\" , } } } } } const istioBase = new blueprints . addons . IstioBaseAddOn (); const istioControlPlane = new blueprints . addons . IstioControlPlaneAddOn ( IstioControlPlaneAddOnProps ) const addOns : Array < blueprints . ClusterAddOn > = [ istioBase , istioControlPlane ]; const blueprint = blueprints . EksBlueprint . builder () . addOns (... addOns ) . build ( app , 'my-stack-name' );","title":"Configuration"},{"location":"addons/istio-control-plane/#functionality","text":"Installs Istio Control Plane deployment","title":"Functionality"},{"location":"addons/jupyterhub/","text":"JupyterHub Add-on \u00b6 JupyterHub add-on is based on the JupyterHub project that supports a multi-user Hub to spawn, manage, and proxy multiple instances of single user Jupyter notebook server. The Hub can offer notebook servers to a class of students, a corporate data science workgroup, a scientific research project, or a high-performance computing group. For more information regarding a Jupyter notebook, please consult the official documentation . IMPORTANT : This add-on depends on EBS CSI Driver Add-on for using EBS as persistent storage. EBS CSI Driver or EFS CSI Driver add-on must be present in add-on array and must be in add-on array before the Jupyter add-on for it to work, as shown in below example (with EBS). Otherwise will run into error Assertion failed: Missing a dependency for <EbsCsiDriverAddOn or EfsCsiDriverAddOn> . Usage \u00b6 import 'source-map-support/register' ; import * as cdk from 'aws-cdk-lib' ; import * as blueprints from '@aws-quickstart/eks-blueprints' ; const app = new cdk . App (); const subdomain : string = utils . valueFromContext ( scope , \"dev.subzone.name\" , \"jupyterhub.some.example.com\" ); const parentDnsAccountId = scope . node . tryGetContext ( \"parent.dns.account\" ) ! ; const parentDomain = utils . valueFromContext ( scope , \"parent.hostedzone.name\" , \"some.example.com\" ); const jupyterHubAddOn = new blueprints . addons . JupyterHubAddOn ({ efsConfig : { removalPolicy : cdk.RemovalPolicy.DESTROY , pvcName : \"efs-persist\" , capacity : \"120Gi\" , }, oidcConfig ?: { callbackUrl : \"<Callback URL>\" , authUrl : \"<Authorization URL>\" , tokenUrl : \"<Token URL>\" , userDataUrl : \"<User Data URL>\" , clientId : \"<Client ID>\" , clientSecret : \"<Client Secret>\" , scope : [], //list of OIDC provider scopes usernameKey : \"<username key>\" , }, serviceType : blueprints.JupyterHubServiceType.ALB , ingressHosts : [ jupyterDNSname ], ingressAnnotations : { 'external-dns.alpha.kubernetes.io/hostname' : ` ${ jupyterDNSname } ` , }, notebookStack : 'jupyter/datascience-notebook' , certificateResourceName ?: 'your-certificate' , }); const awsAlbAddOn = new blueprints . addons . AwsLoadBalancerControllerAddOn (), const efsCsiAddOn = new blueprints . addons . EfsCsiDriverAddOn (); const externalDnsAddOn = new blueprints . addons . ExternalDnsAddOn ({ hostedZoneResources : [ GlobalResources . HostedZone ] }), const addOns : Array < blueprints . ClusterAddOn > = [ awsAlbAddOn , externalDnsAddOn , efsCsiAddOn , jupyterHubAddOn ]; const blueprint = blueprints . EksBlueprint . builder () . resourceProvider ( GlobalResources . HostedZone , new DelegatingHostedZoneProvider ({ parentDomain , subdomain , parentDnsAccountId , delegatingRoleName : 'DomainOperatorRole' , wildcardSubdomain : true })) . addOns (... addOns ) . build ( app , 'my-stack-name' ); To validate that the JupyterHub add-on is running ensure that the add-on deployments for the controller and the webhook are in RUNNING state: # Assuming add-on is installed in the jupyterhub. $ kubectl get po -n jupyterhub NAME READY STATUS RESTARTS AGE continuous-image-puller-2skff 1 /1 Running 0 23m continuous-image-puller-m6s5f 1 /1 Running 0 23m hub-7dc8888f68-67hnl 1 /1 Running 0 23m jupyter-admin 1 /1 Running 0 16m proxy-5df778944c-brrbf 1 /1 Running 0 23m user-scheduler-7dbd789bc4-8zrjs 1 /1 Running 0 23m user-scheduler-7dbd789bc4-gcb8z 1 /1 Running 0 23m Functionality \u00b6 Deploys the jupyterhub helm chart in jupyterhub namespace by default. JupyterHub is backed with persistent storage. You must provide one (and only one) of the following configuration (or otherwise will receive an error): Leverage EBS as persistent storage with storage type and capacity provided. If you provide this configuration, EBS CSI Driver add-on must be present in add-on array and must be in add-on array before the JupyterHub add-on for it to work, as shown in above example. Otherwise it will not work. Leverage EFS as persistent storage with the name, capacity and file system removal policy provided. If you provide this configuration, EFS CSI Driver add-on must be present in add-on array and must be in add-on array before the JupyterHub add-on for it to work, as shown in above example. Otherwise it will not work. (Optional) Leverage OIDC Provider as a way to manage authentication and authorization. If not provided, the default creates no user, and the user will be able to login with any arbitrary username and password. It is highly recommended to leverage an Identity provider for any production use case. Exposes the proxy service in three different way based on configuration: Expose using Ingress controller and AWS Application Load Balancer. This requires AWS Load Balancer Controller add-on and it must be in add-on array before the JupyterHub add-on . This will also look for any additional Ingress annotations provided by the user to be tagged. Expose using Loadbalancer Service and AWS Network Load Balancer. This requires AWS Load Balancer Controller add-on and it must be in add-on array before the JupyterHub add-on . Expose using ClusterIP Service. (Optional) Annotates Ingress with user-provided AWS Certificate Manager certificate name. It will be looked up and automatically tagged to be used with Ingress. It will require user to provide a DNS name and External DNS add-on to be added in add-on array before the JupyterHub add-on . (Optional) User can choose a different notebook stack than the standard one provided. Jupyter team maintains a set of Docker image definition in a GitHub repository as explained here . Supports standard helm configuration options . Note : For custom helm values, please consult the official documentation . Using JupyterHub \u00b6 JupyterHub, by default, creates a proxy service called proxy-public that will be accessible in different way based on the user configuration setting under serviceType . For example, if you set it as NLB , then it is exposed to a LoadBalancer type Kubernetes service, which will integrate with AWS Network Load Balancer as indicated when running the following command: kubectl get svc -n jupyterhub NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE hub ClusterIP 172 .20.171.28 <none> 8081 /TCP 26m proxy-api ClusterIP 172 .20.31.32 <none> 8001 /TCP 26m proxy-public LoadBalancer 172 .20.14.210 xxxxxxxx-1234567890.us-west-2.elb.amazonaws.com 80 :32733/TCP 26m You can log into the JupyterHub portal by accessing the Load Balancer endpoint in any browser. A default arbirary username with password can be entered to log in. Once logged in, you should be able to access the main portal page. As stated above, it is highly recommended to leverage an Identity provider for any production use case. Please consult the official guide here for various OAuth2 based authentication methods.","title":"JupyterHub"},{"location":"addons/jupyterhub/#jupyterhub-add-on","text":"JupyterHub add-on is based on the JupyterHub project that supports a multi-user Hub to spawn, manage, and proxy multiple instances of single user Jupyter notebook server. The Hub can offer notebook servers to a class of students, a corporate data science workgroup, a scientific research project, or a high-performance computing group. For more information regarding a Jupyter notebook, please consult the official documentation . IMPORTANT : This add-on depends on EBS CSI Driver Add-on for using EBS as persistent storage. EBS CSI Driver or EFS CSI Driver add-on must be present in add-on array and must be in add-on array before the Jupyter add-on for it to work, as shown in below example (with EBS). Otherwise will run into error Assertion failed: Missing a dependency for <EbsCsiDriverAddOn or EfsCsiDriverAddOn> .","title":"JupyterHub Add-on"},{"location":"addons/jupyterhub/#usage","text":"import 'source-map-support/register' ; import * as cdk from 'aws-cdk-lib' ; import * as blueprints from '@aws-quickstart/eks-blueprints' ; const app = new cdk . App (); const subdomain : string = utils . valueFromContext ( scope , \"dev.subzone.name\" , \"jupyterhub.some.example.com\" ); const parentDnsAccountId = scope . node . tryGetContext ( \"parent.dns.account\" ) ! ; const parentDomain = utils . valueFromContext ( scope , \"parent.hostedzone.name\" , \"some.example.com\" ); const jupyterHubAddOn = new blueprints . addons . JupyterHubAddOn ({ efsConfig : { removalPolicy : cdk.RemovalPolicy.DESTROY , pvcName : \"efs-persist\" , capacity : \"120Gi\" , }, oidcConfig ?: { callbackUrl : \"<Callback URL>\" , authUrl : \"<Authorization URL>\" , tokenUrl : \"<Token URL>\" , userDataUrl : \"<User Data URL>\" , clientId : \"<Client ID>\" , clientSecret : \"<Client Secret>\" , scope : [], //list of OIDC provider scopes usernameKey : \"<username key>\" , }, serviceType : blueprints.JupyterHubServiceType.ALB , ingressHosts : [ jupyterDNSname ], ingressAnnotations : { 'external-dns.alpha.kubernetes.io/hostname' : ` ${ jupyterDNSname } ` , }, notebookStack : 'jupyter/datascience-notebook' , certificateResourceName ?: 'your-certificate' , }); const awsAlbAddOn = new blueprints . addons . AwsLoadBalancerControllerAddOn (), const efsCsiAddOn = new blueprints . addons . EfsCsiDriverAddOn (); const externalDnsAddOn = new blueprints . addons . ExternalDnsAddOn ({ hostedZoneResources : [ GlobalResources . HostedZone ] }), const addOns : Array < blueprints . ClusterAddOn > = [ awsAlbAddOn , externalDnsAddOn , efsCsiAddOn , jupyterHubAddOn ]; const blueprint = blueprints . EksBlueprint . builder () . resourceProvider ( GlobalResources . HostedZone , new DelegatingHostedZoneProvider ({ parentDomain , subdomain , parentDnsAccountId , delegatingRoleName : 'DomainOperatorRole' , wildcardSubdomain : true })) . addOns (... addOns ) . build ( app , 'my-stack-name' ); To validate that the JupyterHub add-on is running ensure that the add-on deployments for the controller and the webhook are in RUNNING state: # Assuming add-on is installed in the jupyterhub. $ kubectl get po -n jupyterhub NAME READY STATUS RESTARTS AGE continuous-image-puller-2skff 1 /1 Running 0 23m continuous-image-puller-m6s5f 1 /1 Running 0 23m hub-7dc8888f68-67hnl 1 /1 Running 0 23m jupyter-admin 1 /1 Running 0 16m proxy-5df778944c-brrbf 1 /1 Running 0 23m user-scheduler-7dbd789bc4-8zrjs 1 /1 Running 0 23m user-scheduler-7dbd789bc4-gcb8z 1 /1 Running 0 23m","title":"Usage"},{"location":"addons/jupyterhub/#functionality","text":"Deploys the jupyterhub helm chart in jupyterhub namespace by default. JupyterHub is backed with persistent storage. You must provide one (and only one) of the following configuration (or otherwise will receive an error): Leverage EBS as persistent storage with storage type and capacity provided. If you provide this configuration, EBS CSI Driver add-on must be present in add-on array and must be in add-on array before the JupyterHub add-on for it to work, as shown in above example. Otherwise it will not work. Leverage EFS as persistent storage with the name, capacity and file system removal policy provided. If you provide this configuration, EFS CSI Driver add-on must be present in add-on array and must be in add-on array before the JupyterHub add-on for it to work, as shown in above example. Otherwise it will not work. (Optional) Leverage OIDC Provider as a way to manage authentication and authorization. If not provided, the default creates no user, and the user will be able to login with any arbitrary username and password. It is highly recommended to leverage an Identity provider for any production use case. Exposes the proxy service in three different way based on configuration: Expose using Ingress controller and AWS Application Load Balancer. This requires AWS Load Balancer Controller add-on and it must be in add-on array before the JupyterHub add-on . This will also look for any additional Ingress annotations provided by the user to be tagged. Expose using Loadbalancer Service and AWS Network Load Balancer. This requires AWS Load Balancer Controller add-on and it must be in add-on array before the JupyterHub add-on . Expose using ClusterIP Service. (Optional) Annotates Ingress with user-provided AWS Certificate Manager certificate name. It will be looked up and automatically tagged to be used with Ingress. It will require user to provide a DNS name and External DNS add-on to be added in add-on array before the JupyterHub add-on . (Optional) User can choose a different notebook stack than the standard one provided. Jupyter team maintains a set of Docker image definition in a GitHub repository as explained here . Supports standard helm configuration options . Note : For custom helm values, please consult the official documentation .","title":"Functionality"},{"location":"addons/jupyterhub/#using-jupyterhub","text":"JupyterHub, by default, creates a proxy service called proxy-public that will be accessible in different way based on the user configuration setting under serviceType . For example, if you set it as NLB , then it is exposed to a LoadBalancer type Kubernetes service, which will integrate with AWS Network Load Balancer as indicated when running the following command: kubectl get svc -n jupyterhub NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE hub ClusterIP 172 .20.171.28 <none> 8081 /TCP 26m proxy-api ClusterIP 172 .20.31.32 <none> 8001 /TCP 26m proxy-public LoadBalancer 172 .20.14.210 xxxxxxxx-1234567890.us-west-2.elb.amazonaws.com 80 :32733/TCP 26m You can log into the JupyterHub portal by accessing the Load Balancer endpoint in any browser. A default arbirary username with password can be entered to log in. Once logged in, you should be able to access the main portal page. As stated above, it is highly recommended to leverage an Identity provider for any production use case. Please consult the official guide here for various OAuth2 based authentication methods.","title":"Using JupyterHub"},{"location":"addons/karpenter/","text":"Karpenter Add-on \u00b6 Karpenter add-on is based on the Karpenter open source node provisioning project. It provides a more efficient and cost-effective way to manage workloads by launching just the right compute resources to handle a cluster's application. Karpenter works by: Watching for pods that the Kubernetes scheduler has marked as unschedulable, Evaluating scheduling constraints (resource requests, nodeselectors, affinities, tolerations, and topology spread constraints) requested by the pods, Provisioning nodes that meet the requirements of the pods, Scheduling the pods to run on the new nodes, and Removing the nodes when the nodes are no longer needed Prerequisites \u00b6 There is no support for utilizing both Cluster Autoscaler and Karpenter. Therefore, any addons list that has both will result in an error Deploying <name of your stack> failed due to conflicting add-on: ClusterAutoscalerAddOn. . (If using Spot), EC2 Spot Service Linked Role should be created. See here for more details. Usage \u00b6 import 'source-map-support/register' ; import * as cdk from 'aws-cdk-lib' ; import * as blueprints from '@aws-quickstart/eks-blueprints' ; const app = new cdk . App (); const karpenterAddonProps = { requirements : [ { key : 'node.kubernetes.io/instance-type' , op : 'In' , vals : [ 'm5.2xlarge' ] }, { key : 'topology.kubernetes.io/zone' , op : 'NotIn' , vals : [ 'us-west-2c' ]}, { key : 'kubernetes.io/arch' , op : 'In' , vals : [ 'amd64' , 'arm64' ]}, { key : 'karpenter.sh/capacity-type' , op : 'In' , vals : [ 'spot' , 'on-demand' ]}, ], subnetTags : { \"Name\" : \"blueprint-construct-dev/blueprint-construct-dev-vpc/PrivateSubnet1\" , }, securityGroupTags : { \"kubernetes.io/cluster/blueprint-construct-dev\" : \"owned\" , }, taints : [{ key : \"workload\" , value : \"test\" , effect : \"NoSchedule\" , }], amiFamily : \"AL2\" , consolidation : { enabled : true }, ttlSecondsUntilExpired : 2592000 , weight : 20 , interruptionHandling : true , } const karpenterAddOn = new blueprints . addons . KarpenterAddOn ( karpenterAddonProps ); const blueprint = blueprints . EksBlueprint . builder () . addOns ( karpenterAddOn ) . build ( app , 'my-stack-name' ); The add-on automatically sets the following Helm Chart values , and it is highly recommended not to pass these values in (as it will result in errors): - settings.aws.defaultInstanceProfile - settings.aws.clusterEndpoint - settings.aws.clusterName - settings.aws.interruptionQueueName (if interruption handling is enabled) - serviceAccount.create - serviceAccount.name - serviceAccount.annotations.eks.amazonaws.com/role-arn To validate that Karpenter add-on is running ensure that the add-on deployments for the controller and the webhook are in RUNNING state: # Assuming add-on is installed in the karpenter namespace. $ kubectl get po -n karpenter NAME READY STATUS RESTARTS AGE blueprints-addon-karpenter-54fd978b89-hclmp 2 /2 Running 0 99m Functionality \u00b6 Creates Karpenter Node Role, Karpenter Instance Profile, and Karpenter Controller Policy (Please see Karpenter documentation here for more details on what is required and why). Creates karpenter namespace. Creates Kubernetes Service Account, and associate AWS IAM Role with Karpenter Controller Policy attached using IRSA . Deploys Karpenter helm chart in the karpenter namespace, configuring cluster name and cluster endpoint on the controller by default. (Optionally) provisions a default Karpenter Provisioner CRD based on user-provided spec.requirements , AMI type , taints and tags. If created, the provisioner will discover the EKS VPC subnets and security groups to launch the nodes with. NOTE: 1. The default provisioner is created only if both the subnet tags and the security group tags are provided. 2. Provisioner spec requirement fields are not necessary, as karpenter will dynamically choose (i.e. leaving instance-type blank will let karpenter choose approrpriate sizing). 3. Consolidation, which is a flag that enables , is supported on versions 0.15.0 and later. It is also mutually exclusive with ttlSecondsAfterempty , so if you provide both properties, the addon will throw an error. 4. Weight, which is a property to prioritize provisioners based on weight, is supported on versions 0.16.0 and later. Addon will throw an error if weight is provided for earlier versions. 5. Interruption Handling, which is a native way to handle interruption due to involuntary interruption events, is supported on versions 0.19.0 and later. For interruption handling in the earlier versions, Karpenter supports using AWS Node Interruption Handler (which you will need to add as an add-on and must be in add-on array after the Karpenter add-on for it to work. Using Karpenter \u00b6 To use Karpenter, you need to provision a Karpenter provisioner CRD . A single provisioner is capable of handling many different pod shapes. This can be done in 2 ways: Provide the properties as show in Usage . If subnet tags and security group tags are not provided at deploy time, the add-on will be installed without a Provisioner. Use kubectl to apply a sample provisioner manifest: cat <<EOF | kubectl apply -f - apiVersion: karpenter.sh/v1alpha5 kind: Provisioner metadata: name: default spec: requirements: - key: \"node.kubernetes.io/instance-type\" operator: In values: [\"m5.2xlarge\"] - key: \"topology.kubernetes.io/zone\" operator: In values: [\"us-east-1c\"] - key: \"kubernetes.io/arch\" operator: In values: [\"arm64\", \"amd64\"] - key: \"karpenter.sh/capacity-type\" operator: In values: [\"spot\", \"on-demand\"] provider: instanceProfile: <<Name of your Instance Profile>> subnetSelector: Name: blueprint-construct-dev/blueprint-construct-dev-vpc/PrivateSubnet1 securityGroupSelector: \"kubernetes.io/cluster/blueprint-construct-dev\": \"owned\" ttlSecondsAfterEmpty: 30 EOF If you choose to create a provisioner manually, you MUST provide the tags that match the subnet and the security group that you want to use. Testing with a sample deployment \u00b6 Now that the provisioner is deployed, Karpenter is active and ready to provision nodes. Create some pods using a deployment: cat <<EOF | kubectl apply -f - apiVersion: apps/v1 kind: Deployment metadata: name: inflate spec: replicas: 0 selector: matchLabels: app: inflate template: metadata: labels: app: inflate spec: terminationGracePeriodSeconds: 0 containers: - name: inflate image: public.ecr.aws/eks-distro/kubernetes/pause:3.2 resources: requests: cpu: 1 EOF Now scale the deployment: kubectl scale deployment inflate --replicas 10 The provisioner will then start deploying more nodes to deploy the scaled replicas. You can verify by either looking at the karpenter controller logs, kubectl logs -f -n karpenter $( kubectl get pods -n karpenter -l karpenter = controller -o name ) or, by looking at the nodes being created: kubectl get nodes Troubleshooting \u00b6 The following are common troubleshooting issues observed when implementing Karpenter: For Karpenter version older than 0.14.0 deployed on Fargate Profiles, values.yaml must be overridden, setting dnsPolicy to Default . Versions after 0.14.0 has dnsPolicy value set default to Default . This is to ensure CoreDNS is set correctly on Fargate nodes. With the upgrade to the new OCI registry starting with v0.17.0 , if you try to upgrade you may get a following error: Received response status [FAILED] from custom resource. Message returned: Error: b'Error: path \"/tmp/tmpkxgr57q5/blueprints-addon-karpenter\" not found\\n' Karpenter, starting from the OCI registry versions, will untar the files under karpenter release name only. So if you have previous version deployed under a different release name, you will run into the above error. Therefore, in order to upgrade, you will have to take the following steps: Remove the existing add-on. Re-deploy the Karpenter add-on with the release name karpenter . Upgrade Path \u00b6 Using an older version of the Karptner add-on, you may notice the difference in the \"provisionerSpecs\" property: provisionerSpecs: { 'node.kubernetes.io/instance-type': ['m5.2xlarge'], 'topology.kubernetes.io/zone': ['us-east-1c'], 'kubernetes.io/arch': ['amd64','arm64'], 'karpenter.sh/capacity-type': ['spot','on-demand'], }, This now changes to the \"requirement\" property: requirements: [ { key: 'node.kubernetes.io/instance-type', op: 'In', vals: ['m5.2xlarge'] }, { key: 'topology.kubernetes.io/zone', op: 'NotIn', vals: ['us-west-2c']}, { key: 'kubernetes.io/arch', op: 'In', vals: ['amd64','arm64']}, { key: 'karpenter.sh/capacity-type', op: 'In', vals: ['spot','on-demand']}, ], The property is changed to align with the naming convention of the provisioner, and to allow multiple operators (In vs NotIn). The values correspond similarly between the two, with type change being the only difference. Certain upgrades require reapplying the CRDs since Helm does not maintain the lifecycle of CRDs. Please see the official documentations for details. Starting with v0.17.0, Karpenter's Helm chart package is stored in OCI (Open Container Initiative) registry. With this change, charts.karpenter.sh is no longer updated to preserve older versions. You have to adjust for the following: The full URL needs to be present (including 'oci://'). You need to append a v to the version number (i.e. v0.17.0, not 0.17.0) Starting with v0.22.0, Karpenter will no longer work on Kubernetes version prior to 1.21. Either upgrade your Kubernetes to 1.21 or later version and apply Karpenter, or use prior Karpenter versions.","title":"Karpenter"},{"location":"addons/karpenter/#karpenter-add-on","text":"Karpenter add-on is based on the Karpenter open source node provisioning project. It provides a more efficient and cost-effective way to manage workloads by launching just the right compute resources to handle a cluster's application. Karpenter works by: Watching for pods that the Kubernetes scheduler has marked as unschedulable, Evaluating scheduling constraints (resource requests, nodeselectors, affinities, tolerations, and topology spread constraints) requested by the pods, Provisioning nodes that meet the requirements of the pods, Scheduling the pods to run on the new nodes, and Removing the nodes when the nodes are no longer needed","title":"Karpenter Add-on"},{"location":"addons/karpenter/#prerequisites","text":"There is no support for utilizing both Cluster Autoscaler and Karpenter. Therefore, any addons list that has both will result in an error Deploying <name of your stack> failed due to conflicting add-on: ClusterAutoscalerAddOn. . (If using Spot), EC2 Spot Service Linked Role should be created. See here for more details.","title":"Prerequisites"},{"location":"addons/karpenter/#usage","text":"import 'source-map-support/register' ; import * as cdk from 'aws-cdk-lib' ; import * as blueprints from '@aws-quickstart/eks-blueprints' ; const app = new cdk . App (); const karpenterAddonProps = { requirements : [ { key : 'node.kubernetes.io/instance-type' , op : 'In' , vals : [ 'm5.2xlarge' ] }, { key : 'topology.kubernetes.io/zone' , op : 'NotIn' , vals : [ 'us-west-2c' ]}, { key : 'kubernetes.io/arch' , op : 'In' , vals : [ 'amd64' , 'arm64' ]}, { key : 'karpenter.sh/capacity-type' , op : 'In' , vals : [ 'spot' , 'on-demand' ]}, ], subnetTags : { \"Name\" : \"blueprint-construct-dev/blueprint-construct-dev-vpc/PrivateSubnet1\" , }, securityGroupTags : { \"kubernetes.io/cluster/blueprint-construct-dev\" : \"owned\" , }, taints : [{ key : \"workload\" , value : \"test\" , effect : \"NoSchedule\" , }], amiFamily : \"AL2\" , consolidation : { enabled : true }, ttlSecondsUntilExpired : 2592000 , weight : 20 , interruptionHandling : true , } const karpenterAddOn = new blueprints . addons . KarpenterAddOn ( karpenterAddonProps ); const blueprint = blueprints . EksBlueprint . builder () . addOns ( karpenterAddOn ) . build ( app , 'my-stack-name' ); The add-on automatically sets the following Helm Chart values , and it is highly recommended not to pass these values in (as it will result in errors): - settings.aws.defaultInstanceProfile - settings.aws.clusterEndpoint - settings.aws.clusterName - settings.aws.interruptionQueueName (if interruption handling is enabled) - serviceAccount.create - serviceAccount.name - serviceAccount.annotations.eks.amazonaws.com/role-arn To validate that Karpenter add-on is running ensure that the add-on deployments for the controller and the webhook are in RUNNING state: # Assuming add-on is installed in the karpenter namespace. $ kubectl get po -n karpenter NAME READY STATUS RESTARTS AGE blueprints-addon-karpenter-54fd978b89-hclmp 2 /2 Running 0 99m","title":"Usage"},{"location":"addons/karpenter/#functionality","text":"Creates Karpenter Node Role, Karpenter Instance Profile, and Karpenter Controller Policy (Please see Karpenter documentation here for more details on what is required and why). Creates karpenter namespace. Creates Kubernetes Service Account, and associate AWS IAM Role with Karpenter Controller Policy attached using IRSA . Deploys Karpenter helm chart in the karpenter namespace, configuring cluster name and cluster endpoint on the controller by default. (Optionally) provisions a default Karpenter Provisioner CRD based on user-provided spec.requirements , AMI type , taints and tags. If created, the provisioner will discover the EKS VPC subnets and security groups to launch the nodes with. NOTE: 1. The default provisioner is created only if both the subnet tags and the security group tags are provided. 2. Provisioner spec requirement fields are not necessary, as karpenter will dynamically choose (i.e. leaving instance-type blank will let karpenter choose approrpriate sizing). 3. Consolidation, which is a flag that enables , is supported on versions 0.15.0 and later. It is also mutually exclusive with ttlSecondsAfterempty , so if you provide both properties, the addon will throw an error. 4. Weight, which is a property to prioritize provisioners based on weight, is supported on versions 0.16.0 and later. Addon will throw an error if weight is provided for earlier versions. 5. Interruption Handling, which is a native way to handle interruption due to involuntary interruption events, is supported on versions 0.19.0 and later. For interruption handling in the earlier versions, Karpenter supports using AWS Node Interruption Handler (which you will need to add as an add-on and must be in add-on array after the Karpenter add-on for it to work.","title":"Functionality"},{"location":"addons/karpenter/#using-karpenter","text":"To use Karpenter, you need to provision a Karpenter provisioner CRD . A single provisioner is capable of handling many different pod shapes. This can be done in 2 ways: Provide the properties as show in Usage . If subnet tags and security group tags are not provided at deploy time, the add-on will be installed without a Provisioner. Use kubectl to apply a sample provisioner manifest: cat <<EOF | kubectl apply -f - apiVersion: karpenter.sh/v1alpha5 kind: Provisioner metadata: name: default spec: requirements: - key: \"node.kubernetes.io/instance-type\" operator: In values: [\"m5.2xlarge\"] - key: \"topology.kubernetes.io/zone\" operator: In values: [\"us-east-1c\"] - key: \"kubernetes.io/arch\" operator: In values: [\"arm64\", \"amd64\"] - key: \"karpenter.sh/capacity-type\" operator: In values: [\"spot\", \"on-demand\"] provider: instanceProfile: <<Name of your Instance Profile>> subnetSelector: Name: blueprint-construct-dev/blueprint-construct-dev-vpc/PrivateSubnet1 securityGroupSelector: \"kubernetes.io/cluster/blueprint-construct-dev\": \"owned\" ttlSecondsAfterEmpty: 30 EOF If you choose to create a provisioner manually, you MUST provide the tags that match the subnet and the security group that you want to use.","title":"Using Karpenter"},{"location":"addons/karpenter/#testing-with-a-sample-deployment","text":"Now that the provisioner is deployed, Karpenter is active and ready to provision nodes. Create some pods using a deployment: cat <<EOF | kubectl apply -f - apiVersion: apps/v1 kind: Deployment metadata: name: inflate spec: replicas: 0 selector: matchLabels: app: inflate template: metadata: labels: app: inflate spec: terminationGracePeriodSeconds: 0 containers: - name: inflate image: public.ecr.aws/eks-distro/kubernetes/pause:3.2 resources: requests: cpu: 1 EOF Now scale the deployment: kubectl scale deployment inflate --replicas 10 The provisioner will then start deploying more nodes to deploy the scaled replicas. You can verify by either looking at the karpenter controller logs, kubectl logs -f -n karpenter $( kubectl get pods -n karpenter -l karpenter = controller -o name ) or, by looking at the nodes being created: kubectl get nodes","title":"Testing with a sample deployment"},{"location":"addons/karpenter/#troubleshooting","text":"The following are common troubleshooting issues observed when implementing Karpenter: For Karpenter version older than 0.14.0 deployed on Fargate Profiles, values.yaml must be overridden, setting dnsPolicy to Default . Versions after 0.14.0 has dnsPolicy value set default to Default . This is to ensure CoreDNS is set correctly on Fargate nodes. With the upgrade to the new OCI registry starting with v0.17.0 , if you try to upgrade you may get a following error: Received response status [FAILED] from custom resource. Message returned: Error: b'Error: path \"/tmp/tmpkxgr57q5/blueprints-addon-karpenter\" not found\\n' Karpenter, starting from the OCI registry versions, will untar the files under karpenter release name only. So if you have previous version deployed under a different release name, you will run into the above error. Therefore, in order to upgrade, you will have to take the following steps: Remove the existing add-on. Re-deploy the Karpenter add-on with the release name karpenter .","title":"Troubleshooting"},{"location":"addons/karpenter/#upgrade-path","text":"Using an older version of the Karptner add-on, you may notice the difference in the \"provisionerSpecs\" property: provisionerSpecs: { 'node.kubernetes.io/instance-type': ['m5.2xlarge'], 'topology.kubernetes.io/zone': ['us-east-1c'], 'kubernetes.io/arch': ['amd64','arm64'], 'karpenter.sh/capacity-type': ['spot','on-demand'], }, This now changes to the \"requirement\" property: requirements: [ { key: 'node.kubernetes.io/instance-type', op: 'In', vals: ['m5.2xlarge'] }, { key: 'topology.kubernetes.io/zone', op: 'NotIn', vals: ['us-west-2c']}, { key: 'kubernetes.io/arch', op: 'In', vals: ['amd64','arm64']}, { key: 'karpenter.sh/capacity-type', op: 'In', vals: ['spot','on-demand']}, ], The property is changed to align with the naming convention of the provisioner, and to allow multiple operators (In vs NotIn). The values correspond similarly between the two, with type change being the only difference. Certain upgrades require reapplying the CRDs since Helm does not maintain the lifecycle of CRDs. Please see the official documentations for details. Starting with v0.17.0, Karpenter's Helm chart package is stored in OCI (Open Container Initiative) registry. With this change, charts.karpenter.sh is no longer updated to preserve older versions. You have to adjust for the following: The full URL needs to be present (including 'oci://'). You need to append a v to the version number (i.e. v0.17.0, not 0.17.0) Starting with v0.22.0, Karpenter will no longer work on Kubernetes version prior to 1.21. Either upgrade your Kubernetes to 1.21 or later version and apply Karpenter, or use prior Karpenter versions.","title":"Upgrade Path"},{"location":"addons/kasten-k10/","text":"Kasten K10 Add-On for Amazon EKS Blueprints for CDK \u00b6 Kasten K10 by Veeam Overview The K10 data management platform, purpose-built for Kubernetes, provides enterprise operations teams an easy-to-use, scalable, and secure system for backup/restore, disaster recovery, and mobility of Kubernetes applications. K10\u2019s application-centric approach and deep integrations with relational and NoSQL databases, Amazon EKS and AWS Services provides teams the freedom of infrastructure choice without sacrificing operational simplicity. Policy-driven and extensible, K10 provides a native Kubernetes API and includes features such full-spectrum consistency, database integrations, automatic application discovery, application mobility, and a powerful web-based user interface. Given K10\u2019s extensive ecosystem support you have the flexibility to choose environments (public/ private/ hybrid cloud/ on-prem) and Kubernetes distributions (cloud vendor managed or self managed) in support of three principal use cases: Backup and Restore Disaster Recovery Application Mobility The Kasten K10 add-on installs Kasten K10 into your Amazon EKS cluster. Architecture \u00b6 Deploying this Quickstart for a new virtual private cloud (VPC) with default parameters builds the following K10 platform in the AWS Cloud. The diagram shows three Availability Zones, leveraging multiple AWS services. Installation \u00b6 Follow new project setup guidelines from https://github.com/aws-quickstart/cdk-eks-blueprints npm install @kastenhq/kasten-eks-blueprints-addon Basic Usage \u00b6 import { App } from 'aws-cdk-lib' ; import * as blueprints from '@aws-quickstart/eks-blueprints' ; import { KastenK10AddOn } from '@kastenhq/kasten-eks-blueprints-addon' ; const app = new App (); blueprints . EksBlueprint . builder () . addOns ( new blueprints . ClusterAutoScalerAddOn ) . addOns ( new KastenK10AddOn ) . build ( app , 'eks-with-kastenk10' ); Add-on Options \u00b6 Option Description Default repository Repository of the Helm chart \"https://charts.kasten.io/\" release Name of the Helm release \"k10\" namespace Namespace to install Kasten K10 \"kasten-io\" version Version of Kasten K10, defaults to latest release \"\" chart Helm Chart Name \"k10\" values Configuration values passed to the chart, options are documented here {} Functionality \u00b6 Create the IAM Role for Service Account for Kasten K10 pod to make API calls to AWS S3 and EC2 to backup and restore. Installs Kasten K10 in a new namespace \"kasten-io\". The IAM Policy is as follows: \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"ec2:CopySnapshot\", \"ec2:CreateSnapshot\", \"ec2:CreateTags\", \"ec2:CreateVolume\", \"ec2:DeleteTags\", \"ec2:DeleteVolume\", \"ec2:DescribeSnapshotAttribute\", \"ec2:ModifySnapshotAttribute\", \"ec2:DescribeAvailabilityZones\", \"ec2:DescribeSnapshots\", \"ec2:DescribeTags\", \"ec2:DescribeVolumeAttribute\", \"ec2:DescribeVolumesModifications\", \"ec2:DescribeVolumeStatus\", \"ec2:DescribeVolumes\" ], \"Resource\": \"*\" }, { \"Effect\": \"Allow\", \"Action\": \"ec2:DeleteSnapshot\", \"Resource\": \"*\", \"Condition\": { \"StringLike\": { \"ec2:ResourceTag/Name\": \"Kasten: Snapshot*\" } } }, { \"Effect\": \"Allow\", \"Action\": [ \"s3:CreateBucket\", \"s3:PutObject\", \"s3:GetObject\", \"s3:PutBucketPolicy\", \"s3:ListBucket\", \"s3:DeleteObject\", \"s3:DeleteBucketPolicy\", \"s3:GetBucketLocation\", \"s3:GetBucketPolicy\" ], \"Resource\": \"*\" } ] } Validating the Install \u00b6 To validate that K10 has been installed properly, the following command can be run in K10's namespace (the install default is kasten-io ) to watch for the status of all K10 pods: kubectl get pods --namespace kasten-io --watch It may take a couple of minutes for all pods to come up but all pods should ultimately display the status of Running . kubectl get pods --namespace kasten-io NAMESPACE NAME READY STATUS RESTARTS AGE kasten-io aggregatedapis-svc-b45d98bb5-w54pr 1/1 Running 0 1m26s kasten-io auth-svc-8549fc9c59-9c9fb 1/1 Running 0 1m26s kasten-io catalog-svc-f64666fdf-5t5tv 2/2 Running 0 1m26s In the unlikely scenario that pods that are stuck in any other state, please follow the support documentation to debug further. Validate Dashboard Access \u00b6 By default, the K10 dashboard will not be exposed externally. To establish a connection to it, use the following kubectl command to forward a local port to the K10 ingress port: kubectl --namespace kasten-io port-forward service/gateway 8080:8000 The K10 dashboard will be available at http://127.0.0.1:8080/k10/#/ . For a complete list of options for accessing the Kasten K10 dashboard through a LoadBalancer, Ingress or OpenShift Route you can use the instructions here . For full project documentation visit the KastenHQ Github Repo at (https://github.com/kastenhq/kasten-eks-blueprints-addon).","title":"Kasten"},{"location":"addons/kasten-k10/#kasten-k10-add-on-for-amazon-eks-blueprints-for-cdk","text":"Kasten K10 by Veeam Overview The K10 data management platform, purpose-built for Kubernetes, provides enterprise operations teams an easy-to-use, scalable, and secure system for backup/restore, disaster recovery, and mobility of Kubernetes applications. K10\u2019s application-centric approach and deep integrations with relational and NoSQL databases, Amazon EKS and AWS Services provides teams the freedom of infrastructure choice without sacrificing operational simplicity. Policy-driven and extensible, K10 provides a native Kubernetes API and includes features such full-spectrum consistency, database integrations, automatic application discovery, application mobility, and a powerful web-based user interface. Given K10\u2019s extensive ecosystem support you have the flexibility to choose environments (public/ private/ hybrid cloud/ on-prem) and Kubernetes distributions (cloud vendor managed or self managed) in support of three principal use cases: Backup and Restore Disaster Recovery Application Mobility The Kasten K10 add-on installs Kasten K10 into your Amazon EKS cluster.","title":"Kasten K10 Add-On for Amazon EKS Blueprints for CDK"},{"location":"addons/kasten-k10/#architecture","text":"Deploying this Quickstart for a new virtual private cloud (VPC) with default parameters builds the following K10 platform in the AWS Cloud. The diagram shows three Availability Zones, leveraging multiple AWS services.","title":"Architecture"},{"location":"addons/kasten-k10/#installation","text":"Follow new project setup guidelines from https://github.com/aws-quickstart/cdk-eks-blueprints npm install @kastenhq/kasten-eks-blueprints-addon","title":"Installation"},{"location":"addons/kasten-k10/#basic-usage","text":"import { App } from 'aws-cdk-lib' ; import * as blueprints from '@aws-quickstart/eks-blueprints' ; import { KastenK10AddOn } from '@kastenhq/kasten-eks-blueprints-addon' ; const app = new App (); blueprints . EksBlueprint . builder () . addOns ( new blueprints . ClusterAutoScalerAddOn ) . addOns ( new KastenK10AddOn ) . build ( app , 'eks-with-kastenk10' );","title":"Basic Usage"},{"location":"addons/kasten-k10/#add-on-options","text":"Option Description Default repository Repository of the Helm chart \"https://charts.kasten.io/\" release Name of the Helm release \"k10\" namespace Namespace to install Kasten K10 \"kasten-io\" version Version of Kasten K10, defaults to latest release \"\" chart Helm Chart Name \"k10\" values Configuration values passed to the chart, options are documented here {}","title":"Add-on Options"},{"location":"addons/kasten-k10/#functionality","text":"Create the IAM Role for Service Account for Kasten K10 pod to make API calls to AWS S3 and EC2 to backup and restore. Installs Kasten K10 in a new namespace \"kasten-io\". The IAM Policy is as follows: \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": [ \"ec2:CopySnapshot\", \"ec2:CreateSnapshot\", \"ec2:CreateTags\", \"ec2:CreateVolume\", \"ec2:DeleteTags\", \"ec2:DeleteVolume\", \"ec2:DescribeSnapshotAttribute\", \"ec2:ModifySnapshotAttribute\", \"ec2:DescribeAvailabilityZones\", \"ec2:DescribeSnapshots\", \"ec2:DescribeTags\", \"ec2:DescribeVolumeAttribute\", \"ec2:DescribeVolumesModifications\", \"ec2:DescribeVolumeStatus\", \"ec2:DescribeVolumes\" ], \"Resource\": \"*\" }, { \"Effect\": \"Allow\", \"Action\": \"ec2:DeleteSnapshot\", \"Resource\": \"*\", \"Condition\": { \"StringLike\": { \"ec2:ResourceTag/Name\": \"Kasten: Snapshot*\" } } }, { \"Effect\": \"Allow\", \"Action\": [ \"s3:CreateBucket\", \"s3:PutObject\", \"s3:GetObject\", \"s3:PutBucketPolicy\", \"s3:ListBucket\", \"s3:DeleteObject\", \"s3:DeleteBucketPolicy\", \"s3:GetBucketLocation\", \"s3:GetBucketPolicy\" ], \"Resource\": \"*\" } ] }","title":"Functionality"},{"location":"addons/kasten-k10/#validating-the-install","text":"To validate that K10 has been installed properly, the following command can be run in K10's namespace (the install default is kasten-io ) to watch for the status of all K10 pods: kubectl get pods --namespace kasten-io --watch It may take a couple of minutes for all pods to come up but all pods should ultimately display the status of Running . kubectl get pods --namespace kasten-io NAMESPACE NAME READY STATUS RESTARTS AGE kasten-io aggregatedapis-svc-b45d98bb5-w54pr 1/1 Running 0 1m26s kasten-io auth-svc-8549fc9c59-9c9fb 1/1 Running 0 1m26s kasten-io catalog-svc-f64666fdf-5t5tv 2/2 Running 0 1m26s In the unlikely scenario that pods that are stuck in any other state, please follow the support documentation to debug further.","title":"Validating the Install"},{"location":"addons/kasten-k10/#validate-dashboard-access","text":"By default, the K10 dashboard will not be exposed externally. To establish a connection to it, use the following kubectl command to forward a local port to the K10 ingress port: kubectl --namespace kasten-io port-forward service/gateway 8080:8000 The K10 dashboard will be available at http://127.0.0.1:8080/k10/#/ . For a complete list of options for accessing the Kasten K10 dashboard through a LoadBalancer, Ingress or OpenShift Route you can use the instructions here . For full project documentation visit the KastenHQ Github Repo at (https://github.com/kastenhq/kasten-eks-blueprints-addon).","title":"Validate Dashboard Access"},{"location":"addons/keda/","text":"Keda Add-on \u00b6 This add-on installs Keda Kubernetes-based Event Driven Autoscaling. KEDA allows for fine-grained autoscaling (including to/from zero) for event driven Kubernetes workloads. KEDA serves as a Kubernetes Metrics Server and allows users to define autoscaling rules using a dedicated Kubernetes custom resource definition. Usage \u00b6 import 'source-map-support/register' ; import * as cdk from 'aws-cdk-lib' ; import * as blueprints from '@aws-quickstart/eks-blueprints' ; const app = new cdk . App (); # Normal Usage # const addOn = new blueprints . addons . KedaAddOn (); # In case of AWS SQS , there is a workaround required when initializing keda ( Please refer https : //github.com/kedacore/keda/issues/837#issuecomment-789037326 ) const kedaParams = { podSecurityContextFsGroup : 1001 , securityContextRunAsGroup : 1001 , securityContextRunAsUser : 1001 , irsaRoles : [ \"CloudWatchFullAccess\" , \"AmazonSQSFullAccess\" ] } const addOn = new blueprints . addons . KedaAddOn ( kedaParams ) const blueprint = blueprints . EksBlueprint . builder () . addOns ( addOn ) . build ( app , 'my-stack-name' ); Configuration Options \u00b6 version : Version fo the Helm Chart to be used to install Keda kedaOperatorName : Name of the KEDA operator createServiceAccount : Specifies whether a service account should be created by Keda. kedaServiceAccountName : The name of the service account to use. If not set and create is true, a name is generated. podSecurityContextFsGroup : PodSecurityContext holds pod-level security attributes and common container settings. fsGroup is a special supplemental group that applies to all containers in a pod. This is an optional attribute, exposed directly in order to make KedaScalar workaroud - https://github.com/kedacore/keda/issues/837#issuecomment-789037326 securityContextRunAsUser : SecurityContext holds security configuration that will be applied to a container. runAsUser is the UID to run the entrypoint of the container process. This is an optional attribute, exposed directly in order to make KedaScalar workaroud - https://github.com/kedacore/keda/issues/837#issuecomment-789037326 securityContextRunAsGroup : SecurityContext holds security configuration that will be applied to a container. runAsGroup is the GID to run the entrypoint of the container process. This is an optional attribute, exposed directly in order to make KedaScalar workaroud - https://github.com/kedacore/keda/issues/837#issuecomment-789037326 irsaRoles - An array of Managed IAM Policies which Service Account needs for IRSA Eg: irsaRoles:[\"CloudWatchFullAccess\",\"AmazonSQSFullAccess\"]. If not empty the Service Account will be created by the CDK with IAM Roles Mapped (IRSA). In case if its empty, Keda will create the Service Account with out IAM Roles values : Arbitrary values to pass to the chart. Refer to the Keda Helm Chart documentation for additional details Validation \u00b6 To validate that Keda is installed properly in the cluster, check that the Keda deployments, services and stateful sets are running. kubectl get deployments -n keda There should be 2 deployments are created once for operator/agent and another for metrics server kubectl get pods -n keda There should be 2 pods one startin with name keda-operator- (Eg: \"keda-operator-56bb8ddd5b-7fqb7\") Verify IRSA is working properly kubectl describe pods keda-operator-8c5967bcd-vlhpd -n keda | grep -i aws kubectl get sa -n keda kubectl describe sa keda-operator -n keda There should be an Service Account with name KedaOperator created and you can see an annotation on Service Account which have the AWS IAM Role ARN ( with required access for AWS Services like AWS SQS) Testing \u00b6 We will test AWS SQS Scalar here 1) Create SQS Queue in desired region Replace ${AWS_REGION} with your target region aws sqs create-queue --queue-name keda-test --region ${ AWS_REGION } --output json 2) Create a deployment with SQS Consumer, For simplicity we will create an nginx deployment (in real case scenarion it should be an SQS Consumer) After the port-forwarding has started, the application can be accessed by navigating to http://localhost:8080 kubectl create ns sqs-consumer kubectl create deployment sqs-consumer --image nginx -n sqs-consumer 3) Create SQS Scalar by creating and applying following yaml file for eg: keda-scalar.yaml Replace ${AWS_REGION} with your target region --- apiVersion : keda.sh/v1alpha1 # https://keda.sh/docs/2.0/concepts/scaling-deployments/ kind : ScaledObject metadata : name : sqs-consumer-keda-scaler namespace : sqs-consumer labels : app : sqs-consumer deploymentName : sqs-consumer spec : scaleTargetRef : kind : Deployment name : sqs-consumer minReplicaCount : 1 maxReplicaCount : 50 pollingInterval : 10 cooldownPeriod : 500 triggers : - type : aws-sqs-queue metadata : queueURL : https://sqs.${AWS_REGION}.amazonaws.com/ACCOUNT_NUMBER/sqs-consumer queueLength : \"5\" awsRegion : \"${AWS_REGION}\" identityOwner : operator --- kubectl apply -f keda-scalar.yaml 4) Verify HPA is triggered for sqs-consumer namespace kubectl get hpa -n sqs-consumer 5) Now send 10 Messages to sqs queue Replace ${AWS_REGION} with your target region x = 10 a = 0 while [ $a -lt $x ] do aws sqs send-message --region ${ AWS_REGION } --endpoint-url https://sqs. ${ AWS_REGION } .amazonaws.com/ --queue-url https://sqs. ${ AWS_REGION } .amazonaws.com/ACCOUNT_NUMBER/sqs-consumer --message-body '{\"key\": \"value\"}' a = ` expr $a + 1 ` done 6) Verify if the nginx pod is autoscaled to 2 from 1 kubectl get pods -n sqs-consumer 7) Purge the SQS queue to test scale in event Replace ${AWS_REGION} with your target region aws sqs purge-queue --queue-url https://sqs. ${ AWS_REGION } .amazonaws.com/CCOUNT_NUMBER/sqs-consumer 6) Verify if the nginx pod is scaledd in from 2 to 1 after teh cool down perion set (500 in this case) kubectl get pods -n sqs-consumer Functionality \u00b6 Installs keda in the cluster Sets up IRSA so that Pods can interact with AWS Services Supports standard helm configuration options .","title":"Keda"},{"location":"addons/keda/#keda-add-on","text":"This add-on installs Keda Kubernetes-based Event Driven Autoscaling. KEDA allows for fine-grained autoscaling (including to/from zero) for event driven Kubernetes workloads. KEDA serves as a Kubernetes Metrics Server and allows users to define autoscaling rules using a dedicated Kubernetes custom resource definition.","title":"Keda Add-on"},{"location":"addons/keda/#usage","text":"import 'source-map-support/register' ; import * as cdk from 'aws-cdk-lib' ; import * as blueprints from '@aws-quickstart/eks-blueprints' ; const app = new cdk . App (); # Normal Usage # const addOn = new blueprints . addons . KedaAddOn (); # In case of AWS SQS , there is a workaround required when initializing keda ( Please refer https : //github.com/kedacore/keda/issues/837#issuecomment-789037326 ) const kedaParams = { podSecurityContextFsGroup : 1001 , securityContextRunAsGroup : 1001 , securityContextRunAsUser : 1001 , irsaRoles : [ \"CloudWatchFullAccess\" , \"AmazonSQSFullAccess\" ] } const addOn = new blueprints . addons . KedaAddOn ( kedaParams ) const blueprint = blueprints . EksBlueprint . builder () . addOns ( addOn ) . build ( app , 'my-stack-name' );","title":"Usage"},{"location":"addons/keda/#configuration-options","text":"version : Version fo the Helm Chart to be used to install Keda kedaOperatorName : Name of the KEDA operator createServiceAccount : Specifies whether a service account should be created by Keda. kedaServiceAccountName : The name of the service account to use. If not set and create is true, a name is generated. podSecurityContextFsGroup : PodSecurityContext holds pod-level security attributes and common container settings. fsGroup is a special supplemental group that applies to all containers in a pod. This is an optional attribute, exposed directly in order to make KedaScalar workaroud - https://github.com/kedacore/keda/issues/837#issuecomment-789037326 securityContextRunAsUser : SecurityContext holds security configuration that will be applied to a container. runAsUser is the UID to run the entrypoint of the container process. This is an optional attribute, exposed directly in order to make KedaScalar workaroud - https://github.com/kedacore/keda/issues/837#issuecomment-789037326 securityContextRunAsGroup : SecurityContext holds security configuration that will be applied to a container. runAsGroup is the GID to run the entrypoint of the container process. This is an optional attribute, exposed directly in order to make KedaScalar workaroud - https://github.com/kedacore/keda/issues/837#issuecomment-789037326 irsaRoles - An array of Managed IAM Policies which Service Account needs for IRSA Eg: irsaRoles:[\"CloudWatchFullAccess\",\"AmazonSQSFullAccess\"]. If not empty the Service Account will be created by the CDK with IAM Roles Mapped (IRSA). In case if its empty, Keda will create the Service Account with out IAM Roles values : Arbitrary values to pass to the chart. Refer to the Keda Helm Chart documentation for additional details","title":"Configuration Options"},{"location":"addons/keda/#validation","text":"To validate that Keda is installed properly in the cluster, check that the Keda deployments, services and stateful sets are running. kubectl get deployments -n keda There should be 2 deployments are created once for operator/agent and another for metrics server kubectl get pods -n keda There should be 2 pods one startin with name keda-operator- (Eg: \"keda-operator-56bb8ddd5b-7fqb7\") Verify IRSA is working properly kubectl describe pods keda-operator-8c5967bcd-vlhpd -n keda | grep -i aws kubectl get sa -n keda kubectl describe sa keda-operator -n keda There should be an Service Account with name KedaOperator created and you can see an annotation on Service Account which have the AWS IAM Role ARN ( with required access for AWS Services like AWS SQS)","title":"Validation"},{"location":"addons/keda/#testing","text":"We will test AWS SQS Scalar here 1) Create SQS Queue in desired region Replace ${AWS_REGION} with your target region aws sqs create-queue --queue-name keda-test --region ${ AWS_REGION } --output json 2) Create a deployment with SQS Consumer, For simplicity we will create an nginx deployment (in real case scenarion it should be an SQS Consumer) After the port-forwarding has started, the application can be accessed by navigating to http://localhost:8080 kubectl create ns sqs-consumer kubectl create deployment sqs-consumer --image nginx -n sqs-consumer 3) Create SQS Scalar by creating and applying following yaml file for eg: keda-scalar.yaml Replace ${AWS_REGION} with your target region --- apiVersion : keda.sh/v1alpha1 # https://keda.sh/docs/2.0/concepts/scaling-deployments/ kind : ScaledObject metadata : name : sqs-consumer-keda-scaler namespace : sqs-consumer labels : app : sqs-consumer deploymentName : sqs-consumer spec : scaleTargetRef : kind : Deployment name : sqs-consumer minReplicaCount : 1 maxReplicaCount : 50 pollingInterval : 10 cooldownPeriod : 500 triggers : - type : aws-sqs-queue metadata : queueURL : https://sqs.${AWS_REGION}.amazonaws.com/ACCOUNT_NUMBER/sqs-consumer queueLength : \"5\" awsRegion : \"${AWS_REGION}\" identityOwner : operator --- kubectl apply -f keda-scalar.yaml 4) Verify HPA is triggered for sqs-consumer namespace kubectl get hpa -n sqs-consumer 5) Now send 10 Messages to sqs queue Replace ${AWS_REGION} with your target region x = 10 a = 0 while [ $a -lt $x ] do aws sqs send-message --region ${ AWS_REGION } --endpoint-url https://sqs. ${ AWS_REGION } .amazonaws.com/ --queue-url https://sqs. ${ AWS_REGION } .amazonaws.com/ACCOUNT_NUMBER/sqs-consumer --message-body '{\"key\": \"value\"}' a = ` expr $a + 1 ` done 6) Verify if the nginx pod is autoscaled to 2 from 1 kubectl get pods -n sqs-consumer 7) Purge the SQS queue to test scale in event Replace ${AWS_REGION} with your target region aws sqs purge-queue --queue-url https://sqs. ${ AWS_REGION } .amazonaws.com/CCOUNT_NUMBER/sqs-consumer 6) Verify if the nginx pod is scaledd in from 2 to 1 after teh cool down perion set (500 in this case) kubectl get pods -n sqs-consumer","title":"Testing"},{"location":"addons/keda/#functionality","text":"Installs keda in the cluster Sets up IRSA so that Pods can interact with AWS Services Supports standard helm configuration options .","title":"Functionality"},{"location":"addons/knative-operator/","text":"Knative Operator Add-On \u00b6 Knative is an open source enterprise-grade solution to build serverless and Event Driven applications on Kubernetes. The Knative Operator provides support for installing, configuring and managing Knative without using custom CRDs. Knative Add-on supports standard helm configuration options Usage \u00b6 import * as cdk from 'aws-cdk-lib' ; import * as blueprints from '@aws-quickstart/eks-blueprints' ; const app = new cdk . App (); const addOns = [ new blueprints . addons . IstioBaseAddOn (), new blueprints . addons . IstioControlPlaneAddOn (), new blueprints . addons . KNativeOperator () ]; const blueprint = blueprints . EksBlueprint . builder () . addOns (... addOns ) . build ( app , 'my-stack-name' ); Applying KNative Eventing \u00b6 To apply KNative Eventing to a specific namespace, you can use the following YAML: apiVersion : v1 kind : Namespace metadata : name : knative-eventing --- apiVersion : operator.knative.dev/v1beta1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing You can also configure KNative to ingest from specific event sources. More configuration instructions can be found in their documentation about event source configurations Applying KNative Serving \u00b6 To apply KNative Serving to a specific Namespace, you can use the following YAML: apiVersion : v1 kind : Namespace metadata : name : knative-serving --- apiVersion : operator.knative.dev/v1beta1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving You will have to install a networking layer and configure it to ensure KNative Serving functions properly. The KNative Setup website has better documentation. Applying KNative Functions \u00b6 Currently, the Knative Operator does not support the deployment of Knative directly as they're directly run as services. For better instructions check (their documentation.)[https://knative.dev/docs/functions/deploying-functions/]","title":"Knative"},{"location":"addons/knative-operator/#knative-operator-add-on","text":"Knative is an open source enterprise-grade solution to build serverless and Event Driven applications on Kubernetes. The Knative Operator provides support for installing, configuring and managing Knative without using custom CRDs. Knative Add-on supports standard helm configuration options","title":"Knative Operator Add-On"},{"location":"addons/knative-operator/#usage","text":"import * as cdk from 'aws-cdk-lib' ; import * as blueprints from '@aws-quickstart/eks-blueprints' ; const app = new cdk . App (); const addOns = [ new blueprints . addons . IstioBaseAddOn (), new blueprints . addons . IstioControlPlaneAddOn (), new blueprints . addons . KNativeOperator () ]; const blueprint = blueprints . EksBlueprint . builder () . addOns (... addOns ) . build ( app , 'my-stack-name' );","title":"Usage"},{"location":"addons/knative-operator/#applying-knative-eventing","text":"To apply KNative Eventing to a specific namespace, you can use the following YAML: apiVersion : v1 kind : Namespace metadata : name : knative-eventing --- apiVersion : operator.knative.dev/v1beta1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing You can also configure KNative to ingest from specific event sources. More configuration instructions can be found in their documentation about event source configurations","title":"Applying KNative Eventing"},{"location":"addons/knative-operator/#applying-knative-serving","text":"To apply KNative Serving to a specific Namespace, you can use the following YAML: apiVersion : v1 kind : Namespace metadata : name : knative-serving --- apiVersion : operator.knative.dev/v1beta1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving You will have to install a networking layer and configure it to ensure KNative Serving functions properly. The KNative Setup website has better documentation.","title":"Applying KNative Serving"},{"location":"addons/knative-operator/#applying-knative-functions","text":"Currently, the Knative Operator does not support the deployment of Knative directly as they're directly run as services. For better instructions check (their documentation.)[https://knative.dev/docs/functions/deploying-functions/]","title":"Applying KNative Functions"},{"location":"addons/kube-proxy/","text":"Kube-proxy Amazon EKS Add-on \u00b6 The Kube-proxy Amazon EKS Add-on adds support for kube-proxy . Kube-proxy maintains network rules on each Amazon EC2 node. It enables network communication to your pods. Kube-proxy is not deployed to Fargate nodes. For more information, see kube-proxy in the Kubernetes documentation. Installing Kube-proxy as Amazon EKS add-on will reduce the amount of work that is needed to do in order to install, configure, and update add-ons. It includes the latest security patches, bug fixes and is validated by AWS to work with Amazon EKS. This ensures that Amazon EKS clusters are secure and stable. Amazon EKS automatically installs Kube-proxy as self-managed add-on for every cluster. So if it is already running on your cluster, you can still install it as Amazon EKS add-on to start benefiting from the capabilities of Amazon EKS add-ons. Prerequisite \u00b6 Amazon EKS add-ons are only available with Amazon EKS clusters running Kubernetes version 1.18 and later. Usage \u00b6 import 'source-map-support/register' ; import * as cdk from 'aws-cdk-lib' ; import * as blueprints from '@aws-quickstart/eks-blueprints' ; const app = new cdk . App (); const addOn = new blueprints . addons . KubeProxyAddOn ( 'v1.19.6-eksbuild.2' ); // optionally specify the image version to pull or empty constructor const blueprint = blueprints . EksBlueprint . builder () . addOns ( addOn ) . build ( app , 'my-stack-name' ); Configuration Options \u00b6 version : Pass in the kube-proxy plugin version compatible with kubernetes-cluster version as shown below # Assuming cluster version is 1.20, below command shows versions of the Kube-proxy add-on available for the specified cluster's version. aws eks describe-addon-versions \\ --addon-name kube-proxy \\ --kubernetes-version 1 .19 \\ --query \"addons[].addonVersions[].[addonVersion, compatibilities[].defaultVersion]\" \\ --output text # Output v1.19.6-eksbuild.2 True v1.18.8-eksbuild.1 False Validation \u00b6 To validate that kube-proxy add-on is running, ensure that the pod is in Running state $ kubectl get pods -n kube-system | grep kube-proxy NAME READY STATUS RESTARTS AGE kube-proxy-6lrjm 1 /1 Running 0 34d # Assuming cluster-name is my-cluster, below command shows the version of Kube-proxy installed. Check if it is same as the version installed via EKS add-on aws eks describe-addon \\ --cluster-name my-cluster \\ --addon-name kube-proxy \\ --query \"addon.addonVersion\" \\ --output text # Output v1.19.6-eksbuild.2 Functionality \u00b6 Applies Kube-proxy add-on to Amazon EKS cluster.","title":"Kube Proxy"},{"location":"addons/kube-proxy/#kube-proxy-amazon-eks-add-on","text":"The Kube-proxy Amazon EKS Add-on adds support for kube-proxy . Kube-proxy maintains network rules on each Amazon EC2 node. It enables network communication to your pods. Kube-proxy is not deployed to Fargate nodes. For more information, see kube-proxy in the Kubernetes documentation. Installing Kube-proxy as Amazon EKS add-on will reduce the amount of work that is needed to do in order to install, configure, and update add-ons. It includes the latest security patches, bug fixes and is validated by AWS to work with Amazon EKS. This ensures that Amazon EKS clusters are secure and stable. Amazon EKS automatically installs Kube-proxy as self-managed add-on for every cluster. So if it is already running on your cluster, you can still install it as Amazon EKS add-on to start benefiting from the capabilities of Amazon EKS add-ons.","title":"Kube-proxy Amazon EKS Add-on"},{"location":"addons/kube-proxy/#prerequisite","text":"Amazon EKS add-ons are only available with Amazon EKS clusters running Kubernetes version 1.18 and later.","title":"Prerequisite"},{"location":"addons/kube-proxy/#usage","text":"import 'source-map-support/register' ; import * as cdk from 'aws-cdk-lib' ; import * as blueprints from '@aws-quickstart/eks-blueprints' ; const app = new cdk . App (); const addOn = new blueprints . addons . KubeProxyAddOn ( 'v1.19.6-eksbuild.2' ); // optionally specify the image version to pull or empty constructor const blueprint = blueprints . EksBlueprint . builder () . addOns ( addOn ) . build ( app , 'my-stack-name' );","title":"Usage"},{"location":"addons/kube-proxy/#configuration-options","text":"version : Pass in the kube-proxy plugin version compatible with kubernetes-cluster version as shown below # Assuming cluster version is 1.20, below command shows versions of the Kube-proxy add-on available for the specified cluster's version. aws eks describe-addon-versions \\ --addon-name kube-proxy \\ --kubernetes-version 1 .19 \\ --query \"addons[].addonVersions[].[addonVersion, compatibilities[].defaultVersion]\" \\ --output text # Output v1.19.6-eksbuild.2 True v1.18.8-eksbuild.1 False","title":"Configuration Options"},{"location":"addons/kube-proxy/#validation","text":"To validate that kube-proxy add-on is running, ensure that the pod is in Running state $ kubectl get pods -n kube-system | grep kube-proxy NAME READY STATUS RESTARTS AGE kube-proxy-6lrjm 1 /1 Running 0 34d # Assuming cluster-name is my-cluster, below command shows the version of Kube-proxy installed. Check if it is same as the version installed via EKS add-on aws eks describe-addon \\ --cluster-name my-cluster \\ --addon-name kube-proxy \\ --query \"addon.addonVersion\" \\ --output text # Output v1.19.6-eksbuild.2","title":"Validation"},{"location":"addons/kube-proxy/#functionality","text":"Applies Kube-proxy add-on to Amazon EKS cluster.","title":"Functionality"},{"location":"addons/kube-state-metrics/","text":"Kube State Metrics Add-on \u00b6 This add-on installs kube-state-metrics . kube-state-metrics (KSM) is a simple service that listens to the Kubernetes API server and generates metrics about the state of the objects. (See examples in the Metrics section below.) It is not focused on the health of the individual Kubernetes components, but rather on the health of the various objects inside, such as deployments, nodes and pods. kube-state-metrics Add-on is about generating metrics from Kubernetes API objects without modification. This ensures that features provided by kube-state-metrics have the same grade of stability as the Kubernetes API objects themselves. Usage \u00b6 import * as cdk from 'aws-cdk-lib' ; import * as blueprints from '@aws-quickstart/eks-blueprints' ; const app = new cdk . App (); const addOn = new blueprints . addons . KubeStateMetricsAddOn () const blueprint = blueprints . EksBlueprint . builder () . addOns ( addOn ) . build ( app , 'my-stack-name' ); Configuration Options \u00b6 createNamespace : (boolean) If you want CDK to create the namespace for you values : Arbitrary values to pass to the chart. Standard helm configuration options . Validation \u00b6 To validate that kube-state-metrics is installed properly in the cluster, check if the kube-state-metrics pods are running. Verify if the pods are running correctly for kube-state-metrics kubectl get pods -A | awk '/kube-state/ {print ;exit}' Output \u00b6 There should list pod starting with name kube-state For Eg: kube-system kube-state-metrics-5d4c95885d-82bv4 1 /1 Running 0 13h Functionality \u00b6 Applies the kube-state-metrics add-on to an Amazon EKS cluster.","title":"Kube State Metrics"},{"location":"addons/kube-state-metrics/#kube-state-metrics-add-on","text":"This add-on installs kube-state-metrics . kube-state-metrics (KSM) is a simple service that listens to the Kubernetes API server and generates metrics about the state of the objects. (See examples in the Metrics section below.) It is not focused on the health of the individual Kubernetes components, but rather on the health of the various objects inside, such as deployments, nodes and pods. kube-state-metrics Add-on is about generating metrics from Kubernetes API objects without modification. This ensures that features provided by kube-state-metrics have the same grade of stability as the Kubernetes API objects themselves.","title":"Kube State Metrics Add-on"},{"location":"addons/kube-state-metrics/#usage","text":"import * as cdk from 'aws-cdk-lib' ; import * as blueprints from '@aws-quickstart/eks-blueprints' ; const app = new cdk . App (); const addOn = new blueprints . addons . KubeStateMetricsAddOn () const blueprint = blueprints . EksBlueprint . builder () . addOns ( addOn ) . build ( app , 'my-stack-name' );","title":"Usage"},{"location":"addons/kube-state-metrics/#configuration-options","text":"createNamespace : (boolean) If you want CDK to create the namespace for you values : Arbitrary values to pass to the chart. Standard helm configuration options .","title":"Configuration Options"},{"location":"addons/kube-state-metrics/#validation","text":"To validate that kube-state-metrics is installed properly in the cluster, check if the kube-state-metrics pods are running. Verify if the pods are running correctly for kube-state-metrics kubectl get pods -A | awk '/kube-state/ {print ;exit}'","title":"Validation"},{"location":"addons/kube-state-metrics/#output","text":"There should list pod starting with name kube-state For Eg: kube-system kube-state-metrics-5d4c95885d-82bv4 1 /1 Running 0 13h","title":"Output"},{"location":"addons/kube-state-metrics/#functionality","text":"Applies the kube-state-metrics add-on to an Amazon EKS cluster.","title":"Functionality"},{"location":"addons/kubecost/","text":"Kubecost AddOn \u00b6 Kubecost provides real-time cost visibility and insights by uncovering patterns that create overspending on infrastructure to help teams prioritize where to focus optimization efforts. By identifying root causes for negative patterns, customers using Kubecost save 30-50% or more of their Kubernetes cloud infrastructure costs. To read more about Kubecost and how to use it, see the product and technical docs . Installation \u00b6 Using npm : $ npm install @kubecost/kubecost-eks-blueprints-addon Usage \u00b6 import 'source-map-support/register' ; import * as cdk from 'aws-cdk-lib' ; import * as blueprints from '@aws-quickstart/eks-blueprints' ; import { KubecostAddOn } from '@kubecost/kubecost-eks-blueprints-addon' ; const app = new cdk . App (); const addOn = new KubecostAddOn (); const blueprint = blueprints . EksBlueprint . builder () . addOns ( addOn ) . build ( app , 'my-stack-name' ); KubecostAddOn Options (props) \u00b6 namespace: string (optional) \u00b6 The namespace where Kubecost will be installed. Defaults to kubecost . kubecostToken: string (optional) \u00b6 You may get one here . version: string (optional) \u00b6 The cost-analyzer helm chart version. Defaults to the latest stable version specified in this repo ( 1.92.0 at the time of writing). values?: { [key: string]: any } (optional) \u00b6 Custom values to pass to the chart. Config options: https://github.com/kubecost/cost-analyzer-helm-chart/blob/master/README.md#config-options customPrometheus: string (optional) \u00b6 Kubecost comes bundled with a Prometheus installation. However, if you wish to integrate with an external Prometheus deployment, provide your local Prometheus service address with this format http://..svc . Note: integrating with an existing Prometheus is only officially supported under Kubecost paid plans and requires some extra configurations on your Prometheus: https://docs.kubecost.com/custom-prom.html installPrometheusNodeExporter: boolean (optional) \u00b6 Set to false to use an existing Node Exporter DaemonSet. Note: this requires your existing Node Exporter endpoint to be visible from the namespace where Kubecost is installed. https://github.com/kubecost/docs/blob/main/getting-started.md#using-an-existing-node-exporter repository: string , release: string , chart: string (optional) \u00b6 Additional options for customers who may need to supply their own private Helm repository. Support \u00b6 If you have any questions about Kubecost, get in touch with the team on Slack . License \u00b6 The Kubecost Blueprints AddOn is licensed under the Apache 2.0 license. Project repository","title":"Kubecost"},{"location":"addons/kubecost/#kubecost-addon","text":"Kubecost provides real-time cost visibility and insights by uncovering patterns that create overspending on infrastructure to help teams prioritize where to focus optimization efforts. By identifying root causes for negative patterns, customers using Kubecost save 30-50% or more of their Kubernetes cloud infrastructure costs. To read more about Kubecost and how to use it, see the product and technical docs .","title":"Kubecost AddOn"},{"location":"addons/kubecost/#installation","text":"Using npm : $ npm install @kubecost/kubecost-eks-blueprints-addon","title":"Installation"},{"location":"addons/kubecost/#usage","text":"import 'source-map-support/register' ; import * as cdk from 'aws-cdk-lib' ; import * as blueprints from '@aws-quickstart/eks-blueprints' ; import { KubecostAddOn } from '@kubecost/kubecost-eks-blueprints-addon' ; const app = new cdk . App (); const addOn = new KubecostAddOn (); const blueprint = blueprints . EksBlueprint . builder () . addOns ( addOn ) . build ( app , 'my-stack-name' );","title":"Usage"},{"location":"addons/kubecost/#kubecostaddon-options-props","text":"","title":"KubecostAddOn Options (props)"},{"location":"addons/kubecost/#namespace-string-optional","text":"The namespace where Kubecost will be installed. Defaults to kubecost .","title":"namespace: string (optional)"},{"location":"addons/kubecost/#kubecosttoken-string-optional","text":"You may get one here .","title":"kubecostToken: string (optional)"},{"location":"addons/kubecost/#version-string-optional","text":"The cost-analyzer helm chart version. Defaults to the latest stable version specified in this repo ( 1.92.0 at the time of writing).","title":"version: string (optional)"},{"location":"addons/kubecost/#values-key-string-any-optional","text":"Custom values to pass to the chart. Config options: https://github.com/kubecost/cost-analyzer-helm-chart/blob/master/README.md#config-options","title":"values?: { [key: string]: any } (optional)"},{"location":"addons/kubecost/#customprometheus-string-optional","text":"Kubecost comes bundled with a Prometheus installation. However, if you wish to integrate with an external Prometheus deployment, provide your local Prometheus service address with this format http://..svc . Note: integrating with an existing Prometheus is only officially supported under Kubecost paid plans and requires some extra configurations on your Prometheus: https://docs.kubecost.com/custom-prom.html","title":"customPrometheus: string (optional)"},{"location":"addons/kubecost/#installprometheusnodeexporter-boolean-optional","text":"Set to false to use an existing Node Exporter DaemonSet. Note: this requires your existing Node Exporter endpoint to be visible from the namespace where Kubecost is installed. https://github.com/kubecost/docs/blob/main/getting-started.md#using-an-existing-node-exporter","title":"installPrometheusNodeExporter: boolean (optional)"},{"location":"addons/kubecost/#repository-string-release-string-chart-string-optional","text":"Additional options for customers who may need to supply their own private Helm repository.","title":"repository: string, release: string, chart: string (optional)"},{"location":"addons/kubecost/#support","text":"If you have any questions about Kubecost, get in touch with the team on Slack .","title":"Support"},{"location":"addons/kubecost/#license","text":"The Kubecost Blueprints AddOn is licensed under the Apache 2.0 license. Project repository","title":"License"},{"location":"addons/kubeflow/","text":"Kubeflow AddOn \u00b6 The Kubeflow project is dedicated to making deployments of machine learning (ML) workflows on Kubernetes simple, portable and scalable. Our goal is not to recreate other services, but to provide a straightforward way to deploy best-of-breed open-source systems for ML to diverse infrastructures. Anywhere you are running Kubernetes, you should be able to run Kubeflow. Prerequisites: \u00b6 Ensure that you have installed the following tools on your machine. aws cli kubectl cdk npm Installation \u00b6 Using npm : $ npm install eks-blueprints-cdk-kubeflow-ext Usage \u00b6 import 'source-map-support/register' ; import * as cdk from 'aws-cdk-lib' ; import * as blueprints from '@aws-quickstart/eks-blueprints' ; import { KubeflowAddOn } from 'eks-blueprints-cdk-kubeflow-ext' ; const app = new cdk . App (); const addOn = new KubeflowAddOn ( { namespace : 'kubeflow-pipelines' } ); const blueprint = blueprints . EksBlueprint . builder () . addOns ( addOn ) . build ( app , 'my-stack-name' ); Configuration Options \u00b6 namespace : the namespace of your kubernetes cluster to be used to install kubeflow Verify the resources \u00b6 Run update-kubeconfig command. You should be able to get the command from CDK output message. More information can be found at https://aws-quickstart.github.io/cdk-eks-blueprints/getting-started/#cluster-access aws eks update-kubeconfig --name <your cluster name> --region <your region> --role-arn arn:aws:iam::xxxxxxxxx:role/kubeflow-blueprint-kubeflowblueprintMastersRole0C1-saJBO Let\u2019s verify the resources created by Steps above. kubectl get nodes # Output shows the EKS Managed Node group nodes kubectl get ns | kubeflow # Output shows kubeflow namespace kubectl get pods --namespace = kubeflow-pipelines # Output shows kubeflow pods Execute Machine learning jobs on Kubeflow \u00b6 log into Kubeflow pipeline UI by creating a port-forward to the ml-pipeline-ui service kubectl port-forward svc/ml-pipeline-ui 9000 :80 -n = kubeflow-pipelines and open this browser: http://localhost:9000/#/pipelines more pipeline examples can be found at https://www.kubeflow.org/docs/components/pipelines/tutorials/ Cleanup \u00b6 To clean up your EKS Blueprints, run the following commands: cdk destroy kubeflow-blueprint Kubeflow on EKS Pattern \u00b6 For more information about the Kubeflow add module, please visit Kubeflow on EKS Pattern . License \u00b6 The Kubeflow CDK Blueprints AddOn is licensed under the Apache 2.0 license. Project repository Disclaimer \u00b6 This pattern relies on an open source NPM package eks-blueprints-cdk-kubeflow-ext. Please refer to the package npm site for more information. https://www.npmjs.com/package/eks-blueprints-cdk-kubeflow-ext If you have any question about the npm package or find any defect, please post in the source repo at https://github.com/season1946/eks-blueprints-cdk-kubeflow-extension","title":"Kubeflow"},{"location":"addons/kubeflow/#kubeflow-addon","text":"The Kubeflow project is dedicated to making deployments of machine learning (ML) workflows on Kubernetes simple, portable and scalable. Our goal is not to recreate other services, but to provide a straightforward way to deploy best-of-breed open-source systems for ML to diverse infrastructures. Anywhere you are running Kubernetes, you should be able to run Kubeflow.","title":"Kubeflow AddOn"},{"location":"addons/kubeflow/#prerequisites","text":"Ensure that you have installed the following tools on your machine. aws cli kubectl cdk npm","title":"Prerequisites:"},{"location":"addons/kubeflow/#installation","text":"Using npm : $ npm install eks-blueprints-cdk-kubeflow-ext","title":"Installation"},{"location":"addons/kubeflow/#usage","text":"import 'source-map-support/register' ; import * as cdk from 'aws-cdk-lib' ; import * as blueprints from '@aws-quickstart/eks-blueprints' ; import { KubeflowAddOn } from 'eks-blueprints-cdk-kubeflow-ext' ; const app = new cdk . App (); const addOn = new KubeflowAddOn ( { namespace : 'kubeflow-pipelines' } ); const blueprint = blueprints . EksBlueprint . builder () . addOns ( addOn ) . build ( app , 'my-stack-name' );","title":"Usage"},{"location":"addons/kubeflow/#configuration-options","text":"namespace : the namespace of your kubernetes cluster to be used to install kubeflow","title":"Configuration Options"},{"location":"addons/kubeflow/#verify-the-resources","text":"Run update-kubeconfig command. You should be able to get the command from CDK output message. More information can be found at https://aws-quickstart.github.io/cdk-eks-blueprints/getting-started/#cluster-access aws eks update-kubeconfig --name <your cluster name> --region <your region> --role-arn arn:aws:iam::xxxxxxxxx:role/kubeflow-blueprint-kubeflowblueprintMastersRole0C1-saJBO Let\u2019s verify the resources created by Steps above. kubectl get nodes # Output shows the EKS Managed Node group nodes kubectl get ns | kubeflow # Output shows kubeflow namespace kubectl get pods --namespace = kubeflow-pipelines # Output shows kubeflow pods","title":"Verify the resources"},{"location":"addons/kubeflow/#execute-machine-learning-jobs-on-kubeflow","text":"log into Kubeflow pipeline UI by creating a port-forward to the ml-pipeline-ui service kubectl port-forward svc/ml-pipeline-ui 9000 :80 -n = kubeflow-pipelines and open this browser: http://localhost:9000/#/pipelines more pipeline examples can be found at https://www.kubeflow.org/docs/components/pipelines/tutorials/","title":"Execute Machine learning jobs on Kubeflow"},{"location":"addons/kubeflow/#cleanup","text":"To clean up your EKS Blueprints, run the following commands: cdk destroy kubeflow-blueprint","title":"Cleanup"},{"location":"addons/kubeflow/#kubeflow-on-eks-pattern","text":"For more information about the Kubeflow add module, please visit Kubeflow on EKS Pattern .","title":"Kubeflow on EKS Pattern"},{"location":"addons/kubeflow/#license","text":"The Kubeflow CDK Blueprints AddOn is licensed under the Apache 2.0 license. Project repository","title":"License"},{"location":"addons/kubeflow/#disclaimer","text":"This pattern relies on an open source NPM package eks-blueprints-cdk-kubeflow-ext. Please refer to the package npm site for more information. https://www.npmjs.com/package/eks-blueprints-cdk-kubeflow-ext If you have any question about the npm package or find any defect, please post in the source repo at https://github.com/season1946/eks-blueprints-cdk-kubeflow-extension","title":"Disclaimer"},{"location":"addons/kubevious/","text":"Kubevious Add-on \u00b6 This add-on installs Kubevious open source Kubernetes dashboard on Amazon EKS. Kubevious provides logical grouping of application resources eliminating the need to dig through selectors and labels. It also provides the ability identify potential misconfigurations using both standard and user created rules that monitor the cluster Usage \u00b6 import 'source-map-support/register' ; import * as cdk from 'aws-cdk-lib' ; import * as blueprints from '@aws-quickstart/eks-blueprints' ; const app = new cdk . App (); const addOn = new blueprints . addons . KubeviousAddOn (); const blueprint = blueprints . EksBlueprint . builder () . addOns ( addOn ) . build ( app , 'my-stack-name' ); Configuration Options \u00b6 version : Version fo the Helm Chart to be used to install Kubevious ingressEnabled : Indicates whether to expose Kubevious using an ingress gateway. Set to false by default kubeviousServiceType : Type of service used to expose Kubevious backend. Set to 'ClusterIP' by default values : Arbitrary values to pass to the chart. Refer to the Kubevious Helm Chart documentation for additional details Validation \u00b6 To validate that Kubevious is installed properly in the cluster, check that the Kubevious deployments, services and stateful sets are running. kubectl get all -n kubevious Note that Kubevious is installed in its own kubevious namespace Accessing the Kubevious dashboard \u00b6 To access the application, set up port-forwarding as follows: kubectl port-forward $( kubectl get pods -n kubevious -l \"app.kubernetes.io/component=kubevious-ui\" -o jsonpath = \"{.items[0].metadata.name}\" ) 8080 :80 -n kubevious After the port-forwarding has started, the application can be accessed by navigating to http://localhost:8080 Alternatively, Kubevious can be exposed by enabling the ingress by setting the ingressEnabled configuration option to true. MySQL root password \u00b6 Kubevious internally deploys and uses MySQL to persist data. The Kubevious add-on secures access to the database by generating a random password for the MySQL root user. While it is not usually necessary to access the Kubevious MySQL database externally, it is possible to retrieve the generated value by executing the command below: echo $( kubectl get secret kubevious-mysql-secret-root -o jsonpath = '{.data.MYSQL_ROOT_PASSWORD}' -n kubevious ) | base64 --decode Persistent Volume usage \u00b6 Kubevious automatically creates a Persistent Volume (PV) to store the MySQL database data. However, per the Kubevious documentation , the PV is not removed when Kubevious is uninstalled and must be removed manually: kubectl delete pvc data-kubevious-mysql-0 -n kubevious Functionality \u00b6 Installs Kubevious in the cluster Sets up all AIM necessary roles to integrate Kubevious in AWS EKS Supports standard helm configuration options .","title":"Kubevious"},{"location":"addons/kubevious/#kubevious-add-on","text":"This add-on installs Kubevious open source Kubernetes dashboard on Amazon EKS. Kubevious provides logical grouping of application resources eliminating the need to dig through selectors and labels. It also provides the ability identify potential misconfigurations using both standard and user created rules that monitor the cluster","title":"Kubevious Add-on"},{"location":"addons/kubevious/#usage","text":"import 'source-map-support/register' ; import * as cdk from 'aws-cdk-lib' ; import * as blueprints from '@aws-quickstart/eks-blueprints' ; const app = new cdk . App (); const addOn = new blueprints . addons . KubeviousAddOn (); const blueprint = blueprints . EksBlueprint . builder () . addOns ( addOn ) . build ( app , 'my-stack-name' );","title":"Usage"},{"location":"addons/kubevious/#configuration-options","text":"version : Version fo the Helm Chart to be used to install Kubevious ingressEnabled : Indicates whether to expose Kubevious using an ingress gateway. Set to false by default kubeviousServiceType : Type of service used to expose Kubevious backend. Set to 'ClusterIP' by default values : Arbitrary values to pass to the chart. Refer to the Kubevious Helm Chart documentation for additional details","title":"Configuration Options"},{"location":"addons/kubevious/#validation","text":"To validate that Kubevious is installed properly in the cluster, check that the Kubevious deployments, services and stateful sets are running. kubectl get all -n kubevious Note that Kubevious is installed in its own kubevious namespace","title":"Validation"},{"location":"addons/kubevious/#accessing-the-kubevious-dashboard","text":"To access the application, set up port-forwarding as follows: kubectl port-forward $( kubectl get pods -n kubevious -l \"app.kubernetes.io/component=kubevious-ui\" -o jsonpath = \"{.items[0].metadata.name}\" ) 8080 :80 -n kubevious After the port-forwarding has started, the application can be accessed by navigating to http://localhost:8080 Alternatively, Kubevious can be exposed by enabling the ingress by setting the ingressEnabled configuration option to true.","title":"Accessing the Kubevious dashboard"},{"location":"addons/kubevious/#mysql-root-password","text":"Kubevious internally deploys and uses MySQL to persist data. The Kubevious add-on secures access to the database by generating a random password for the MySQL root user. While it is not usually necessary to access the Kubevious MySQL database externally, it is possible to retrieve the generated value by executing the command below: echo $( kubectl get secret kubevious-mysql-secret-root -o jsonpath = '{.data.MYSQL_ROOT_PASSWORD}' -n kubevious ) | base64 --decode","title":"MySQL root password"},{"location":"addons/kubevious/#persistent-volume-usage","text":"Kubevious automatically creates a Persistent Volume (PV) to store the MySQL database data. However, per the Kubevious documentation , the PV is not removed when Kubevious is uninstalled and must be removed manually: kubectl delete pvc data-kubevious-mysql-0 -n kubevious","title":"Persistent Volume usage"},{"location":"addons/kubevious/#functionality","text":"Installs Kubevious in the cluster Sets up all AIM necessary roles to integrate Kubevious in AWS EKS Supports standard helm configuration options .","title":"Functionality"},{"location":"addons/metrics-server/","text":"Metrics Server AddOn \u00b6 Metrics Server is a scalable, efficient source of container resource metrics for Kubernetes built-in autoscaling pipelines. It is not deployed by default in Amazon EKS clusters. The Metrics Server is commonly used by other Kubernetes add ons, such as the Horizontal Pod Autoscaler , Vertical Autoscaling or the Kubernetes Dashboard . Important : Don't use Metrics Server when you need an accurate source of resource usage metrics or as a monitoring solution. Usage \u00b6 index.ts \u00b6 import 'source-map-support/register' ; import * as cdk from 'aws-cdk-lib' ; import * as blueprints from '@aws-quickstart/eks-blueprints' ; const app = new cdk . App (); const addOn = new blueprints . addons . MetricsServerAddOn ( 'v0.5.0' ); const blueprint = blueprints . EksBlueprint . builder () . addOns ( addOn ) . build ( app , 'my-stack-name' ); Once deployed, you can see metrics-server pod in the kube-system namespace. $ kubectl get deployments -n kube-system NAME READY UP-TO-DATE AVAILABLE AGE blueprints-addon-metrics-server 1 /1 1 1 20m Functionality \u00b6 Deploys the metrics-server helm chart in kube-system namespace by default. Supports standard helm configuration options . Testing with Kubernetes Dashboard \u00b6 For testing, we will use the Kubernetes Dashboard to view CPU and memory metrics of our cluster. Apply the kubernetes dashboard manifest. $ kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.5/aio/deploy/recommended.yaml namespace/kubernetes-dashboard created serviceaccount/kubernetes-dashboard created service/kubernetes-dashboard created secret/kubernetes-dashboard-certs created secret/kubernetes-dashboard-csrf created secret/kubernetes-dashboard-key-holder created configmap/kubernetes-dashboard-settings created role.rbac.authorization.k8s.io/kubernetes-dashboard created clusterrole.rbac.authorization.k8s.io/kubernetes-dashboard created rolebinding.rbac.authorization.k8s.io/kubernetes-dashboard created clusterrolebinding.rbac.authorization.k8s.io/kubernetes-dashboard created deployment.apps/kubernetes-dashboard created service/dashboard-metrics-scraper created deployment.apps/dashboard-metrics-scraper created Create a file called eks-admin-service-account.yaml with the text below. This manifest defines a service account and cluster role binding called eks-admin. $ cat << 'EOF' >> eks-admin-service-account.yaml --- apiVersion: v1 kind: ServiceAccount metadata: name: eks-admin namespace: kube-system --- apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRoleBinding metadata: name: eks-admin roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: eks-admin namespace: kube-system EOF Apply the service account and cluster role binding to your cluster. $ kubectl apply -f eks-admin-service-account.yaml serviceaccount/eks-admin created clusterrolebinding.rbac.authorization.k8s.io/eks-admin created Retrieve an authentication token for the eks-admin service account. Copy the value from the output. You use this token to connect to the dashboard. $ kubectl -n kube-system describe secret $( kubectl -n kube-system get secret | grep eks-admin | awk '{print $1}' ) Name: eks-admin-token-dwzb2 Namespace: kube-system Labels: <none> Annotations: kubernetes.io/service-account.name: eks-admin kubernetes.io/service-account.uid: 6fb4eb46-553e-44bf-b0e7-9ae8f5f500d6 Type: kubernetes.io/service-account-token Data ==== ca.crt: 1066 bytes namespace: 11 bytes token: XXXXXXXXXXXXXXXXXXXXXX Start the kubectl proxy. $ kubectl proxy Open the dashboard in your browser and login using the value for token above. Note : It may take a few minutes before CPU and memory metrics appear in the dashboard","title":"Metrics Server"},{"location":"addons/metrics-server/#metrics-server-addon","text":"Metrics Server is a scalable, efficient source of container resource metrics for Kubernetes built-in autoscaling pipelines. It is not deployed by default in Amazon EKS clusters. The Metrics Server is commonly used by other Kubernetes add ons, such as the Horizontal Pod Autoscaler , Vertical Autoscaling or the Kubernetes Dashboard . Important : Don't use Metrics Server when you need an accurate source of resource usage metrics or as a monitoring solution.","title":"Metrics Server AddOn"},{"location":"addons/metrics-server/#usage","text":"","title":"Usage"},{"location":"addons/metrics-server/#indexts","text":"import 'source-map-support/register' ; import * as cdk from 'aws-cdk-lib' ; import * as blueprints from '@aws-quickstart/eks-blueprints' ; const app = new cdk . App (); const addOn = new blueprints . addons . MetricsServerAddOn ( 'v0.5.0' ); const blueprint = blueprints . EksBlueprint . builder () . addOns ( addOn ) . build ( app , 'my-stack-name' ); Once deployed, you can see metrics-server pod in the kube-system namespace. $ kubectl get deployments -n kube-system NAME READY UP-TO-DATE AVAILABLE AGE blueprints-addon-metrics-server 1 /1 1 1 20m","title":"index.ts"},{"location":"addons/metrics-server/#functionality","text":"Deploys the metrics-server helm chart in kube-system namespace by default. Supports standard helm configuration options .","title":"Functionality"},{"location":"addons/metrics-server/#testing-with-kubernetes-dashboard","text":"For testing, we will use the Kubernetes Dashboard to view CPU and memory metrics of our cluster. Apply the kubernetes dashboard manifest. $ kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.5/aio/deploy/recommended.yaml namespace/kubernetes-dashboard created serviceaccount/kubernetes-dashboard created service/kubernetes-dashboard created secret/kubernetes-dashboard-certs created secret/kubernetes-dashboard-csrf created secret/kubernetes-dashboard-key-holder created configmap/kubernetes-dashboard-settings created role.rbac.authorization.k8s.io/kubernetes-dashboard created clusterrole.rbac.authorization.k8s.io/kubernetes-dashboard created rolebinding.rbac.authorization.k8s.io/kubernetes-dashboard created clusterrolebinding.rbac.authorization.k8s.io/kubernetes-dashboard created deployment.apps/kubernetes-dashboard created service/dashboard-metrics-scraper created deployment.apps/dashboard-metrics-scraper created Create a file called eks-admin-service-account.yaml with the text below. This manifest defines a service account and cluster role binding called eks-admin. $ cat << 'EOF' >> eks-admin-service-account.yaml --- apiVersion: v1 kind: ServiceAccount metadata: name: eks-admin namespace: kube-system --- apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRoleBinding metadata: name: eks-admin roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-admin subjects: - kind: ServiceAccount name: eks-admin namespace: kube-system EOF Apply the service account and cluster role binding to your cluster. $ kubectl apply -f eks-admin-service-account.yaml serviceaccount/eks-admin created clusterrolebinding.rbac.authorization.k8s.io/eks-admin created Retrieve an authentication token for the eks-admin service account. Copy the value from the output. You use this token to connect to the dashboard. $ kubectl -n kube-system describe secret $( kubectl -n kube-system get secret | grep eks-admin | awk '{print $1}' ) Name: eks-admin-token-dwzb2 Namespace: kube-system Labels: <none> Annotations: kubernetes.io/service-account.name: eks-admin kubernetes.io/service-account.uid: 6fb4eb46-553e-44bf-b0e7-9ae8f5f500d6 Type: kubernetes.io/service-account-token Data ==== ca.crt: 1066 bytes namespace: 11 bytes token: XXXXXXXXXXXXXXXXXXXXXX Start the kubectl proxy. $ kubectl proxy Open the dashboard in your browser and login using the value for token above. Note : It may take a few minutes before CPU and memory metrics appear in the dashboard","title":"Testing with Kubernetes Dashboard"},{"location":"addons/newrelic/","text":"New Relic Addon - AWS EKS Blueprints for CDK \u00b6 This repository contains the source code for the New Relic AddOn for AWS EKS Blueprints. EKS Blueprints for CDK is a framework that makes it easy for customers to configure and deploy New Relic's Kubernetes integration as part of an EKS Blueprints cluster on Amazon EKS . Installation \u00b6 Using npm : npm install @newrelic/newrelic-eks-blueprints-addon For a quick tutorial on EKS Blueprints, visit the Getting Started guide . Retrieving keys \u00b6 The New Relic and Pixie keys can be obtained from the New Relic Guided Install for Kubernetes . AWS Secrets Manager key format \u00b6 { \"nrLicenseKey\": \"xxxxNRAL\", \"pixieDeployKey\": \"px-dep-xxxx\", \"pixieApiKey\": \"px-api-xxxx\" } Example Configuration (using keys stored in Secrets Manager): \u00b6 import 'source-map-support/register' ; import * as cdk from 'aws-cdk-lib' ; import * as blueprints from '@aws-quickstart/eks-blueprints' ; import { NewRelicAddOn } from '@newrelic/newrelic-eks-blueprints-addon' ; const app = new cdk . App (); blueprints . EksBlueprint . builder () . addOns ( new blueprints . MetricsServerAddOn ) . addOns ( new blueprints . ClusterAutoScalerAddOn ) . addOns ( new blueprints . addons . SSMAgentAddOn ) . addOns ( new blueprints . addons . SecretsStoreAddOn ) . addOns ( new NewRelicAddOn ({ version : \"4.2.0-beta\" , newRelicClusterName : \"demo-cluster\" , awsSecretName : \"newrelic-pixie-combined\" , // Secret Name in AWS Secrets Manager installPixie : true , installPixieIntegration : true , })) . region ( process . env . AWS_REGION ) . account ( process . env . AWS_ACCOUNT ) . build ( app , 'demo-cluster' ); Example Configuration (using keys): \u00b6 import 'source-map-support/register' ; import * as cdk from 'aws-cdk-lib' ; import * as blueprints from '@aws-quickstart/eks-blueprints' ; import { NewRelicAddOn } from '@newrelic/newrelic-eks-blueprints-addon' ; const app = new cdk . App (); blueprints . EksBlueprint . builder () . addOns ( new blueprints . MetricsServerAddOn ) . addOns ( new blueprints . ClusterAutoScalerAddOn ) . addOns ( new blueprints . addons . SSMAgentAddOn ) . addOns ( new blueprints . addons . SecretsStoreAddOn ) . addOns ( new NewRelicAddOn ({ version : \"4.2.0-beta\" , newRelicClusterName : \"demo-cluster\" , newRelicLicenseKey : \"NEW RELIC LICENSE KEY\" , installPixie : true , installPixieIntegration : true , pixieApiKey : \"PIXIE API KEY\" , pixieDeployKey : \"PIXIE DEPLOY KEY\" })) . region ( process . env . AWS_REGION ) . account ( process . env . AWS_ACCOUNT ) . build ( app , 'demo-cluster' ); Validation \u00b6 NRQL Query \u00b6 Almost immediately after the New Relic pods enter a Running state in the cluster, data should be reported to New Relic. You can validate that metrics are making it to New Relic with the following NRQL query: FROM K8sClusterSample, K8sNodeSample select latest(clusterK8sVersion), latest(agentVersion) as 'NR Agent Ver.', uniqueCount(nodeName) as 'Node Count' facet clusterName limit max New Relic One UI \u00b6 After installing the New Relic add-on, you can validate a successful installation by visiting New Relic's Entity Explorer filtered to Kubernetes Clusters. Variables \u00b6 Variable Type Required Description newRelicLicenseKey string True New Relic License Key (plain text). Use awsSecretName instead for AWS Secrets Manager support and added security. awsSecretName string True AWS Secret name containing the New Relic and Pixie keys in AWS Secrets Manager. Define secret in JSON format with the following keys: { \"nrLicenseKey\": \"REPLACE WITH YOUR NEW RELIC LICENSE KEY\", \"pixieDeployKey\": \"REPLACE WITH YOUR PIXIE LICENSE KEY\", \"pixieApiKey\": \"REPLACE WITH YOUR PIXIE API KEY\" } Keys can be obtained in the New Relic Guided Install for Kubernetes newRelicClusterName string True Name for the cluster in the New Relic UI. pixieApiKey string Pixie Api Key can be obtained in New Relic's Guided Install for Kubernetes (plaintext). Use awsSecretName instead for AWS Secrets Manager support and added security. pixieDeployKey string Pixie Deploy Key can be obtained in New Relic's Guided Install for Kubernetes - (plaintext). Use awsSecretName instead for AWS Secrets Manager support and added security. namespace string The namespace where New Relic components will be installed. Defaults to newrelic . lowDataMode boolean Default true . Set to false to disable lowDataMode . For more details, visit the Reducing Data Ingest Docs installInfrastructure boolean Default true . Set to false to disable installation of the New Relic Infrastructure Daemonset. installKSM boolean Default true . Set to false to disable installation of Kube State Metrics. An instance of KSM is required in the cluster for the New Relic Infrastructure Daemonset to function properly. installKubeEvents boolean Default true . Set to false to disable installation of the New Relic Kubernetes Events integration. installLogging boolean Default true . Set to false to disable installation of the New Relic Logging (Fluent-Bit) Daemonset. installMetricsAdapter boolean Default false . Set to true to enable installation of the New Relic Kubernetes Metrics Adapter. installPrometheus boolean Default true . Set to false to disable installation of the Prometheus OpenMetrics Integration. installPixie boolean Default false . Set to true to enable installation Pixie into the cluster. installPixieIntegration boolean Default false . Set to true to enable installation the New Relic <-> Pixie integration pod into the cluster. version string Helm chart version. repository string Additional options for customers who may need to supply their own private Helm repository. release string Additional options for customers who may need to supply their own private Helm repository. chart string Additional options for customers who may need to supply their own private Helm repository. values { [key: string]: any } Custom values to pass to the chart. Config options: https://github.com/newrelic/helm-charts/tree/master/charts/nri-bundle#configuration Support \u00b6 New Relic hosts and moderates an online forum where customers can interact with New Relic employees as well as other customers to get help and share best practices. https://discuss.newrelic.com/ Contributing \u00b6 We encourage your contributions to improve the New Relic Addon for EKS Blueprints! Keep in mind when you submit your pull request, you'll need to sign the CLA via the click-through using CLA-Assistant. You only have to sign the CLA one time per project. If you have any questions, or to execute our corporate CLA, required if your contribution is on behalf of a company, please drop us an email at opensource@newrelic.com. A note about vulnerabilities As noted in our security policy , New Relic is committed to the privacy and security of our customers and their data. We believe that providing coordinated disclosure by security researchers and engaging with the security community are important means to achieve our security goals. If you believe you have found a security vulnerability in this project or any of New Relic's products or websites, we welcome and greatly appreciate you reporting it to New Relic through HackerOne . License \u00b6 The New Relic Addon for EKS Blueprints is licensed under the Apache 2.0 License.","title":"New Relic"},{"location":"addons/newrelic/#new-relic-addon-aws-eks-blueprints-for-cdk","text":"This repository contains the source code for the New Relic AddOn for AWS EKS Blueprints. EKS Blueprints for CDK is a framework that makes it easy for customers to configure and deploy New Relic's Kubernetes integration as part of an EKS Blueprints cluster on Amazon EKS .","title":"New Relic Addon - AWS EKS Blueprints for CDK"},{"location":"addons/newrelic/#installation","text":"Using npm : npm install @newrelic/newrelic-eks-blueprints-addon For a quick tutorial on EKS Blueprints, visit the Getting Started guide .","title":"Installation"},{"location":"addons/newrelic/#retrieving-keys","text":"The New Relic and Pixie keys can be obtained from the New Relic Guided Install for Kubernetes .","title":"Retrieving keys"},{"location":"addons/newrelic/#aws-secrets-manager-key-format","text":"{ \"nrLicenseKey\": \"xxxxNRAL\", \"pixieDeployKey\": \"px-dep-xxxx\", \"pixieApiKey\": \"px-api-xxxx\" }","title":"AWS Secrets Manager key format"},{"location":"addons/newrelic/#example-configuration-using-keys-stored-in-secrets-manager","text":"import 'source-map-support/register' ; import * as cdk from 'aws-cdk-lib' ; import * as blueprints from '@aws-quickstart/eks-blueprints' ; import { NewRelicAddOn } from '@newrelic/newrelic-eks-blueprints-addon' ; const app = new cdk . App (); blueprints . EksBlueprint . builder () . addOns ( new blueprints . MetricsServerAddOn ) . addOns ( new blueprints . ClusterAutoScalerAddOn ) . addOns ( new blueprints . addons . SSMAgentAddOn ) . addOns ( new blueprints . addons . SecretsStoreAddOn ) . addOns ( new NewRelicAddOn ({ version : \"4.2.0-beta\" , newRelicClusterName : \"demo-cluster\" , awsSecretName : \"newrelic-pixie-combined\" , // Secret Name in AWS Secrets Manager installPixie : true , installPixieIntegration : true , })) . region ( process . env . AWS_REGION ) . account ( process . env . AWS_ACCOUNT ) . build ( app , 'demo-cluster' );","title":"Example Configuration (using keys stored in Secrets Manager):"},{"location":"addons/newrelic/#example-configuration-using-keys","text":"import 'source-map-support/register' ; import * as cdk from 'aws-cdk-lib' ; import * as blueprints from '@aws-quickstart/eks-blueprints' ; import { NewRelicAddOn } from '@newrelic/newrelic-eks-blueprints-addon' ; const app = new cdk . App (); blueprints . EksBlueprint . builder () . addOns ( new blueprints . MetricsServerAddOn ) . addOns ( new blueprints . ClusterAutoScalerAddOn ) . addOns ( new blueprints . addons . SSMAgentAddOn ) . addOns ( new blueprints . addons . SecretsStoreAddOn ) . addOns ( new NewRelicAddOn ({ version : \"4.2.0-beta\" , newRelicClusterName : \"demo-cluster\" , newRelicLicenseKey : \"NEW RELIC LICENSE KEY\" , installPixie : true , installPixieIntegration : true , pixieApiKey : \"PIXIE API KEY\" , pixieDeployKey : \"PIXIE DEPLOY KEY\" })) . region ( process . env . AWS_REGION ) . account ( process . env . AWS_ACCOUNT ) . build ( app , 'demo-cluster' );","title":"Example Configuration (using keys):"},{"location":"addons/newrelic/#validation","text":"","title":"Validation"},{"location":"addons/newrelic/#nrql-query","text":"Almost immediately after the New Relic pods enter a Running state in the cluster, data should be reported to New Relic. You can validate that metrics are making it to New Relic with the following NRQL query: FROM K8sClusterSample, K8sNodeSample select latest(clusterK8sVersion), latest(agentVersion) as 'NR Agent Ver.', uniqueCount(nodeName) as 'Node Count' facet clusterName limit max","title":"NRQL Query"},{"location":"addons/newrelic/#new-relic-one-ui","text":"After installing the New Relic add-on, you can validate a successful installation by visiting New Relic's Entity Explorer filtered to Kubernetes Clusters.","title":"New Relic One UI"},{"location":"addons/newrelic/#variables","text":"Variable Type Required Description newRelicLicenseKey string True New Relic License Key (plain text). Use awsSecretName instead for AWS Secrets Manager support and added security. awsSecretName string True AWS Secret name containing the New Relic and Pixie keys in AWS Secrets Manager. Define secret in JSON format with the following keys: { \"nrLicenseKey\": \"REPLACE WITH YOUR NEW RELIC LICENSE KEY\", \"pixieDeployKey\": \"REPLACE WITH YOUR PIXIE LICENSE KEY\", \"pixieApiKey\": \"REPLACE WITH YOUR PIXIE API KEY\" } Keys can be obtained in the New Relic Guided Install for Kubernetes newRelicClusterName string True Name for the cluster in the New Relic UI. pixieApiKey string Pixie Api Key can be obtained in New Relic's Guided Install for Kubernetes (plaintext). Use awsSecretName instead for AWS Secrets Manager support and added security. pixieDeployKey string Pixie Deploy Key can be obtained in New Relic's Guided Install for Kubernetes - (plaintext). Use awsSecretName instead for AWS Secrets Manager support and added security. namespace string The namespace where New Relic components will be installed. Defaults to newrelic . lowDataMode boolean Default true . Set to false to disable lowDataMode . For more details, visit the Reducing Data Ingest Docs installInfrastructure boolean Default true . Set to false to disable installation of the New Relic Infrastructure Daemonset. installKSM boolean Default true . Set to false to disable installation of Kube State Metrics. An instance of KSM is required in the cluster for the New Relic Infrastructure Daemonset to function properly. installKubeEvents boolean Default true . Set to false to disable installation of the New Relic Kubernetes Events integration. installLogging boolean Default true . Set to false to disable installation of the New Relic Logging (Fluent-Bit) Daemonset. installMetricsAdapter boolean Default false . Set to true to enable installation of the New Relic Kubernetes Metrics Adapter. installPrometheus boolean Default true . Set to false to disable installation of the Prometheus OpenMetrics Integration. installPixie boolean Default false . Set to true to enable installation Pixie into the cluster. installPixieIntegration boolean Default false . Set to true to enable installation the New Relic <-> Pixie integration pod into the cluster. version string Helm chart version. repository string Additional options for customers who may need to supply their own private Helm repository. release string Additional options for customers who may need to supply their own private Helm repository. chart string Additional options for customers who may need to supply their own private Helm repository. values { [key: string]: any } Custom values to pass to the chart. Config options: https://github.com/newrelic/helm-charts/tree/master/charts/nri-bundle#configuration","title":"Variables"},{"location":"addons/newrelic/#support","text":"New Relic hosts and moderates an online forum where customers can interact with New Relic employees as well as other customers to get help and share best practices. https://discuss.newrelic.com/","title":"Support"},{"location":"addons/newrelic/#contributing","text":"We encourage your contributions to improve the New Relic Addon for EKS Blueprints! Keep in mind when you submit your pull request, you'll need to sign the CLA via the click-through using CLA-Assistant. You only have to sign the CLA one time per project. If you have any questions, or to execute our corporate CLA, required if your contribution is on behalf of a company, please drop us an email at opensource@newrelic.com. A note about vulnerabilities As noted in our security policy , New Relic is committed to the privacy and security of our customers and their data. We believe that providing coordinated disclosure by security researchers and engaging with the security community are important means to achieve our security goals. If you believe you have found a security vulnerability in this project or any of New Relic's products or websites, we welcome and greatly appreciate you reporting it to New Relic through HackerOne .","title":"Contributing"},{"location":"addons/newrelic/#license","text":"The New Relic Addon for EKS Blueprints is licensed under the Apache 2.0 License.","title":"License"},{"location":"addons/nginx/","text":"NGINX Add-on \u00b6 This add-on installs NGINX Ingress Controller on Amazon EKS. NGINX ingress controller is using NGINX as a reverse proxy and load balancer. Other than handling Kubernetes ingress objects, this ingress controller can facilitate multi-tenancy and segregation of workload ingresses based on host name (host-based routing) and/or URL Path (path based routing). IMPORTANT : This add-on depends on AWS Load Balancer Controller Add-on in order to enable NLB support. AWS Load Balancer Controller add-on must be present in add-on array and must be in add-on array before the NGINX ingress controller add-on for it to work, as shown in below example. Otherwise will run into error Assertion failed: Missing a dependency for AwsLoadBalancerControllerAddOn . Usage \u00b6 import 'source-map-support/register' ; import * as cdk from 'aws-cdk-lib' ; import * as blueprints from '@aws-quickstart/eks-blueprints' ; const app = new cdk . App (); const externalDnsHostname = ...; const awsLbControllerAddOn = new blueprints . addons . AwsLoadBalancerControllerAddOn (); const nginxAddOn = new blueprints . addons . NginxAddOn ({ externalDnsHostname }) const addOns : Array < blueprints . ClusterAddOn > = [ awsLbControllerAddOn , nginxAddOn ]; const blueprint = blueprints . EksBlueprint . builder () . addOns (... addOns ) . build ( app , 'my-stack-name' ); To validate that installation is successful run the following command: $ kubectl get po -n kube-system NAME READY STATUS RESTARTS AGE blueprints-addon-nginx-ingress-78b8567p4q6 1 /1 Running 0 4d10h Note that the ingress controller is deployed in the kube-system namespace. Once deployed, it allows applications to create ingress objects and use host based routing with external DNS support, if External DNS Add-on is installed. Configuration \u00b6 backendProtocol : indication for AWS Load Balancer controller with respect to the protocol supported on the load balancer. TCP by default. crossZoneEnabled : whether to create a cross-zone load balancer with the service that backs NGINX. internetFacing : whether the created load balancer is internet facing. Defaults to true if not specified. Internal load balancer is provisioned if set to false targetType : IP or instance mode. Defaults to IP which requires VPC-CNI and has better performance eliminating a hop through kubeproxy. Instance mode leverages traditional NodePort mode on the instances. externaDnsHostname : Used in conjunction with the external DNS add-on to handle automatic registration of the service with Route53. values : Arbitrary values to pass to the chart as per https://docs.nginx.com/nginx-ingress-controller/installation/installation-with-helm/# DNS Integration and Routing \u00b6 If External DNS Add-on is installed, it is possible to configure NGINX ingress with an external NLB load balancer and leverage wild-card DNS domains (and public certificate) to route external traffic to individual workloads. The following example provides support for AWS Load Balancer controller, External DNS and NGINX add-ons to enable such routing: blueprints . EksBlueprint . builder () // Register hosted zone1 under the name of MyHostedZone1 . resourceProvider ( \"MyHostedZone1\" , new blueprints . DelegatingHostedZoneProvider ({ parentDomain : 'myglobal-domain.com' , subdomain : 'dev.myglobal-domain.com' , parentAccountId : parentDnsAccountId , delegatingRoleName : 'DomainOperatorRole' , wildcardSubdomain : true }) . addOns ( new blueprints . addons . ExternalDnsAddOn ({ hostedZoneProviders : [ \"MyHostedZone1\" ]; }) . addOns ( new blueprints . NginxAddOn ({ internetFacing : true , backendProtocol : \"tcp\" , externaDnsHostname : subdomain , crossZoneEnabled : false }) . build (...); Assuming the subdomain in the above example is dev.my-domain.com and wildcard is enabled for the external DNS add-on customers can now create ingress objects for host-based routing. Let's define an ingress object for team-riker that is currently deploying guestbook application with no ingress: apiVersion : extensions/v1beta1 kind : Ingress metadata : annotations : kubernetes.io/ingress.class : nginx name : ingress-riker namespace : team-riker spec : rules : - host : riker.dev.my-domain.com http : paths : - backend : serviceName : guestbook-ui servicePort : 80 path : / pathType : Prefix A similar ingress may be defined for team-troi routing to the workloads deployed by that team: apiVersion : extensions/v1beta1 kind : Ingress metadata : annotations : kubernetes.io/ingress.class : nginx name : ingress-troi namespace : team-troi spec : rules : - host : troi.dev.my-domain.com http : paths : - backend : serviceName : guestbook-ui servicePort : 80 path : / pathType : Prefix After the above ingresses applied (ideally through a GitOps engine) you can now navigate to the specified hosts respectively: http://riker.dev.my-domain.com http://troi.dev.my-domain.com TLS Termination and Certificates \u00b6 You can configure the NGINX add-on to terminate TLS at the load balancer and supply an ACM certificate through the platform blueprint. A certificate can be registered using a named resource provider . For convenience the framework provides a couple of common certificate providers: Import Certificate This case is used when certificate is already created and you just need to reference it with the blueprint stack: const myCertArn = \"\" ; blueprints . EksBlueprint . builder () . resourceProvider ( GlobalResources . Certificate , new ImportCertificateProvider ( myCertArn , \"cert1-id\" )) . addOns ( new NginxAddOn ({ certificateResourceName : GlobalResources.Certificate , externalDnsHostname : 'my.domain.com' })) . teams (...) . build ( app , 'stack-with-cert-provider' ); Create Certificate This approach is used when certificate should be created with the blueprint stack. In this case, the new certificate requires DNS validation which can be accomplished automatically if the corresponding Route53 hosted zone is provisioned (either along with the stack or separately) and registered as a resource provider. blueprints . EksBlueprint . builder () . resourceProvider ( GlobalResources . HostedZone , new ImportHostedZoneProvider ( 'hosted-zone-id1' , 'my.domain.com' )) . resourceProvider ( GlobalResources . Certificate , new CreateCertificateProvider ( 'domain-wildcard-cert' , '*.my.domain.com' , GlobalResources . HostedZone )) // referencing hosted zone for automatic DNS validation . addOns ( new AwsLoadBalancerControllerAddOn ()) // Use hosted zone for External DNS . addOns ( new ExternalDnsAddOn ({ hostedZoneResources : [ GlobalResources . HostedZone ]})) // Use certificate registered before with NginxAddon . addOns ( new NginxAddOn ({ certificateResourceName : GlobalResources.Certificate , externalDnsHostname : 'my.domain.com' })) . teams (...) . build ( app , 'stack-with-resource-providers' ); Functionality \u00b6 Installs NGINX ingress controller Provides convenience options to integrate with AWS Load Balancer controller to leverage NLB for the load balancer Provides convenience options to integrate with External DNS add-on for integration with Amazon Route 53. Allows configuring TLS termination at the load balancer provisioned with the add-on. Supports standard helm configuration options .","title":"Nginx"},{"location":"addons/nginx/#nginx-add-on","text":"This add-on installs NGINX Ingress Controller on Amazon EKS. NGINX ingress controller is using NGINX as a reverse proxy and load balancer. Other than handling Kubernetes ingress objects, this ingress controller can facilitate multi-tenancy and segregation of workload ingresses based on host name (host-based routing) and/or URL Path (path based routing). IMPORTANT : This add-on depends on AWS Load Balancer Controller Add-on in order to enable NLB support. AWS Load Balancer Controller add-on must be present in add-on array and must be in add-on array before the NGINX ingress controller add-on for it to work, as shown in below example. Otherwise will run into error Assertion failed: Missing a dependency for AwsLoadBalancerControllerAddOn .","title":"NGINX Add-on"},{"location":"addons/nginx/#usage","text":"import 'source-map-support/register' ; import * as cdk from 'aws-cdk-lib' ; import * as blueprints from '@aws-quickstart/eks-blueprints' ; const app = new cdk . App (); const externalDnsHostname = ...; const awsLbControllerAddOn = new blueprints . addons . AwsLoadBalancerControllerAddOn (); const nginxAddOn = new blueprints . addons . NginxAddOn ({ externalDnsHostname }) const addOns : Array < blueprints . ClusterAddOn > = [ awsLbControllerAddOn , nginxAddOn ]; const blueprint = blueprints . EksBlueprint . builder () . addOns (... addOns ) . build ( app , 'my-stack-name' ); To validate that installation is successful run the following command: $ kubectl get po -n kube-system NAME READY STATUS RESTARTS AGE blueprints-addon-nginx-ingress-78b8567p4q6 1 /1 Running 0 4d10h Note that the ingress controller is deployed in the kube-system namespace. Once deployed, it allows applications to create ingress objects and use host based routing with external DNS support, if External DNS Add-on is installed.","title":"Usage"},{"location":"addons/nginx/#configuration","text":"backendProtocol : indication for AWS Load Balancer controller with respect to the protocol supported on the load balancer. TCP by default. crossZoneEnabled : whether to create a cross-zone load balancer with the service that backs NGINX. internetFacing : whether the created load balancer is internet facing. Defaults to true if not specified. Internal load balancer is provisioned if set to false targetType : IP or instance mode. Defaults to IP which requires VPC-CNI and has better performance eliminating a hop through kubeproxy. Instance mode leverages traditional NodePort mode on the instances. externaDnsHostname : Used in conjunction with the external DNS add-on to handle automatic registration of the service with Route53. values : Arbitrary values to pass to the chart as per https://docs.nginx.com/nginx-ingress-controller/installation/installation-with-helm/#","title":"Configuration"},{"location":"addons/nginx/#dns-integration-and-routing","text":"If External DNS Add-on is installed, it is possible to configure NGINX ingress with an external NLB load balancer and leverage wild-card DNS domains (and public certificate) to route external traffic to individual workloads. The following example provides support for AWS Load Balancer controller, External DNS and NGINX add-ons to enable such routing: blueprints . EksBlueprint . builder () // Register hosted zone1 under the name of MyHostedZone1 . resourceProvider ( \"MyHostedZone1\" , new blueprints . DelegatingHostedZoneProvider ({ parentDomain : 'myglobal-domain.com' , subdomain : 'dev.myglobal-domain.com' , parentAccountId : parentDnsAccountId , delegatingRoleName : 'DomainOperatorRole' , wildcardSubdomain : true }) . addOns ( new blueprints . addons . ExternalDnsAddOn ({ hostedZoneProviders : [ \"MyHostedZone1\" ]; }) . addOns ( new blueprints . NginxAddOn ({ internetFacing : true , backendProtocol : \"tcp\" , externaDnsHostname : subdomain , crossZoneEnabled : false }) . build (...); Assuming the subdomain in the above example is dev.my-domain.com and wildcard is enabled for the external DNS add-on customers can now create ingress objects for host-based routing. Let's define an ingress object for team-riker that is currently deploying guestbook application with no ingress: apiVersion : extensions/v1beta1 kind : Ingress metadata : annotations : kubernetes.io/ingress.class : nginx name : ingress-riker namespace : team-riker spec : rules : - host : riker.dev.my-domain.com http : paths : - backend : serviceName : guestbook-ui servicePort : 80 path : / pathType : Prefix A similar ingress may be defined for team-troi routing to the workloads deployed by that team: apiVersion : extensions/v1beta1 kind : Ingress metadata : annotations : kubernetes.io/ingress.class : nginx name : ingress-troi namespace : team-troi spec : rules : - host : troi.dev.my-domain.com http : paths : - backend : serviceName : guestbook-ui servicePort : 80 path : / pathType : Prefix After the above ingresses applied (ideally through a GitOps engine) you can now navigate to the specified hosts respectively: http://riker.dev.my-domain.com http://troi.dev.my-domain.com","title":"DNS Integration and Routing"},{"location":"addons/nginx/#tls-termination-and-certificates","text":"You can configure the NGINX add-on to terminate TLS at the load balancer and supply an ACM certificate through the platform blueprint. A certificate can be registered using a named resource provider . For convenience the framework provides a couple of common certificate providers: Import Certificate This case is used when certificate is already created and you just need to reference it with the blueprint stack: const myCertArn = \"\" ; blueprints . EksBlueprint . builder () . resourceProvider ( GlobalResources . Certificate , new ImportCertificateProvider ( myCertArn , \"cert1-id\" )) . addOns ( new NginxAddOn ({ certificateResourceName : GlobalResources.Certificate , externalDnsHostname : 'my.domain.com' })) . teams (...) . build ( app , 'stack-with-cert-provider' ); Create Certificate This approach is used when certificate should be created with the blueprint stack. In this case, the new certificate requires DNS validation which can be accomplished automatically if the corresponding Route53 hosted zone is provisioned (either along with the stack or separately) and registered as a resource provider. blueprints . EksBlueprint . builder () . resourceProvider ( GlobalResources . HostedZone , new ImportHostedZoneProvider ( 'hosted-zone-id1' , 'my.domain.com' )) . resourceProvider ( GlobalResources . Certificate , new CreateCertificateProvider ( 'domain-wildcard-cert' , '*.my.domain.com' , GlobalResources . HostedZone )) // referencing hosted zone for automatic DNS validation . addOns ( new AwsLoadBalancerControllerAddOn ()) // Use hosted zone for External DNS . addOns ( new ExternalDnsAddOn ({ hostedZoneResources : [ GlobalResources . HostedZone ]})) // Use certificate registered before with NginxAddon . addOns ( new NginxAddOn ({ certificateResourceName : GlobalResources.Certificate , externalDnsHostname : 'my.domain.com' })) . teams (...) . build ( app , 'stack-with-resource-providers' );","title":"TLS Termination and Certificates"},{"location":"addons/nginx/#functionality","text":"Installs NGINX ingress controller Provides convenience options to integrate with AWS Load Balancer controller to leverage NLB for the load balancer Provides convenience options to integrate with External DNS add-on for integration with Amazon Route 53. Allows configuring TLS termination at the load balancer provisioned with the add-on. Supports standard helm configuration options .","title":"Functionality"},{"location":"addons/opa-gatekeeper/","text":"What is OPA Gatekeeper? (Not Currently Supported, In Progress) \u00b6 The Open Policy Agent (OPA, pronounced \u201coh-pa\u201d) is an open source, general-purpose policy engine that unifies policy enforcement across the stack. OPA provides a high-level declarative language that lets you specify policy as code and simple APIs to offload policy decision-making from your software. You can use OPA to enforce policies in microservices, Kubernetes, CI/CD pipelines, API gateways, and more. OPA uses a policy language known as Rego which is a query language which was purpose built to support structured document models such as JSON. To learn more about Rego check out this link . OPA Gatekeeper is an open-source project that provides a first-class integration between OPA and Kubernetes. What Gatekeeper adds is an extensible parameterized policy library that includes native Kubernetes CRD's for instantiating and extending the OPA policy library. The Kubernetes API Server is configured to query OPA for admission control decisions when objects (e.g., Pods, Services, etc.) are created, updated, or deleted. The API Server sends the entire Kubernetes object in the webhook request to OPA. OPA evaluates the policies it has loaded using the admission review as input. Gatekeeper also provides audit functionality as well. The diagram below shows the flow between a user making a request to the Kube-API server and how AdmissionReview and AdmissionRequests are made through OPA Gatekeeper. ) In the context of a development platform running on Amazon EKS, platform teams and administrators need a way of being able to set policies to adhere to governance and security requirements for all workloads and teams working on the same cluster. Examples of standard use cases for using policies via OPA Gatekeeper are listed below: Which users can access which resources? Which subnets egress traffic is allowed to? Which clusters a workload must be deployed to? Which registries binaries can be downloaded from? Which OS capabilities a container can execute with? Which times of day the system can be accessed at? RBAC (role-based access control) can help with some of the scenarios above but roles are nothing but a group of permissions that you then assign to users leveraging rolebindings. If for example, a user tries to perform an operation (get, list, watch, create, etc...) that particular user may do so if they have the appropriate role. Please note that RBAC should be used in conjunction with OPA Gatekeeper policies to fully secure your cluster. Key Terminology \u00b6 OPA Constraint Framework - Framework that enforces CRD-based policies and allow declaratively configured policies to be reliably shareable Constraint - A Constraint is a declaration that its author wants a system to meet a given set of requirements. Each Constraint is written with Rego, a declarative query language used by OPA to enumerate instances of data that violate the expected state of the system. All Constraints are evaluated as a logical AND. If one Constraint is not satisfied, then the whole request is rejected. Enforcement Point - Places where constraints can be enforced. Examples are Git hooks, Kubernetes admission controllers, and audit systems. Constraint Template - Templates that allows users to declare new constraints Target - Represents a coherent set of objects sharing a common identification and/or selection scheme, generic purpose, and can be analyzed in the same validation context Usage \u00b6 import * as cdk from 'aws-cdk-lib' ; import * as blueprints from '@aws-quickstart/eks-blueprints' ; const app = new cdk . App (); const addOn = new blueprints . addons . OpaGatekeeperAddOn (); const blueprint = blueprints . EksBlueprint . builder () . addOns ( addOn ) . build ( app , 'my-stack-name' ); To validate that OPA Gatekeeper is running within your cluster run the following command: k get po -n gatekeeper-system You should see the following output: NAME READY STATUS RESTARTS AGE gatekeeper-audit-7c5998d4c-b5n7j 1 /1 Running 0 1d gatekeeper-controller-manager-5894545cc9-b86zm 1 /1 Running 0 1d gatekeeper-controller-manager-5894545cc9-bntdt 1 /1 Running 0 1d gatekeeper-controller-manager-5894545cc9-tb7fz 1 /1 Running 0 1d You will notice the gatekeeper-audit-7c5998d4c-b5n7j pod that is created when we deploy the OpaGatekeeperAddOn . The audit functionality enables periodic evaluations of replicated resources against the Constraints enforced in the cluster to detect pre-existing misconfigurations. Gatekeeper stores audit results as violations listed in the status field of the relevant Constraint. The gatekeeper-controller-manager is simply there to manage the OpaGatekeeperAddOn . Example with OPA Gatekeeper \u00b6 For the purposes of operating within a platform defined by EKS Blueprints , we will be focusing on how to use a policy driven approach to secure our cluster using OPA Gatekeeper. The OPA Gatekeeper community has created a library of example policies and constraint templates which can be found here . In this example we will create a policy that enforces including labels for newly created namespaces and pods. The ConstraintTemplate can be found here . Run the following command to create the ConstraintTemplate: kubectl apply -f https://raw.githubusercontent.com/open-policy-agent/gatekeeper-library/master/library/general/requiredlabels/template.yaml To verify that the ConstraintTemplate was created run the following command: kubectl get constrainttemplate You should see the following output: NAME AGE k8srequiredlabels 45s You will notice that if you create a new namespace without any labels, the request will go through and that is because we now need to create the individual Constraint CRD as defined by the Constraint Template that we created above. Let's create the individal Constraint CRD using the command below: k apply -f https://raw.githubusercontent.com/open-policy-agent/gatekeeper-library/master/library/general/requiredlabels/samples/all-must-have-owner/constraint.yaml If we then try and create a namespace by running kubectl create ns test (notice that we are not adding any labels) you will get the following error message: Error from server ([ all-must-have-owner ] All namespaces must have an ` owner ` label that points to your company username ) : admission webhook \"validation.gatekeeper.sh\" denied the request: [ all-must-have-owner ] All namespaces must have an ` owner ` label that points to your company username For more information on OPA Gatekeeper please refer to the links below: https://github.com/open-policy-agent https://open-policy-agent.github.io/gatekeeper/website/docs/ https://github.com/open-policy-agent/gatekeeper-library","title":"OPA Gatekeeper"},{"location":"addons/opa-gatekeeper/#what-is-opa-gatekeeper-not-currently-supported-in-progress","text":"The Open Policy Agent (OPA, pronounced \u201coh-pa\u201d) is an open source, general-purpose policy engine that unifies policy enforcement across the stack. OPA provides a high-level declarative language that lets you specify policy as code and simple APIs to offload policy decision-making from your software. You can use OPA to enforce policies in microservices, Kubernetes, CI/CD pipelines, API gateways, and more. OPA uses a policy language known as Rego which is a query language which was purpose built to support structured document models such as JSON. To learn more about Rego check out this link . OPA Gatekeeper is an open-source project that provides a first-class integration between OPA and Kubernetes. What Gatekeeper adds is an extensible parameterized policy library that includes native Kubernetes CRD's for instantiating and extending the OPA policy library. The Kubernetes API Server is configured to query OPA for admission control decisions when objects (e.g., Pods, Services, etc.) are created, updated, or deleted. The API Server sends the entire Kubernetes object in the webhook request to OPA. OPA evaluates the policies it has loaded using the admission review as input. Gatekeeper also provides audit functionality as well. The diagram below shows the flow between a user making a request to the Kube-API server and how AdmissionReview and AdmissionRequests are made through OPA Gatekeeper. ) In the context of a development platform running on Amazon EKS, platform teams and administrators need a way of being able to set policies to adhere to governance and security requirements for all workloads and teams working on the same cluster. Examples of standard use cases for using policies via OPA Gatekeeper are listed below: Which users can access which resources? Which subnets egress traffic is allowed to? Which clusters a workload must be deployed to? Which registries binaries can be downloaded from? Which OS capabilities a container can execute with? Which times of day the system can be accessed at? RBAC (role-based access control) can help with some of the scenarios above but roles are nothing but a group of permissions that you then assign to users leveraging rolebindings. If for example, a user tries to perform an operation (get, list, watch, create, etc...) that particular user may do so if they have the appropriate role. Please note that RBAC should be used in conjunction with OPA Gatekeeper policies to fully secure your cluster.","title":"What is OPA Gatekeeper? (Not Currently Supported, In Progress)"},{"location":"addons/opa-gatekeeper/#key-terminology","text":"OPA Constraint Framework - Framework that enforces CRD-based policies and allow declaratively configured policies to be reliably shareable Constraint - A Constraint is a declaration that its author wants a system to meet a given set of requirements. Each Constraint is written with Rego, a declarative query language used by OPA to enumerate instances of data that violate the expected state of the system. All Constraints are evaluated as a logical AND. If one Constraint is not satisfied, then the whole request is rejected. Enforcement Point - Places where constraints can be enforced. Examples are Git hooks, Kubernetes admission controllers, and audit systems. Constraint Template - Templates that allows users to declare new constraints Target - Represents a coherent set of objects sharing a common identification and/or selection scheme, generic purpose, and can be analyzed in the same validation context","title":"Key Terminology"},{"location":"addons/opa-gatekeeper/#usage","text":"import * as cdk from 'aws-cdk-lib' ; import * as blueprints from '@aws-quickstart/eks-blueprints' ; const app = new cdk . App (); const addOn = new blueprints . addons . OpaGatekeeperAddOn (); const blueprint = blueprints . EksBlueprint . builder () . addOns ( addOn ) . build ( app , 'my-stack-name' ); To validate that OPA Gatekeeper is running within your cluster run the following command: k get po -n gatekeeper-system You should see the following output: NAME READY STATUS RESTARTS AGE gatekeeper-audit-7c5998d4c-b5n7j 1 /1 Running 0 1d gatekeeper-controller-manager-5894545cc9-b86zm 1 /1 Running 0 1d gatekeeper-controller-manager-5894545cc9-bntdt 1 /1 Running 0 1d gatekeeper-controller-manager-5894545cc9-tb7fz 1 /1 Running 0 1d You will notice the gatekeeper-audit-7c5998d4c-b5n7j pod that is created when we deploy the OpaGatekeeperAddOn . The audit functionality enables periodic evaluations of replicated resources against the Constraints enforced in the cluster to detect pre-existing misconfigurations. Gatekeeper stores audit results as violations listed in the status field of the relevant Constraint. The gatekeeper-controller-manager is simply there to manage the OpaGatekeeperAddOn .","title":"Usage"},{"location":"addons/opa-gatekeeper/#example-with-opa-gatekeeper","text":"For the purposes of operating within a platform defined by EKS Blueprints , we will be focusing on how to use a policy driven approach to secure our cluster using OPA Gatekeeper. The OPA Gatekeeper community has created a library of example policies and constraint templates which can be found here . In this example we will create a policy that enforces including labels for newly created namespaces and pods. The ConstraintTemplate can be found here . Run the following command to create the ConstraintTemplate: kubectl apply -f https://raw.githubusercontent.com/open-policy-agent/gatekeeper-library/master/library/general/requiredlabels/template.yaml To verify that the ConstraintTemplate was created run the following command: kubectl get constrainttemplate You should see the following output: NAME AGE k8srequiredlabels 45s You will notice that if you create a new namespace without any labels, the request will go through and that is because we now need to create the individual Constraint CRD as defined by the Constraint Template that we created above. Let's create the individal Constraint CRD using the command below: k apply -f https://raw.githubusercontent.com/open-policy-agent/gatekeeper-library/master/library/general/requiredlabels/samples/all-must-have-owner/constraint.yaml If we then try and create a namespace by running kubectl create ns test (notice that we are not adding any labels) you will get the following error message: Error from server ([ all-must-have-owner ] All namespaces must have an ` owner ` label that points to your company username ) : admission webhook \"validation.gatekeeper.sh\" denied the request: [ all-must-have-owner ] All namespaces must have an ` owner ` label that points to your company username For more information on OPA Gatekeeper please refer to the links below: https://github.com/open-policy-agent https://open-policy-agent.github.io/gatekeeper/website/docs/ https://github.com/open-policy-agent/gatekeeper-library","title":"Example with OPA Gatekeeper"},{"location":"addons/pixie/","text":"Pixie Addon \u00b6 The Pixie Addon deploys Pixie on Amazon EKS using the EKS Blueprints CDK . Pixie is an open source observability tool for Kubernetes applications. Use Pixie to view the high-level state of your cluster (service maps, cluster resources, application traffic) and also drill-down into more detailed views (pod state, flame graphs, individual full-body application requests). Three features enable Pixie's magical developer experience: Auto-telemetry: Pixie uses eBPF to automatically collect telemetry data such as full-body requests, resource and network metrics, application profiles, and more. See the full list of data sources here . In-Cluster Edge Compute: Pixie collects, stores and queries all telemetry data locally in the cluster. Pixie uses less than 5% of cluster CPU, and in most cases less than 2%. Scriptability: PxL , Pixie\u2019s flexible Pythonic query language, can be used across Pixie\u2019s UI, CLI, and client APIs. Prerequisite \u00b6 You must have either: You need to have a Pixie account and deployment key on Community Cloud for Pixie . Or a Pixie account and deployment key on a self-hosted Pixie Cloud . Usage \u00b6 Run the following command to install the pixie-eks-blueprints-addon dependency in your project. npm i @pixie-labs/pixie-eks-blueprints-addon Using deploy key: \u00b6 import * as cdk from 'aws-cdk-lib'; import * as blueprints from '@aws-quickstart/eks-blueprints'; import { PixieAddOn } from '@pixie-labs/pixie-ssp-addon'; const app = new App(); const addOns: Array<blueprints.ClusterAddOn> = [ new PixieAddOn({ deployKey: 'pixie-deploy-key', // Create and copy from Pixie Admin UI }), ]; new blueprints.EksBlueprint( app, { id: 'my-stack-name', addOns, }, { env:{ account: <AWS_ACCOUNT_ID>, region: <AWS_REGION>, } }); Using deploy key stored in Secrets Manager: \u00b6 import * as cdk from 'aws-cdk-lib'; import * as blueprints from '@aws-quickstart/eks-blueprints'; import { PixieAddOn } from '@pixie-labs/pixie-ssp-addon'; const app = new App(); const addOns: Array<blueprints.ClusterAddOn> = [ new blueprints.addons.SecretsStoreAddOn, new PixieAddOn({ deployKeySecretName: \"pixie-deploy-key-secret\", // Name of secret in Secrets Manager. }), ]; new blueprints.EksBlueprint( app, { id: 'my-stack-name', addOns, }, { env:{ account: <AWS_ACCOUNT_ID>, region: <AWS_REGION>, } }); Addon Options (props) \u00b6 deployKey: string (optional) \u00b6 Pixie deployment key (plain text). Log into the Admin UI in Pixie to generate a deployment key. This attaches your Pixie deployment to your org. deployKeySecretName: string (optional) \u00b6 The name of the Pixie deployment key secret in Secrets Manager. The value of the key in Secrets Manager should be the deploy key in plaintext. Do not nest it inside a JSON object. namespace?: string (optional) \u00b6 Namespace to deploy Pixie to. Default: pl cloudAddr?: string (optional) \u00b6 The address of Pixie Cloud. This should only be modified if you have deployed your own self-hosted Pixie Cloud. By default, it will be set to Community Cloud for Pixie . devCloudNamespace?: string (optional) \u00b6 If running in a self-hosted cloud with no DNS configured, the namespace in which the self-hosted cloud is running. clusterName?: string (optional) \u00b6 The name of cluster. If none is specified, a random name will be generated. useEtcdOperator?: boolean (optional) \u00b6 Whether the metadata store should use etcd to store metadata, or use a persistent volume store. If not specified, the operator will deploy based on the cluster's storageClass configuration. pemMemoryLimit?: string (optional) \u00b6 The memory limit applied to the PEMs (data collectors). Set to 2Gi by default. dataAccess?: \"Full\"|\"Restricted\"|\"PIIRestricted\" (optional) \u00b6 DataAccess defines the level of data that may be accesssed when executing a script on the cluster. If none specified, assumes full data access. patches?: [key: string]: string (optional) \u00b6 Custom K8s patches which should be applied to the Pixie YAMLs. The key should be the name of the K8s resource, and the value is the patch that should be applied. version?: string (optional) \u00b6 Helm chart version. repository?: string , release?: string , chart?: string (optional) \u00b6 Additional options for customers who may need to supply their own private Helm repository.","title":"Pixie"},{"location":"addons/pixie/#pixie-addon","text":"The Pixie Addon deploys Pixie on Amazon EKS using the EKS Blueprints CDK . Pixie is an open source observability tool for Kubernetes applications. Use Pixie to view the high-level state of your cluster (service maps, cluster resources, application traffic) and also drill-down into more detailed views (pod state, flame graphs, individual full-body application requests). Three features enable Pixie's magical developer experience: Auto-telemetry: Pixie uses eBPF to automatically collect telemetry data such as full-body requests, resource and network metrics, application profiles, and more. See the full list of data sources here . In-Cluster Edge Compute: Pixie collects, stores and queries all telemetry data locally in the cluster. Pixie uses less than 5% of cluster CPU, and in most cases less than 2%. Scriptability: PxL , Pixie\u2019s flexible Pythonic query language, can be used across Pixie\u2019s UI, CLI, and client APIs.","title":"Pixie Addon"},{"location":"addons/pixie/#prerequisite","text":"You must have either: You need to have a Pixie account and deployment key on Community Cloud for Pixie . Or a Pixie account and deployment key on a self-hosted Pixie Cloud .","title":"Prerequisite"},{"location":"addons/pixie/#usage","text":"Run the following command to install the pixie-eks-blueprints-addon dependency in your project. npm i @pixie-labs/pixie-eks-blueprints-addon","title":"Usage"},{"location":"addons/pixie/#using-deploy-key","text":"import * as cdk from 'aws-cdk-lib'; import * as blueprints from '@aws-quickstart/eks-blueprints'; import { PixieAddOn } from '@pixie-labs/pixie-ssp-addon'; const app = new App(); const addOns: Array<blueprints.ClusterAddOn> = [ new PixieAddOn({ deployKey: 'pixie-deploy-key', // Create and copy from Pixie Admin UI }), ]; new blueprints.EksBlueprint( app, { id: 'my-stack-name', addOns, }, { env:{ account: <AWS_ACCOUNT_ID>, region: <AWS_REGION>, } });","title":"Using deploy key:"},{"location":"addons/pixie/#using-deploy-key-stored-in-secrets-manager","text":"import * as cdk from 'aws-cdk-lib'; import * as blueprints from '@aws-quickstart/eks-blueprints'; import { PixieAddOn } from '@pixie-labs/pixie-ssp-addon'; const app = new App(); const addOns: Array<blueprints.ClusterAddOn> = [ new blueprints.addons.SecretsStoreAddOn, new PixieAddOn({ deployKeySecretName: \"pixie-deploy-key-secret\", // Name of secret in Secrets Manager. }), ]; new blueprints.EksBlueprint( app, { id: 'my-stack-name', addOns, }, { env:{ account: <AWS_ACCOUNT_ID>, region: <AWS_REGION>, } });","title":"Using deploy key stored in Secrets Manager:"},{"location":"addons/pixie/#addon-options-props","text":"","title":"Addon Options (props)"},{"location":"addons/pixie/#deploykey-string-optional","text":"Pixie deployment key (plain text). Log into the Admin UI in Pixie to generate a deployment key. This attaches your Pixie deployment to your org.","title":"deployKey: string (optional)"},{"location":"addons/pixie/#deploykeysecretname-string-optional","text":"The name of the Pixie deployment key secret in Secrets Manager. The value of the key in Secrets Manager should be the deploy key in plaintext. Do not nest it inside a JSON object.","title":"deployKeySecretName: string (optional)"},{"location":"addons/pixie/#namespace-string-optional","text":"Namespace to deploy Pixie to. Default: pl","title":"namespace?: string (optional)"},{"location":"addons/pixie/#cloudaddr-string-optional","text":"The address of Pixie Cloud. This should only be modified if you have deployed your own self-hosted Pixie Cloud. By default, it will be set to Community Cloud for Pixie .","title":"cloudAddr?: string (optional)"},{"location":"addons/pixie/#devcloudnamespace-string-optional","text":"If running in a self-hosted cloud with no DNS configured, the namespace in which the self-hosted cloud is running.","title":"devCloudNamespace?: string (optional)"},{"location":"addons/pixie/#clustername-string-optional","text":"The name of cluster. If none is specified, a random name will be generated.","title":"clusterName?: string (optional)"},{"location":"addons/pixie/#useetcdoperator-boolean-optional","text":"Whether the metadata store should use etcd to store metadata, or use a persistent volume store. If not specified, the operator will deploy based on the cluster's storageClass configuration.","title":"useEtcdOperator?: boolean (optional)"},{"location":"addons/pixie/#pemmemorylimit-string-optional","text":"The memory limit applied to the PEMs (data collectors). Set to 2Gi by default.","title":"pemMemoryLimit?: string (optional)"},{"location":"addons/pixie/#dataaccess-fullrestrictedpiirestricted-optional","text":"DataAccess defines the level of data that may be accesssed when executing a script on the cluster. If none specified, assumes full data access.","title":"dataAccess?: \"Full\"|\"Restricted\"|\"PIIRestricted\" (optional)"},{"location":"addons/pixie/#patches-key-string-string-optional","text":"Custom K8s patches which should be applied to the Pixie YAMLs. The key should be the name of the K8s resource, and the value is the patch that should be applied.","title":"patches?: [key: string]: string (optional)"},{"location":"addons/pixie/#version-string-optional","text":"Helm chart version.","title":"version?: string (optional)"},{"location":"addons/pixie/#repository-string-release-string-chart-string-optional","text":"Additional options for customers who may need to supply their own private Helm repository.","title":"repository?: string, release?: string, chart?: string (optional)"},{"location":"addons/prometheus-node-exporter/","text":"Prometheus Node Exporter Add-on \u00b6 This add-on installs prometheus-node-exporter . prometheus-node-exporter Add-on enables you to measure various machine resources such as memory, disk and CPU utilization. Usage \u00b6 import * as cdk from 'aws-cdk-lib' ; import * as blueprints from '@aws-quickstart/eks-blueprints' ; const app = new cdk . App (); const addOn = new blueprints . addons . PrometheusNodeExporterAddOn () const blueprint = blueprints . EksBlueprint . builder () . addOns ( addOn ) . build ( app , 'my-stack-name' ); Configuration Options \u00b6 createNamespace : (boolean) If you want CDK to create the namespace for you values : Arbitrary values to pass to the chart. Standard helm configuration options . Validation \u00b6 To validate that prometheus-node-exporter is installed properly in the cluster, check if the prometheus-node-exporter namespace is created and pods are running. Verify if the pods are running correctly for prometheus-node-exporter in prometheus-node-exporter namespace. kubectl get pods -n prometheus-node-exporter Output \u00b6 There should list pods starting with name prometheus-node-exporter For Eg: NAME READY STATUS RESTARTS AGE prometheus-node-exporter-l7s25 1 /1 Running 0 105m prometheus-node-exporter-zh5sn 1 /1 Running 0 105m Functionality \u00b6 Applies the prometheus-node-exporter add-on to an Amazon EKS cluster.","title":"Prometheus Node Exporter"},{"location":"addons/prometheus-node-exporter/#prometheus-node-exporter-add-on","text":"This add-on installs prometheus-node-exporter . prometheus-node-exporter Add-on enables you to measure various machine resources such as memory, disk and CPU utilization.","title":"Prometheus Node Exporter Add-on"},{"location":"addons/prometheus-node-exporter/#usage","text":"import * as cdk from 'aws-cdk-lib' ; import * as blueprints from '@aws-quickstart/eks-blueprints' ; const app = new cdk . App (); const addOn = new blueprints . addons . PrometheusNodeExporterAddOn () const blueprint = blueprints . EksBlueprint . builder () . addOns ( addOn ) . build ( app , 'my-stack-name' );","title":"Usage"},{"location":"addons/prometheus-node-exporter/#configuration-options","text":"createNamespace : (boolean) If you want CDK to create the namespace for you values : Arbitrary values to pass to the chart. Standard helm configuration options .","title":"Configuration Options"},{"location":"addons/prometheus-node-exporter/#validation","text":"To validate that prometheus-node-exporter is installed properly in the cluster, check if the prometheus-node-exporter namespace is created and pods are running. Verify if the pods are running correctly for prometheus-node-exporter in prometheus-node-exporter namespace. kubectl get pods -n prometheus-node-exporter","title":"Validation"},{"location":"addons/prometheus-node-exporter/#output","text":"There should list pods starting with name prometheus-node-exporter For Eg: NAME READY STATUS RESTARTS AGE prometheus-node-exporter-l7s25 1 /1 Running 0 105m prometheus-node-exporter-zh5sn 1 /1 Running 0 105m","title":"Output"},{"location":"addons/prometheus-node-exporter/#functionality","text":"Applies the prometheus-node-exporter add-on to an Amazon EKS cluster.","title":"Functionality"},{"location":"addons/rafay/","text":"Rafay Addon - AWS EKS Blueprints for CDK \u00b6 EKS Blueprints for CDK is a framework that makes it easy for customers to configure and deploy Rafay Kubernetes Operator as part of an EKS Blueprints cluster on Amazon EKS . This Addon deploys Rafay\u2019s Kubernetes Operations Platform (KOP) for Amazon Elastic Kubernetes Service (Amazon EKS) management and operations. With KOP, your platform and site-reliability engineering (SRE) teams can deploy, operate, and manage the lifecycle of Kubernetes clusters and containerized applications in both AWS Cloud and on-premises environments. With the Rafay Kubernetes Operations Platform, enterprises use a single operations platform to manage the lifecycle of Amazon EKS clusters and containerized applications. You can speed up the deployment of new applications to production, reduce application downtimes, and reduce security and compliance risks associated with your infrastructure. Rafay automates the deployment of containerized applications and enables access to Kubernetes clusters through a zero-trust connectivity model. A unified dashboard provides enterprise-grade capabilities, such as monitoring across AWS Regions, role-based access control, and governance. Installation \u00b6 Using npm : npm install @rafaysystems/rafay-eks-blueprints-addon For a quick tutorial on EKS Blueprints, visit the Getting Started guide . Basic Usage \u00b6 import * as blueprints from '@aws-quickstart/eks-blueprints' ; import * as rafayAddOn from '@rafaysystems/rafay-eks-blueprints-addon' ; import { Construct } from \"constructs\" ; export default class RafayConstruct extends Construct { constructor ( scope : Construct , id : string ) { super ( scope , id ); const stackId = ` ${ id } -blueprint` ; let rafayConfig = { organizationName : \"rafay-eks-org-1\" , // replace with your organization Name email : \"abc@example.com\" , // replace with your email firstName : \"John\" , // replace with your first Name lastName : \"Doe\" , // replace with your last Name password : \"P@$$word\" , // replace with a password of your own clusterName : \"eks-cluster-1\" , // replace with the name that you want the cluster to be created in Rafay Console blueprintName : \"minimal\" } as rafayAddOn . RafayConfig const addOns : Array < blueprints . ClusterAddOn > = [ new rafayAddOn . RafayClusterAddOn ( rafayConfig ) ]; blueprints . EksBlueprint . builder () . account ( process . env . CDK_DEFAULT_ACCOUNT ! ) . region ( process . env . CDK_DEFAULT_REGION ) . addOns (... addOns ) . build ( scope , stackId ); } } Validation \u00b6 kubectl get po -n rafay-system NAME READY STATUS RESTARTS AGE controller-manager-v3-54b4945f7f-5kvwk 1/1 Running 0 4m49s edge-client-65995fbb78-hvp25 1/1 Running 0 6m rafay-connector-v3-5dc986d5d9-hb2t7 1/1 Running 0 4m49s relay-agent-6f555c4dbf-hrr4b 1/1 Running 0 6m Validation from Rafay Console Login to Rafay console with the credentials used in the blueprint Navigate to Clusters and click on \"cluster-name\"","title":"Rafay"},{"location":"addons/rafay/#rafay-addon-aws-eks-blueprints-for-cdk","text":"EKS Blueprints for CDK is a framework that makes it easy for customers to configure and deploy Rafay Kubernetes Operator as part of an EKS Blueprints cluster on Amazon EKS . This Addon deploys Rafay\u2019s Kubernetes Operations Platform (KOP) for Amazon Elastic Kubernetes Service (Amazon EKS) management and operations. With KOP, your platform and site-reliability engineering (SRE) teams can deploy, operate, and manage the lifecycle of Kubernetes clusters and containerized applications in both AWS Cloud and on-premises environments. With the Rafay Kubernetes Operations Platform, enterprises use a single operations platform to manage the lifecycle of Amazon EKS clusters and containerized applications. You can speed up the deployment of new applications to production, reduce application downtimes, and reduce security and compliance risks associated with your infrastructure. Rafay automates the deployment of containerized applications and enables access to Kubernetes clusters through a zero-trust connectivity model. A unified dashboard provides enterprise-grade capabilities, such as monitoring across AWS Regions, role-based access control, and governance.","title":"Rafay Addon - AWS EKS Blueprints for CDK"},{"location":"addons/rafay/#installation","text":"Using npm : npm install @rafaysystems/rafay-eks-blueprints-addon For a quick tutorial on EKS Blueprints, visit the Getting Started guide .","title":"Installation"},{"location":"addons/rafay/#basic-usage","text":"import * as blueprints from '@aws-quickstart/eks-blueprints' ; import * as rafayAddOn from '@rafaysystems/rafay-eks-blueprints-addon' ; import { Construct } from \"constructs\" ; export default class RafayConstruct extends Construct { constructor ( scope : Construct , id : string ) { super ( scope , id ); const stackId = ` ${ id } -blueprint` ; let rafayConfig = { organizationName : \"rafay-eks-org-1\" , // replace with your organization Name email : \"abc@example.com\" , // replace with your email firstName : \"John\" , // replace with your first Name lastName : \"Doe\" , // replace with your last Name password : \"P@$$word\" , // replace with a password of your own clusterName : \"eks-cluster-1\" , // replace with the name that you want the cluster to be created in Rafay Console blueprintName : \"minimal\" } as rafayAddOn . RafayConfig const addOns : Array < blueprints . ClusterAddOn > = [ new rafayAddOn . RafayClusterAddOn ( rafayConfig ) ]; blueprints . EksBlueprint . builder () . account ( process . env . CDK_DEFAULT_ACCOUNT ! ) . region ( process . env . CDK_DEFAULT_REGION ) . addOns (... addOns ) . build ( scope , stackId ); } }","title":"Basic Usage"},{"location":"addons/rafay/#validation","text":"kubectl get po -n rafay-system NAME READY STATUS RESTARTS AGE controller-manager-v3-54b4945f7f-5kvwk 1/1 Running 0 4m49s edge-client-65995fbb78-hvp25 1/1 Running 0 6m rafay-connector-v3-5dc986d5d9-hb2t7 1/1 Running 0 4m49s relay-agent-6f555c4dbf-hrr4b 1/1 Running 0 6m Validation from Rafay Console Login to Rafay console with the credentials used in the blueprint Navigate to Clusters and click on \"cluster-name\"","title":"Validation"},{"location":"addons/secrets-store/","text":"Secrets Store Add-on \u00b6 The Secrets Store Add-on provisions the AWS Secrets Manager and Config Provider(ASCP) for Secret Store CSI Driver on your EKS cluster. With ASCP, you now have a plugin for the industry-standard Kubernetes Secrets Store Container Storage Interface (CSI) Driver used for providing secrets to applications operating on Amazon Elastic Kubernetes Service. With ASCP, you can securely store and manage your secrets in AWS Secrets Manager or AWS Systems Manager Parameter Store and retrieve them through your application workloads running on Kubernetes. You no longer have to write custom code for your applications. Usage \u00b6 index.ts \u00b6 import 'source-map-support/register' ; import * as cdk from 'aws-cdk-lib' ; import * as blueprints from '@aws-quickstart/eks-blueprints' ; const app = new cdk . App (); const addOn = new blueprints . addons . SecretsStoreAddOn (); /* Setup application team with secrets * Here we are generating a new SecretManager secret for AuthPassword * We are also looking up a pre-existing secret in Parameter Store called GITHUB_TOKEN */ export class TeamBurnham extends ApplicationTeam { constructor ( scope : Construct ) { super ({ name : \"burnham\" , users : getUserArns ( scope , \"team-burnham.users\" ), teamSecrets : [ { secretProvider : new blueprints . GenerateSecretManagerProvider ( 'AuthPassword' ), kubernetesSecret : { secretName : 'auth-password' , data : [ { key : 'password' } ] } }, { secretProvider : new blueprints . LookupSsmSecretByAttrs ( 'GITHUB_TOKEN' , 1 ), kubernetesSecret : { secretName : 'github' } } ] }); } } const blueprint = blueprints . EksBlueprint . builder () . addOns ( addOn ) . teams ( new TeamBurnham ( app )) . build ( app , 'my-stack-name' ); Functionality \u00b6 Installs the Kubernetes Secrets Store CSI Driver in the kube-system namespace. Installs AWS Secrets Manager and Config Provider for Secret Store CSI Driver in the kube-system namespace. Creates an IAM access policy for scoped down to just the secrets the provided namespace should have access to. Updates IAM roles for service accounts [team-name]-sa for policies to grant read access to the provided secrets. Creates a SecretProviderClass [team-name]-aws-secrets which tells the AWS provider which secrets can be mounted in an application pod in the provided namespace. Security Considerations \u00b6 The AWS Secrets Manger and Config Provider provides compatibility for legacy applications that access secrets as mounted files in the pod. Security conscious applications should use the native AWS APIs to fetch secrets and optionally cache them in memory rather than storing them in the file system. Example \u00b6 After the Blueprint stack is deployed you can test consuming the secret from within a deployment . This sample deployment shows how to consume the secrets as mounted volumes as well as environment variables. cat << EOF >> test-secrets.yaml apiVersion: apps/v1 kind: Deployment metadata: name: app-deployment labels: app: myapp namespace: team-burnham spec: replicas: 1 selector: matchLabels: app: myapp template: metadata: labels: app: myapp spec: serviceAccountName: burnham-sa volumes: - name: secrets-store-inline csi: driver: secrets-store.csi.k8s.io readOnly: true volumeAttributes: secretProviderClass: burnham-aws-secrets containers: - name: test-secrets image: public.ecr.aws/ubuntu/ubuntu:latest command: [ \"/bin/bash\", \"-c\", \"--\" ] args: [ \"while true; do sleep 30; done;\" ] resources: limits: cpu: \"100m\" memory: \"128Mi\" requests: cpu: \"100m\" memory: \"128Mi\" env: - name: PASSWORD valueFrom: secretKeyRef: name: auth-password key: password - name: GITHUB_TOKEN valueFrom: secretKeyRef: name: github key: GITHUB_TOKEN volumeMounts: - name: secrets-store-inline mountPath: /mnt/secrets-store readOnly: true EOF The values for serviceAccountName and the secretProviderClass shown in the example above are obtained from CloudFormation outputs of the blueprint stack shown in the screenshot below as burnhamsa and teamburnhamsecretproviderclass . Apply the manifest. $ kubectl apply -f test-secrets.yaml deployment.apps/app-deployment created Test that kubernetes secret burnham-github-secrets was created. kubectl get secrets -n team-burnham NAME TYPE DATA AGE auth-password Opaque 1 19s burnham-sa-token-fqjqw kubernetes.io/service-account-token 3 64m default-token-7fn69 kubernetes.io/service-account-token 3 64m github Opaque 1 19s Test that the deployment has completed and the pod is running successfully. $ kubectl get pods -n team-burnham NAME READY STATUS RESTARTS AGE app-deployment-6867fc6bd6-jzdwh 1 /1 Running 0 46s Next, test whether the secret PASSWORD is available as an environment variable from within the app-deployment pod. $ kubectl exec app-deployment-6867fc6bd6-jzdwh -n team-burnham -- echo $PASSWORD XXXXXXXXXXXXXXXXXX Test whether GITHUB_TOKEN is available as an environment variable from within the app-deployment pod. $ kubectl exec app-deployment-6867fc6bd6-jzdwh -n team-burnham -- echo $GITHUB_TOKEN ghp_XXXXXXXXXXXXXXXXXXXXXXXXXX","title":"Secrets Store"},{"location":"addons/secrets-store/#secrets-store-add-on","text":"The Secrets Store Add-on provisions the AWS Secrets Manager and Config Provider(ASCP) for Secret Store CSI Driver on your EKS cluster. With ASCP, you now have a plugin for the industry-standard Kubernetes Secrets Store Container Storage Interface (CSI) Driver used for providing secrets to applications operating on Amazon Elastic Kubernetes Service. With ASCP, you can securely store and manage your secrets in AWS Secrets Manager or AWS Systems Manager Parameter Store and retrieve them through your application workloads running on Kubernetes. You no longer have to write custom code for your applications.","title":"Secrets Store Add-on"},{"location":"addons/secrets-store/#usage","text":"","title":"Usage"},{"location":"addons/secrets-store/#indexts","text":"import 'source-map-support/register' ; import * as cdk from 'aws-cdk-lib' ; import * as blueprints from '@aws-quickstart/eks-blueprints' ; const app = new cdk . App (); const addOn = new blueprints . addons . SecretsStoreAddOn (); /* Setup application team with secrets * Here we are generating a new SecretManager secret for AuthPassword * We are also looking up a pre-existing secret in Parameter Store called GITHUB_TOKEN */ export class TeamBurnham extends ApplicationTeam { constructor ( scope : Construct ) { super ({ name : \"burnham\" , users : getUserArns ( scope , \"team-burnham.users\" ), teamSecrets : [ { secretProvider : new blueprints . GenerateSecretManagerProvider ( 'AuthPassword' ), kubernetesSecret : { secretName : 'auth-password' , data : [ { key : 'password' } ] } }, { secretProvider : new blueprints . LookupSsmSecretByAttrs ( 'GITHUB_TOKEN' , 1 ), kubernetesSecret : { secretName : 'github' } } ] }); } } const blueprint = blueprints . EksBlueprint . builder () . addOns ( addOn ) . teams ( new TeamBurnham ( app )) . build ( app , 'my-stack-name' );","title":"index.ts"},{"location":"addons/secrets-store/#functionality","text":"Installs the Kubernetes Secrets Store CSI Driver in the kube-system namespace. Installs AWS Secrets Manager and Config Provider for Secret Store CSI Driver in the kube-system namespace. Creates an IAM access policy for scoped down to just the secrets the provided namespace should have access to. Updates IAM roles for service accounts [team-name]-sa for policies to grant read access to the provided secrets. Creates a SecretProviderClass [team-name]-aws-secrets which tells the AWS provider which secrets can be mounted in an application pod in the provided namespace.","title":"Functionality"},{"location":"addons/secrets-store/#security-considerations","text":"The AWS Secrets Manger and Config Provider provides compatibility for legacy applications that access secrets as mounted files in the pod. Security conscious applications should use the native AWS APIs to fetch secrets and optionally cache them in memory rather than storing them in the file system.","title":"Security Considerations"},{"location":"addons/secrets-store/#example","text":"After the Blueprint stack is deployed you can test consuming the secret from within a deployment . This sample deployment shows how to consume the secrets as mounted volumes as well as environment variables. cat << EOF >> test-secrets.yaml apiVersion: apps/v1 kind: Deployment metadata: name: app-deployment labels: app: myapp namespace: team-burnham spec: replicas: 1 selector: matchLabels: app: myapp template: metadata: labels: app: myapp spec: serviceAccountName: burnham-sa volumes: - name: secrets-store-inline csi: driver: secrets-store.csi.k8s.io readOnly: true volumeAttributes: secretProviderClass: burnham-aws-secrets containers: - name: test-secrets image: public.ecr.aws/ubuntu/ubuntu:latest command: [ \"/bin/bash\", \"-c\", \"--\" ] args: [ \"while true; do sleep 30; done;\" ] resources: limits: cpu: \"100m\" memory: \"128Mi\" requests: cpu: \"100m\" memory: \"128Mi\" env: - name: PASSWORD valueFrom: secretKeyRef: name: auth-password key: password - name: GITHUB_TOKEN valueFrom: secretKeyRef: name: github key: GITHUB_TOKEN volumeMounts: - name: secrets-store-inline mountPath: /mnt/secrets-store readOnly: true EOF The values for serviceAccountName and the secretProviderClass shown in the example above are obtained from CloudFormation outputs of the blueprint stack shown in the screenshot below as burnhamsa and teamburnhamsecretproviderclass . Apply the manifest. $ kubectl apply -f test-secrets.yaml deployment.apps/app-deployment created Test that kubernetes secret burnham-github-secrets was created. kubectl get secrets -n team-burnham NAME TYPE DATA AGE auth-password Opaque 1 19s burnham-sa-token-fqjqw kubernetes.io/service-account-token 3 64m default-token-7fn69 kubernetes.io/service-account-token 3 64m github Opaque 1 19s Test that the deployment has completed and the pod is running successfully. $ kubectl get pods -n team-burnham NAME READY STATUS RESTARTS AGE app-deployment-6867fc6bd6-jzdwh 1 /1 Running 0 46s Next, test whether the secret PASSWORD is available as an environment variable from within the app-deployment pod. $ kubectl exec app-deployment-6867fc6bd6-jzdwh -n team-burnham -- echo $PASSWORD XXXXXXXXXXXXXXXXXX Test whether GITHUB_TOKEN is available as an environment variable from within the app-deployment pod. $ kubectl exec app-deployment-6867fc6bd6-jzdwh -n team-burnham -- echo $GITHUB_TOKEN ghp_XXXXXXXXXXXXXXXXXXXXXXXXXX","title":"Example"},{"location":"addons/ssm-agent/","text":"SSM Agent Add-on \u00b6 This add-on uses the Kubernetes DaemonSet resource type to install AWS Systems Manager Agent (SSM Agent) on all worker nodes, instead of installing it manually or replacing the Amazon Machine Image (AMI) for the nodes. DaemonSet uses a CronJob on the worker node to schedule the installation of SSM Agent. A common use-case for installing SSM Agent on the worker nodes is to be able open a terminal session on an instance without the need to create a bastion instance and without having to install SSH keys on the worker nodes. The AWS Identity and Access Management (IAM) managed role AmazonSSMManagedInstanceCore provides the required permissions for SSM Agent to run on EC2 instances. This role is automatically attached to the instances when this add-on is enabled. Limitations This add-on isn't applicable to AWS Fargate, because DaemonSets aren't supported on the Fargate platform. This add-on applies only to Linux-based worker nodes. The DaemonSet pods run in privileged mode. If the Amazon EKS cluster has a webhook that blocks pods in privileged mode, the SSM Agent will not be installed. Only latest version of SSM Agent add-on can be installed. Usage \u00b6 import 'source-map-support/register' ; import * as cdk from 'aws-cdk-lib' ; import * as blueprints from '@aws-quickstart/eks-blueprints' ; const app = new cdk . App (); const addOn = new blueprints . addons . SSMAgentAddOn (); const blueprint = blueprints . EksBlueprint . builder () . addOns ( addOn ) . build ( app , 'my-stack-name' ); To validate that SSM Agent is running on worker node instance: Pre-Requisite : Install the Session Manager plugin for the AWS CLI as per instructions for your OS. Get the EC2 Instance Id of a worker node instance_id = $( kubectl get nodes -o custom-columns = NAME:.metadata.name,INSTANCEID:.spec.providerID | awk -F/ 'FNR == 2 {print $5}' ) Use the start-session api to see if you can open a terminal into the instance aws ssm start-session --target $instance_id Use Case: Private Clusters \u00b6 If you are disabling public access for your EKS cluster endpoint such that the cluster endpoint is provisioned as private only i.e endpointPublicAccess=false and endpointPrivateAccess=true , then you can use one of the worker nodes as a TCP jump box to your EKS cluster api. To set up a TCP tunnel with your worker node as a jump box: Use SSM send-command api to create a TCP tunnel to Cluster API using socat : # Get the Cluster API endpoint first CLUSTER_NAME = <insert your cluster name, e.g. blueprint-construct-dev> CLUSTER_API = $( aws eks describe-cluster --name $CLUSTER_NAME | jq -r '.cluster.endpoint' | awk -F/ '{print $3}' ) aws ssm send-command \\ --instance-ids $instance_id \\ --document-name \"AWS-RunShellScript\" \\ --comment \"tcp tunnel to cluster api\" \\ --parameters commands = \"nohup sudo socat TCP-LISTEN:443\\,fork TCP: $CLUSTER_API :443 &\" Update ~/.kube/config to use port 8443 instead of 443 as your local host may not allow you to bind port 443 (depending on your machine network configuration you may not be able to bind to port 443. In such a case, you can bind to port 8443) sed -i -e \"s/https:\\/\\/ $CLUSTER_API /https:\\/\\/ $CLUSTER_API :8443/\" ~/.kube/config Update /etc/hosts so that $CLUSTER_API resolves to 127.0.0.1 . sudo echo \"127.0.0.1 $CLUSTER_API \" >> /etc/hosts 4. Start an SSM session to forward remote port 443 to local port 8443: aws ssm start-session \\ --target $instance_id \\ --document-name AWS-StartPortForwardingSession \\ --parameters '{\"portNumber\":[\"443\"], \"localPortNumber\":[\"8443\"]}' At this point you should be able to execute kubectl ... commands against your cluster API from another terminal window. Limitations This approach cannot be used for Fargate or BottleRocket based providers. socat is available on EKS optimized AMI out of the box but may have to be explicitly installed on others AMIs.","title":"SSM Agent"},{"location":"addons/ssm-agent/#ssm-agent-add-on","text":"This add-on uses the Kubernetes DaemonSet resource type to install AWS Systems Manager Agent (SSM Agent) on all worker nodes, instead of installing it manually or replacing the Amazon Machine Image (AMI) for the nodes. DaemonSet uses a CronJob on the worker node to schedule the installation of SSM Agent. A common use-case for installing SSM Agent on the worker nodes is to be able open a terminal session on an instance without the need to create a bastion instance and without having to install SSH keys on the worker nodes. The AWS Identity and Access Management (IAM) managed role AmazonSSMManagedInstanceCore provides the required permissions for SSM Agent to run on EC2 instances. This role is automatically attached to the instances when this add-on is enabled. Limitations This add-on isn't applicable to AWS Fargate, because DaemonSets aren't supported on the Fargate platform. This add-on applies only to Linux-based worker nodes. The DaemonSet pods run in privileged mode. If the Amazon EKS cluster has a webhook that blocks pods in privileged mode, the SSM Agent will not be installed. Only latest version of SSM Agent add-on can be installed.","title":"SSM Agent Add-on"},{"location":"addons/ssm-agent/#usage","text":"import 'source-map-support/register' ; import * as cdk from 'aws-cdk-lib' ; import * as blueprints from '@aws-quickstart/eks-blueprints' ; const app = new cdk . App (); const addOn = new blueprints . addons . SSMAgentAddOn (); const blueprint = blueprints . EksBlueprint . builder () . addOns ( addOn ) . build ( app , 'my-stack-name' ); To validate that SSM Agent is running on worker node instance: Pre-Requisite : Install the Session Manager plugin for the AWS CLI as per instructions for your OS. Get the EC2 Instance Id of a worker node instance_id = $( kubectl get nodes -o custom-columns = NAME:.metadata.name,INSTANCEID:.spec.providerID | awk -F/ 'FNR == 2 {print $5}' ) Use the start-session api to see if you can open a terminal into the instance aws ssm start-session --target $instance_id","title":"Usage"},{"location":"addons/ssm-agent/#use-case-private-clusters","text":"If you are disabling public access for your EKS cluster endpoint such that the cluster endpoint is provisioned as private only i.e endpointPublicAccess=false and endpointPrivateAccess=true , then you can use one of the worker nodes as a TCP jump box to your EKS cluster api. To set up a TCP tunnel with your worker node as a jump box: Use SSM send-command api to create a TCP tunnel to Cluster API using socat : # Get the Cluster API endpoint first CLUSTER_NAME = <insert your cluster name, e.g. blueprint-construct-dev> CLUSTER_API = $( aws eks describe-cluster --name $CLUSTER_NAME | jq -r '.cluster.endpoint' | awk -F/ '{print $3}' ) aws ssm send-command \\ --instance-ids $instance_id \\ --document-name \"AWS-RunShellScript\" \\ --comment \"tcp tunnel to cluster api\" \\ --parameters commands = \"nohup sudo socat TCP-LISTEN:443\\,fork TCP: $CLUSTER_API :443 &\" Update ~/.kube/config to use port 8443 instead of 443 as your local host may not allow you to bind port 443 (depending on your machine network configuration you may not be able to bind to port 443. In such a case, you can bind to port 8443) sed -i -e \"s/https:\\/\\/ $CLUSTER_API /https:\\/\\/ $CLUSTER_API :8443/\" ~/.kube/config Update /etc/hosts so that $CLUSTER_API resolves to 127.0.0.1 . sudo echo \"127.0.0.1 $CLUSTER_API \" >> /etc/hosts 4. Start an SSM session to forward remote port 443 to local port 8443: aws ssm start-session \\ --target $instance_id \\ --document-name AWS-StartPortForwardingSession \\ --parameters '{\"portNumber\":[\"443\"], \"localPortNumber\":[\"8443\"]}' At this point you should be able to execute kubectl ... commands against your cluster API from another terminal window. Limitations This approach cannot be used for Fargate or BottleRocket based providers. socat is available on EKS optimized AMI out of the box but may have to be explicitly installed on others AMIs.","title":"Use Case: Private Clusters"},{"location":"addons/upbound-universal-crossplane/","text":"Upbound Universal Crossplane (UXP) Amazon EKS Add-on \u00b6 The Upbound Universal Crossplane Amazon EKS Add-on allows Amazon Elastic Kubernetes Service (Amazon EKS) clusters to manage the lifecycle of Crossplane distribution. Upbound Universal Crossplane (UXP) is Upbound's official enterprise-grade Crossplane distribution. It's free, open source, and fully conformant with upstream Crossplane. UXP is hardened and tested by Upbound so customers can confidently deploy control plane architectures to production. Connect UXP to Upbound Cloud is enabled with a free Upbound account for simplified management. For more information on the driver, please review the aws marketplace and please make sure to subscribe this product in marketplace before using this addon. Prerequisites \u00b6 Subscription to Upbound Universal Crossplane (UXP) in AWS Marketplace. Usage \u00b6 import * as cdk from 'aws-cdk-lib' ; import * as blueprints from '@aws-quickstart/eks-blueprints' ; const app = new cdk . App (); const addOn = new blueprints . addons . UpboundUniversalCrossplaneAddOn (), const blueprint = blueprints . EksBlueprint . builder () . addOns ( addOn ) . build ( app , 'my-stack-name' ); Configuration Options \u00b6 version : Version of the Upbound Universal Crossplane add-on to be installed. # Command to show versions of the UXP add-on aws eks describe-addon-versions --addon-name upbound_universal-crossplane \\ --kubernetes-version 1 .24 \\ --query \"addons[].addonVersions[].[addonVersion, compatibilities[].defaultVersion]\" \\ --output text # Output v1.9.1-eksbuild.0 Validation \u00b6 To validate that EBS CSI Driver add-on is installed properly, ensure that the ebs pods are running in the cluster kubectl get all -n upbound-syste # Output NAME READY STATUS RESTARTS AGE pod/crossplane-776449cbc7-t9jnn 1 /1 Running 0 25m pod/upbound-bootstrapper-844f84fcf4-xgpj9 1 /1 Running 0 25m pod/xgql-55d7475b48-dlfgc 1 /1 Running 2 ( 25m ago ) 25m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE service/upbound-agent ClusterIP 172 .20.131.84 <none> 6443 /TCP 25m service/xgql ClusterIP 172 .20.138.213 <none> 443 /TCP 25m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/crossplane 1 /1 1 1 25m deployment.apps/upbound-bootstrapper 1 /1 1 1 25m deployment.apps/xgql 1 /1 1 1 25m NAME DESIRED CURRENT READY AGE replicaset.apps/crossplane-776449cbc7 1 1 1 25m replicaset.apps/upbound-bootstrapper-844f84fcf4 1 1 1 25m replicaset.apps/xgql-55d7475b48 1 1 1 25m Additionally, the aws cli can be used to determine which version of the UXP add-on is installed in the cluster # Assuming cluster-name is my-cluster, below command shows the version of coredns installed. Check if it is same as the version installed via EKS add-on aws eks describe-addon \\ --cluster-name my-cluster \\ --addon-name upbound_universal-crossplane \\ --query \"addon.addonVersion\" \\ --output text # Output v1.9.1-eksbuild.0 Functionality \u00b6 Applies the Upbound Universal Crossplane add-on to an Amazon EKS cluster.","title":"Upbound Universal Crossplane"},{"location":"addons/upbound-universal-crossplane/#upbound-universal-crossplane-uxp-amazon-eks-add-on","text":"The Upbound Universal Crossplane Amazon EKS Add-on allows Amazon Elastic Kubernetes Service (Amazon EKS) clusters to manage the lifecycle of Crossplane distribution. Upbound Universal Crossplane (UXP) is Upbound's official enterprise-grade Crossplane distribution. It's free, open source, and fully conformant with upstream Crossplane. UXP is hardened and tested by Upbound so customers can confidently deploy control plane architectures to production. Connect UXP to Upbound Cloud is enabled with a free Upbound account for simplified management. For more information on the driver, please review the aws marketplace and please make sure to subscribe this product in marketplace before using this addon.","title":"Upbound Universal Crossplane (UXP) Amazon EKS Add-on"},{"location":"addons/upbound-universal-crossplane/#prerequisites","text":"Subscription to Upbound Universal Crossplane (UXP) in AWS Marketplace.","title":"Prerequisites"},{"location":"addons/upbound-universal-crossplane/#usage","text":"import * as cdk from 'aws-cdk-lib' ; import * as blueprints from '@aws-quickstart/eks-blueprints' ; const app = new cdk . App (); const addOn = new blueprints . addons . UpboundUniversalCrossplaneAddOn (), const blueprint = blueprints . EksBlueprint . builder () . addOns ( addOn ) . build ( app , 'my-stack-name' );","title":"Usage"},{"location":"addons/upbound-universal-crossplane/#configuration-options","text":"version : Version of the Upbound Universal Crossplane add-on to be installed. # Command to show versions of the UXP add-on aws eks describe-addon-versions --addon-name upbound_universal-crossplane \\ --kubernetes-version 1 .24 \\ --query \"addons[].addonVersions[].[addonVersion, compatibilities[].defaultVersion]\" \\ --output text # Output v1.9.1-eksbuild.0","title":"Configuration Options"},{"location":"addons/upbound-universal-crossplane/#validation","text":"To validate that EBS CSI Driver add-on is installed properly, ensure that the ebs pods are running in the cluster kubectl get all -n upbound-syste # Output NAME READY STATUS RESTARTS AGE pod/crossplane-776449cbc7-t9jnn 1 /1 Running 0 25m pod/upbound-bootstrapper-844f84fcf4-xgpj9 1 /1 Running 0 25m pod/xgql-55d7475b48-dlfgc 1 /1 Running 2 ( 25m ago ) 25m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE service/upbound-agent ClusterIP 172 .20.131.84 <none> 6443 /TCP 25m service/xgql ClusterIP 172 .20.138.213 <none> 443 /TCP 25m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/crossplane 1 /1 1 1 25m deployment.apps/upbound-bootstrapper 1 /1 1 1 25m deployment.apps/xgql 1 /1 1 1 25m NAME DESIRED CURRENT READY AGE replicaset.apps/crossplane-776449cbc7 1 1 1 25m replicaset.apps/upbound-bootstrapper-844f84fcf4 1 1 1 25m replicaset.apps/xgql-55d7475b48 1 1 1 25m Additionally, the aws cli can be used to determine which version of the UXP add-on is installed in the cluster # Assuming cluster-name is my-cluster, below command shows the version of coredns installed. Check if it is same as the version installed via EKS add-on aws eks describe-addon \\ --cluster-name my-cluster \\ --addon-name upbound_universal-crossplane \\ --query \"addon.addonVersion\" \\ --output text # Output v1.9.1-eksbuild.0","title":"Validation"},{"location":"addons/upbound-universal-crossplane/#functionality","text":"Applies the Upbound Universal Crossplane add-on to an Amazon EKS cluster.","title":"Functionality"},{"location":"addons/velero/","text":"Velero Add-On \u00b6 The Velero(formerly Heptio Ark) is a tool to backup and restore your Kubernetes cluster resources and persistent volumes. Velero lets you : Take backups of your cluster and restore in case of loss. Migrate cluster resources to other clusters. Replicate your production cluster to development and testing clusters. Velero consists of: A server that runs on your cluster A command-line client that runs locally The Velero add-on installs Velero on Amazon EKS. By default it will create a private encrypted S3 Bucket to be the Velero backup destination. It leverages IAM Role for Service Accounts (IRSA) feature to enable Velero pod to make API calls with S3 and EC2 natively without the need to use kube2iam or AWS credentials. Usage \u00b6 import 'source-map-support/register' ; import * as cdk from 'aws-cdk-lib' ; import * as blueprints from '@aws-quickstart/eks-blueprints' ; const app = new cdk . App (); const addOn = new blueprints . addons . VeleroAddOn (); const blueprint = blueprints . EksBlueprint . builder () . addOns ( addOn ) . build ( app , 'my-stack-name' ); Functionality \u00b6 By default create a private S3 bucket (blocking all public access) with SSE-KMS encryption with AWS Managed Key from KMS(Encryption At Rest) for the Velero Backup destination. Configure S3 Bucket policy to enable encryption in transit. Create the IAM Role for Service Account for Velero pod to make API calls to AWS S3 and EC2 to backup and restore. Preset Velero Helm Chart Values . Allow users to pass Velero Helm Chart Values for customization purposes. Supports standard helm configuration options . Limitations \u00b6 Velero has a known bug for support of S3 with SSE-KMS encryption with Customer master key (CMK). Please refer to Velero GitHub Issue #83 . As a result of #1, Velero is unable to leverage the S3 Bucket Key feature which requires using AWS CMK to achieve \"reduce AWS KMS request costs by up to 99 percent by decreasing the request traffic from Amazon S3 to AWS KMS.\" Testing Velero Functionality \u00b6 The following steps will help test the backup and restore Kuberenetes resources from Velero Deploy a sample app as deployment into a newly created namespace Backup the sample app from the namespace Delete the sample app namespace Restore the sample app Deploy a sample app into a newly created namespace \u00b6 Properly configure your kubeconfig to use kubectl command. By successfully deploying the EKS cluster, you can run commands to set up your kubeconfig correctly by: aws eks update-kubeconfig --name <EKS_Cluster_Name> --region <AWS_Region> --role-arn arn:aws:iam::<AWS_ACCOUNT_ID>:role/<IRSA_Role_Name> # Create the test01 namespace $ kubectl create ns test01 namespace/test01 created # Deploy the Nginx Stateless applications on to namespace test01 $ kubectl apply -f https://k8s.io/examples/application/deployment.yaml -n test01 deployment.apps/nginx-deployment created # Check the nginx pods $ kubectl get pods -n test01 NAME READY STATUS RESTARTS AGE nginx-deployment-66b6c48dd5-qf7lc 1 /1 Running 0 53s nginx-deployment-66b6c48dd5-wvxjx 1 /1 Running 0 53s # Deploy the Stateful Nginx Application with PV to namespace nginx-example $ kubectl apply -f https://raw.githubusercontent.com/vmware-tanzu/velero/main/examples/nginx-app/with-pv.yaml namespace/nginx-example created persistentvolumeclaim/nginx-logs created deployment.apps/nginx-deployment created service/my-nginx created # Check the application and PV $ kubectl get pods -n nginx-example NAME READY STATUS RESTARTS AGE nginx-deployment-66689547d-4mqsd 2 /2 Running 0 106s haofeif@a483e70791e6 ~ $ kubectl get pv -n nginx-example NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-73498192-6571-4e31-b455-e7e7efbf2fb7 1Gi RWO Delete Bound nginx-example/nginx-logs gp2 110s ## EBS Volume got created Backup the sample app from the namespace \u00b6 To install Velero client Cli, please refer to the User Guide The backup will be created into the S3 bucket created or specified by the users. # Create the backup for stateless app at namespace test01 $ velero backup create test01 --include-namespaces test01 Backup request \"test01\" submitted successfully. Run ` velero backup describe test01 ` or ` velero backup logs test01 ` for more details. # Check for the backup $ velero backup get NAME STATUS ERRORS WARNINGS CREATED EXPIRES STORAGE LOCATION SELECTOR test01 Completed 0 0 2021 -09-20 12 :40:37 +1000 AEST 29d default <none> # Check the logs of the backup $ velero backup logs test01 time = \"2021-09-19T02:40:37Z\" level = info msg = \"Setting up backup temp file\" backup = velero/test01 logSource = \"pkg/controller/backup_controller.go:556\" time = \"2021-09-19T02:40:37Z\" level = info msg = \"Setting up plugin manager\" backup = velero/test01 logSource = \"pkg/controller/backup_controller.go:563\" time = \"2021-09-19T02:40:37Z\" level = info msg = \"Getting backup item actions\" backup = velero/test01 logSource = \"pkg/controller/backup_controller.go:567\" ... # Check the backup location, the Access mode shows the S3 bucket name and its folders. $ velero backup-location get NAME PROVIDER BUCKET/PREFIX PHASE LAST VALIDATED ACCESS MODE DEFAULT default aws my-stack-name-mystacknamevelerobackupxxx/velero/my-stack-name Available 2021 -09-20 12 :29:35 +1000 AEST ReadWrite true # Screenshot of the S3 bucket folder for the backup test01 # Create the Backup with the PV at namespace nginx-example $ velero backup create nginx-backup --include-namespaces nginx-example Backup request \"nginx-backup\" submitted successfully. Run ` velero backup describe nginx-backup ` or ` velero backup logs nginx-backup ` for more details. ## Check the backup status $ velero backup get NAME STATUS ERRORS WARNINGS CREATED EXPIRES STORAGE LOCATION SELECTOR nginx-backup Completed 0 0 2021 -09-20 12 :37:36 +1000 AEST 29d default <none> # Screenshot of the S3 bucket folder for the backup nginx-backup (with PV) Delete the sample app namespace \u00b6 # Delete the namespace test01 $ kubectl delete ns test01 # Delete the namespace nginx-example $ kubectl delete ns nginx-example # Note: Because the default reclaim policy for dynamically-provisioned PVs is \u201cDelete\u201d, these commands should trigger AWS to delete the EBS Volume that backs the PV. Deletion is asynchronous, so this may take some time. Restore the sample app \u00b6 # Restore from the backup of test01 $ velero restore create test01 --from-backup test01 Restore request \"test01\" submitted successfully. Run ` velero restore describe test01 ` or ` velero restore logs test01 ` for more details. # Check the restore status of test01 $ velero restore get NAME BACKUP STATUS STARTED COMPLETED ERRORS WARNINGS CREATED SELECTOR test01 test01 Completed 2021 -09-20 12 :41:38 +1000 AEST <nil> 0 0 2021 -09-20 12 :41:36 +1000 AEST <none> # Check the stateless application restore completed $ kubectl get pods -n test01 NAME READY STATUS RESTARTS AGE nginx-deployment-66b6c48dd5-qf7lc 1 /1 Running 0 53s nginx-deployment-66b6c48dd5-wvxjx 1 /1 Running 0 53s # Restore from the backup of nginx-backup (With PV) after confirming EBS volume has been deleted successfully $ velero restore create --from-backup nginx-backup # Check the Restore status $ velero restore get NAME BACKUP STATUS STARTED COMPLETED ERRORS WARNINGS CREATED SELECTOR nginx-backup-20210920124336 nginx-backup Completed 2021 -09-20 12 :43:42 +1000 AEST 2021 -09-20 12 :43:45 +1000 AEST 0 0 2021 -09-20 12 :43:40 +1000 AEST <none> # Check the status of pod and PV in namespace nginx-example $ kubectl get pods -n nginx-example NAME READY STATUS RESTARTS AGE nginx-deployment-66689547d-4mqsd 2 /2 Running 0 2m12s $ kubectl get pv -n nginx-example NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-73498192-6571-4e31-b455-e7e7efbf2fb7 1Gi RWO Delete Bound nginx-example/nginx-logs gp2 2m22s # EBS Volume is back","title":"Velero"},{"location":"addons/velero/#velero-add-on","text":"The Velero(formerly Heptio Ark) is a tool to backup and restore your Kubernetes cluster resources and persistent volumes. Velero lets you : Take backups of your cluster and restore in case of loss. Migrate cluster resources to other clusters. Replicate your production cluster to development and testing clusters. Velero consists of: A server that runs on your cluster A command-line client that runs locally The Velero add-on installs Velero on Amazon EKS. By default it will create a private encrypted S3 Bucket to be the Velero backup destination. It leverages IAM Role for Service Accounts (IRSA) feature to enable Velero pod to make API calls with S3 and EC2 natively without the need to use kube2iam or AWS credentials.","title":"Velero Add-On"},{"location":"addons/velero/#usage","text":"import 'source-map-support/register' ; import * as cdk from 'aws-cdk-lib' ; import * as blueprints from '@aws-quickstart/eks-blueprints' ; const app = new cdk . App (); const addOn = new blueprints . addons . VeleroAddOn (); const blueprint = blueprints . EksBlueprint . builder () . addOns ( addOn ) . build ( app , 'my-stack-name' );","title":"Usage"},{"location":"addons/velero/#functionality","text":"By default create a private S3 bucket (blocking all public access) with SSE-KMS encryption with AWS Managed Key from KMS(Encryption At Rest) for the Velero Backup destination. Configure S3 Bucket policy to enable encryption in transit. Create the IAM Role for Service Account for Velero pod to make API calls to AWS S3 and EC2 to backup and restore. Preset Velero Helm Chart Values . Allow users to pass Velero Helm Chart Values for customization purposes. Supports standard helm configuration options .","title":"Functionality"},{"location":"addons/velero/#limitations","text":"Velero has a known bug for support of S3 with SSE-KMS encryption with Customer master key (CMK). Please refer to Velero GitHub Issue #83 . As a result of #1, Velero is unable to leverage the S3 Bucket Key feature which requires using AWS CMK to achieve \"reduce AWS KMS request costs by up to 99 percent by decreasing the request traffic from Amazon S3 to AWS KMS.\"","title":"Limitations"},{"location":"addons/velero/#testing-velero-functionality","text":"The following steps will help test the backup and restore Kuberenetes resources from Velero Deploy a sample app as deployment into a newly created namespace Backup the sample app from the namespace Delete the sample app namespace Restore the sample app","title":"Testing Velero Functionality"},{"location":"addons/velero/#deploy-a-sample-app-into-a-newly-created-namespace","text":"Properly configure your kubeconfig to use kubectl command. By successfully deploying the EKS cluster, you can run commands to set up your kubeconfig correctly by: aws eks update-kubeconfig --name <EKS_Cluster_Name> --region <AWS_Region> --role-arn arn:aws:iam::<AWS_ACCOUNT_ID>:role/<IRSA_Role_Name> # Create the test01 namespace $ kubectl create ns test01 namespace/test01 created # Deploy the Nginx Stateless applications on to namespace test01 $ kubectl apply -f https://k8s.io/examples/application/deployment.yaml -n test01 deployment.apps/nginx-deployment created # Check the nginx pods $ kubectl get pods -n test01 NAME READY STATUS RESTARTS AGE nginx-deployment-66b6c48dd5-qf7lc 1 /1 Running 0 53s nginx-deployment-66b6c48dd5-wvxjx 1 /1 Running 0 53s # Deploy the Stateful Nginx Application with PV to namespace nginx-example $ kubectl apply -f https://raw.githubusercontent.com/vmware-tanzu/velero/main/examples/nginx-app/with-pv.yaml namespace/nginx-example created persistentvolumeclaim/nginx-logs created deployment.apps/nginx-deployment created service/my-nginx created # Check the application and PV $ kubectl get pods -n nginx-example NAME READY STATUS RESTARTS AGE nginx-deployment-66689547d-4mqsd 2 /2 Running 0 106s haofeif@a483e70791e6 ~ $ kubectl get pv -n nginx-example NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-73498192-6571-4e31-b455-e7e7efbf2fb7 1Gi RWO Delete Bound nginx-example/nginx-logs gp2 110s ## EBS Volume got created","title":"Deploy a sample app into a newly created namespace"},{"location":"addons/velero/#backup-the-sample-app-from-the-namespace","text":"To install Velero client Cli, please refer to the User Guide The backup will be created into the S3 bucket created or specified by the users. # Create the backup for stateless app at namespace test01 $ velero backup create test01 --include-namespaces test01 Backup request \"test01\" submitted successfully. Run ` velero backup describe test01 ` or ` velero backup logs test01 ` for more details. # Check for the backup $ velero backup get NAME STATUS ERRORS WARNINGS CREATED EXPIRES STORAGE LOCATION SELECTOR test01 Completed 0 0 2021 -09-20 12 :40:37 +1000 AEST 29d default <none> # Check the logs of the backup $ velero backup logs test01 time = \"2021-09-19T02:40:37Z\" level = info msg = \"Setting up backup temp file\" backup = velero/test01 logSource = \"pkg/controller/backup_controller.go:556\" time = \"2021-09-19T02:40:37Z\" level = info msg = \"Setting up plugin manager\" backup = velero/test01 logSource = \"pkg/controller/backup_controller.go:563\" time = \"2021-09-19T02:40:37Z\" level = info msg = \"Getting backup item actions\" backup = velero/test01 logSource = \"pkg/controller/backup_controller.go:567\" ... # Check the backup location, the Access mode shows the S3 bucket name and its folders. $ velero backup-location get NAME PROVIDER BUCKET/PREFIX PHASE LAST VALIDATED ACCESS MODE DEFAULT default aws my-stack-name-mystacknamevelerobackupxxx/velero/my-stack-name Available 2021 -09-20 12 :29:35 +1000 AEST ReadWrite true # Screenshot of the S3 bucket folder for the backup test01 # Create the Backup with the PV at namespace nginx-example $ velero backup create nginx-backup --include-namespaces nginx-example Backup request \"nginx-backup\" submitted successfully. Run ` velero backup describe nginx-backup ` or ` velero backup logs nginx-backup ` for more details. ## Check the backup status $ velero backup get NAME STATUS ERRORS WARNINGS CREATED EXPIRES STORAGE LOCATION SELECTOR nginx-backup Completed 0 0 2021 -09-20 12 :37:36 +1000 AEST 29d default <none> # Screenshot of the S3 bucket folder for the backup nginx-backup (with PV)","title":"Backup the sample app from the namespace"},{"location":"addons/velero/#delete-the-sample-app-namespace","text":"# Delete the namespace test01 $ kubectl delete ns test01 # Delete the namespace nginx-example $ kubectl delete ns nginx-example # Note: Because the default reclaim policy for dynamically-provisioned PVs is \u201cDelete\u201d, these commands should trigger AWS to delete the EBS Volume that backs the PV. Deletion is asynchronous, so this may take some time.","title":"Delete the sample app namespace"},{"location":"addons/velero/#restore-the-sample-app","text":"# Restore from the backup of test01 $ velero restore create test01 --from-backup test01 Restore request \"test01\" submitted successfully. Run ` velero restore describe test01 ` or ` velero restore logs test01 ` for more details. # Check the restore status of test01 $ velero restore get NAME BACKUP STATUS STARTED COMPLETED ERRORS WARNINGS CREATED SELECTOR test01 test01 Completed 2021 -09-20 12 :41:38 +1000 AEST <nil> 0 0 2021 -09-20 12 :41:36 +1000 AEST <none> # Check the stateless application restore completed $ kubectl get pods -n test01 NAME READY STATUS RESTARTS AGE nginx-deployment-66b6c48dd5-qf7lc 1 /1 Running 0 53s nginx-deployment-66b6c48dd5-wvxjx 1 /1 Running 0 53s # Restore from the backup of nginx-backup (With PV) after confirming EBS volume has been deleted successfully $ velero restore create --from-backup nginx-backup # Check the Restore status $ velero restore get NAME BACKUP STATUS STARTED COMPLETED ERRORS WARNINGS CREATED SELECTOR nginx-backup-20210920124336 nginx-backup Completed 2021 -09-20 12 :43:42 +1000 AEST 2021 -09-20 12 :43:45 +1000 AEST 0 0 2021 -09-20 12 :43:40 +1000 AEST <none> # Check the status of pod and PV in namespace nginx-example $ kubectl get pods -n nginx-example NAME READY STATUS RESTARTS AGE nginx-deployment-66689547d-4mqsd 2 /2 Running 0 2m12s $ kubectl get pv -n nginx-example NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-73498192-6571-4e31-b455-e7e7efbf2fb7 1Gi RWO Delete Bound nginx-example/nginx-logs gp2 2m22s # EBS Volume is back","title":"Restore the sample app"},{"location":"addons/vpc-cni/","text":"VPC CNI Amazon EKS Add-on \u00b6 The VPC CNI Amazon EKS Add-on adds support for Amazon VPC Container Network Interface (CNI) plugin. Amazon EKS supports native VPC networking with the Amazon VPC Container Network Interface (CNI) plugin for Kubernetes. Using this plugin allows Kubernetes pods to have the same IP address inside the pod as they do on the VPC network. For more information, see Pod networking (CNI) . Installing VPC CNI as Amazon EKS add-on will reduce the amount of work that is needed to do in order to install, configure, and update add-ons. It includes the latest security patches, bug fixes and is validated by AWS to work with Amazon EKS. This ensures that Amazon EKS clusters are secure and stable. Amazon EKS automatically installs VPC CNI as self-managed add-on for every cluster. So if it is already running on your cluster, you can still install it as Amazon EKS add-on to start benefiting from the capabilities of Amazon EKS add-ons. Amazon EKS VPC CNI Addon now supports advanced configurations which means we can now pass configuration values as a JSON blob for setting up advanced configurations in Amazon VPC CNI. Please refer Amazon EKS add-ons: Advanced configuration for more informatoion. Prerequisite \u00b6 Amazon EKS add-ons are only available with Amazon EKS clusters running Kubernetes version 1.18 and later. Usage \u00b6 This add-on can be used with three different patterns : Pattern # 1 : Simple and Easy. With all default values. This pattern wont create custom networking or setup any environment variables as part of configuration Values. import 'source-map-support/register' ; import * as cdk from 'aws-cdk-lib' ; import * as blueprints from '@aws-quickstart/eks-blueprints' ; const app = new cdk . App (); const addOn = new blueprints . addons . VpcCniAddOn (); const blueprint = blueprints . EksBlueprint . builder () . addOns ( addOn ) . build ( app , 'my-stack-name' ); Pattern # 2 : Custom networking with new Secondary CIDR ranges. This pattern will first create Secondary CIDRs and Secondary Subnets with specified range of CIDRs as shown below in resourceProvider command. Then the VPC CNI addon will setup custom networking based on the parameters awsVpcK8sCniCustomNetworkCfg , eniConfigLabelDef: \"topology.kubernetes.io/zone\" for your Amazon EKS cluster workloads with created secondary subnet ranges to solve IP exhaustion. Note: - When you are passing secondary CIDRs to the VPC resource provider, then we create secondary subnets for the customer and register them under names secondary-cidr-subnet-${order} with the resource providers. import 'source-map-support/register' ; import * as cdk from 'aws-cdk-lib' ; import * as blueprints from '@aws-quickstart/eks-blueprints' ; const app = new cdk . App (); const addOn = new blueprints . addons . VpcCniAddOn ({ customNetworkingConfig : { subnets : [ blueprints . getNamedResource ( \"secondary-cidr-subnet-0\" ), blueprints . getNamedResource ( \"secondary-cidr-subnet-1\" ), blueprints . getNamedResource ( \"secondary-cidr-subnet-2\" ), ] }, awsVpcK8sCniCustomNetworkCfg : true , eniConfigLabelDef : 'topology.kubernetes.io/zone' }); const blueprint = blueprints . EksBlueprint . builder () . addOns ( addOn ) . resourceProvider ( blueprints . GlobalResources . Vpc , new VpcProvider ( undefined , \"100.64.0.0/24\" ,[ \"100.64.0.0/25\" , \"100.64.0.128/26\" , \"100.64.0.192/26\" ],)) . build ( app , 'my-stack-name' ); Pattern # 3 : Custom networking with custom VPC and Secondary Subnets. This pattern will use the custom VPC ID and Secondary subnet IDs passed by the user to create the blueprints stack. Then the VPC CNI addon will setup custom networking based on the parameters awsVpcK8sCniCustomNetworkCfg , eniConfigLabelDef: \"topology.kubernetes.io/zone\" for your Amazon EKS cluster workloads with passed secondary subnet ranges to solve IP exhaustion. Note : - When you are passing secondary subnet ids to the VPC resource provider, then we register them under names secondary-cidr-subnet-${order} with the resource providers. - When you are passing your own Secondary subnets using this pattern, Please make sure the tag Key: kubernetes.io/role/internal-elb\", Value: \"1\" is added to your secondary subnets. Please register your secondary subnets in any arbitary name as shown below in resourceProvider .Please check out Custom Networking Tutorial to learn how custome networking is manually setup on your Amazon EKS cluster. import 'source-map-support/register' ; import * as cdk from 'aws-cdk-lib' ; import * as blueprints from '@aws-quickstart/eks-blueprints' ; const app = new cdk . App (); const addOn = new blueprints . addons . VpcCniAddOn ({ customNetworkingConfig : { subnets : [ blueprints . getNamedResource ( \"secondary-cidr-subnet-0\" ), blueprints . getNamedResource ( \"secondary-cidr-subnet-1\" ), blueprints . getNamedResource ( \"secondary-cidr-subnet-2\" ), ] }, awsVpcK8sCniCustomNetworkCfg : true , eniConfigLabelDef : 'topology.kubernetes.io/zone' }); const blueprint = blueprints . EksBlueprint . builder () . addOns ( addOn ) . resourceProvider ( blueprints . GlobalResources . Vpc , new VpcProvider ( yourVpcId )) . resourceProvider ( \"secondary-cidr-subnet-0\" , new LookupSubnetProvider ( subnet1Id ) . resourceProvider ( \"secondary-cidr-subnet-1\" , new LookupSubnetProvider ( subnet2Id ) . resourceProvider ( \"secondary-cidr-subnet-2\" , new LookupSubnetProvider ( subnet3Id ) . build ( app , 'my-stack-name' ); VPC-CNI Service Account and IRSA config \u00b6 VPC CNI add-on supports creation of an IRSA role for the add-on if the customer supplies managed policies for the add-on configuration (i.e. if the serviceAccountPolicies field is populated). Example: const vpcCniAddOn = new VpcCniAddOn ({ serviceAccountPolicies : [ iam . ManagedPolicy . fromAwsManagedPolicyName ( \"AmazonEKS_CNI_Policy\" )] }); The example above is using the CNI policy sufficient for ENI allocation for IPv4. Please consult the AWS documentation for IPv6 policies. Note : when using IRSA account with the VPC-CNI plug-in, the node instance role does not need the AmazonEKS_CNI_Policy. It can be removed from the node instance role by supplying a custom role. Example blueprint with Node Instance Role without CNI policy: const nodeRole = new blueprints . CreateRoleProvider ( \"blueprint-node-role\" , new iam . ServicePrincipal ( \"ec2.amazonaws.com\" ), [ iam . ManagedPolicy . fromAwsManagedPolicyName ( \"AmazonEKSWorkerNodePolicy\" ), iam . ManagedPolicy . fromAwsManagedPolicyName ( \"AmazonEC2ContainerRegistryReadOnly\" ), iam . ManagedPolicy . fromAwsManagedPolicyName ( \"AmazonSSMManagedInstanceCore\" ) ]); const clusterProvider = new blueprints . GenericClusterProvider ({ version : KubernetesVersion.V1_24 , managedNodeGroups : [ { id : \"mng1\" , instanceTypes : [ new ec2 . InstanceType ( 'm5.4xlarge' )], nodeRole : blueprints.getNamedResource ( \"node-role\" ) as iam . Role } ] }); blueprints . EksBlueprint . builder () . addOns (... addOns ) . resourceProvider ( \"node-role\" , nodeRole ) . clusterProvider ( clusterProvider ) . build ( scope , \"blueprint\" , props ); Configuration Options \u00b6 version : Pass in the optional vpc-cni plugin version compatible with kubernetes-cluster version as shown below # Assuming cluster version is 1.19, below command shows versions of the vpc-cni add-on available for the specified cluster's version. aws eks describe-addon-versions \\ --addon-name vpc-cni \\ --kubernetes-version 1 .23 \\ --query \"addons[].addonVersions[].[addonVersion, compatibilities[].defaultVersion]\" \\ --output text # Output v1.12.1-eksbuild.2 False v1.12.1-eksbuild.1 False v1.12.0-eksbuild.2 False v1.12.0-eksbuild.1 False v1.11.4-eksbuild.3 False v1.11.4-eksbuild.2 False v1.11.4-eksbuild.1 False v1.11.3-eksbuild.3 False v1.11.3-eksbuild.2 False v1.11.3-eksbuild.1 False v1.11.2-eksbuild.3 False ... ... v1.7.5-eksbuild.1 False v1.6.3-eksbuild.2 False Validation \u00b6 To validate that vpc-cni add-on is running, ensure that the pod is in Running state. $ kubectl -n kube-system get ds aws-node -oyaml|grep AWS_VPC_K8S_CNI_CUSTOM_NETWORK_CFG -A1 # Assuming cluster-name is my-cluster, below command shows the version of vpc-cni add-on installed. Check if it is same as the version installed via EKS add-on aws eks describe-addon \\ --cluster-name my-cluster \\ --addon-name vpc-cni \\ --query \"addon.addonVersion\" \\ --output text # Output v1.12.1-eksbuild.2 Functionality \u00b6 Applies VPC CNI add-on to Amazon EKS cluster. Custom Networking \u00b6 We are installing VPC CNI addon of CDK EKS blueprints ahead of your data plane node creation so we dont have a need to cordon and drain the nodes to gracefully shutdown the Pods and then terminate the nodes. VPC CNI addon will be first installed on Amazon EKS Control Plane, then data plan nodes will be deployed and then all the other day 2 operational addons you have opted in will be installed. This solves IP exhaustion via custom networking of VPC CNI addon out of the box without any manual intervention. Please check our Amazon EKS Best Practices Guide for Networking for more information on custom networking. References \u00b6 Reference amazon-vpc-cni-k8s to learn more about different VPC CNI configuration Values Reference VpcCniAddon to learn more about this addon Reference Amazon EKS Best Practices Guide for Networking to learn about Amazon EKS networking best practices Reference Custom Networking Tutorial to learn how custome networking is manually setup on your Amazon EKS cluster.","title":"Vpc Cni"},{"location":"addons/vpc-cni/#vpc-cni-amazon-eks-add-on","text":"The VPC CNI Amazon EKS Add-on adds support for Amazon VPC Container Network Interface (CNI) plugin. Amazon EKS supports native VPC networking with the Amazon VPC Container Network Interface (CNI) plugin for Kubernetes. Using this plugin allows Kubernetes pods to have the same IP address inside the pod as they do on the VPC network. For more information, see Pod networking (CNI) . Installing VPC CNI as Amazon EKS add-on will reduce the amount of work that is needed to do in order to install, configure, and update add-ons. It includes the latest security patches, bug fixes and is validated by AWS to work with Amazon EKS. This ensures that Amazon EKS clusters are secure and stable. Amazon EKS automatically installs VPC CNI as self-managed add-on for every cluster. So if it is already running on your cluster, you can still install it as Amazon EKS add-on to start benefiting from the capabilities of Amazon EKS add-ons. Amazon EKS VPC CNI Addon now supports advanced configurations which means we can now pass configuration values as a JSON blob for setting up advanced configurations in Amazon VPC CNI. Please refer Amazon EKS add-ons: Advanced configuration for more informatoion.","title":"VPC CNI Amazon EKS Add-on"},{"location":"addons/vpc-cni/#prerequisite","text":"Amazon EKS add-ons are only available with Amazon EKS clusters running Kubernetes version 1.18 and later.","title":"Prerequisite"},{"location":"addons/vpc-cni/#usage","text":"This add-on can be used with three different patterns : Pattern # 1 : Simple and Easy. With all default values. This pattern wont create custom networking or setup any environment variables as part of configuration Values. import 'source-map-support/register' ; import * as cdk from 'aws-cdk-lib' ; import * as blueprints from '@aws-quickstart/eks-blueprints' ; const app = new cdk . App (); const addOn = new blueprints . addons . VpcCniAddOn (); const blueprint = blueprints . EksBlueprint . builder () . addOns ( addOn ) . build ( app , 'my-stack-name' ); Pattern # 2 : Custom networking with new Secondary CIDR ranges. This pattern will first create Secondary CIDRs and Secondary Subnets with specified range of CIDRs as shown below in resourceProvider command. Then the VPC CNI addon will setup custom networking based on the parameters awsVpcK8sCniCustomNetworkCfg , eniConfigLabelDef: \"topology.kubernetes.io/zone\" for your Amazon EKS cluster workloads with created secondary subnet ranges to solve IP exhaustion. Note: - When you are passing secondary CIDRs to the VPC resource provider, then we create secondary subnets for the customer and register them under names secondary-cidr-subnet-${order} with the resource providers. import 'source-map-support/register' ; import * as cdk from 'aws-cdk-lib' ; import * as blueprints from '@aws-quickstart/eks-blueprints' ; const app = new cdk . App (); const addOn = new blueprints . addons . VpcCniAddOn ({ customNetworkingConfig : { subnets : [ blueprints . getNamedResource ( \"secondary-cidr-subnet-0\" ), blueprints . getNamedResource ( \"secondary-cidr-subnet-1\" ), blueprints . getNamedResource ( \"secondary-cidr-subnet-2\" ), ] }, awsVpcK8sCniCustomNetworkCfg : true , eniConfigLabelDef : 'topology.kubernetes.io/zone' }); const blueprint = blueprints . EksBlueprint . builder () . addOns ( addOn ) . resourceProvider ( blueprints . GlobalResources . Vpc , new VpcProvider ( undefined , \"100.64.0.0/24\" ,[ \"100.64.0.0/25\" , \"100.64.0.128/26\" , \"100.64.0.192/26\" ],)) . build ( app , 'my-stack-name' ); Pattern # 3 : Custom networking with custom VPC and Secondary Subnets. This pattern will use the custom VPC ID and Secondary subnet IDs passed by the user to create the blueprints stack. Then the VPC CNI addon will setup custom networking based on the parameters awsVpcK8sCniCustomNetworkCfg , eniConfigLabelDef: \"topology.kubernetes.io/zone\" for your Amazon EKS cluster workloads with passed secondary subnet ranges to solve IP exhaustion. Note : - When you are passing secondary subnet ids to the VPC resource provider, then we register them under names secondary-cidr-subnet-${order} with the resource providers. - When you are passing your own Secondary subnets using this pattern, Please make sure the tag Key: kubernetes.io/role/internal-elb\", Value: \"1\" is added to your secondary subnets. Please register your secondary subnets in any arbitary name as shown below in resourceProvider .Please check out Custom Networking Tutorial to learn how custome networking is manually setup on your Amazon EKS cluster. import 'source-map-support/register' ; import * as cdk from 'aws-cdk-lib' ; import * as blueprints from '@aws-quickstart/eks-blueprints' ; const app = new cdk . App (); const addOn = new blueprints . addons . VpcCniAddOn ({ customNetworkingConfig : { subnets : [ blueprints . getNamedResource ( \"secondary-cidr-subnet-0\" ), blueprints . getNamedResource ( \"secondary-cidr-subnet-1\" ), blueprints . getNamedResource ( \"secondary-cidr-subnet-2\" ), ] }, awsVpcK8sCniCustomNetworkCfg : true , eniConfigLabelDef : 'topology.kubernetes.io/zone' }); const blueprint = blueprints . EksBlueprint . builder () . addOns ( addOn ) . resourceProvider ( blueprints . GlobalResources . Vpc , new VpcProvider ( yourVpcId )) . resourceProvider ( \"secondary-cidr-subnet-0\" , new LookupSubnetProvider ( subnet1Id ) . resourceProvider ( \"secondary-cidr-subnet-1\" , new LookupSubnetProvider ( subnet2Id ) . resourceProvider ( \"secondary-cidr-subnet-2\" , new LookupSubnetProvider ( subnet3Id ) . build ( app , 'my-stack-name' );","title":"Usage"},{"location":"addons/vpc-cni/#vpc-cni-service-account-and-irsa-config","text":"VPC CNI add-on supports creation of an IRSA role for the add-on if the customer supplies managed policies for the add-on configuration (i.e. if the serviceAccountPolicies field is populated). Example: const vpcCniAddOn = new VpcCniAddOn ({ serviceAccountPolicies : [ iam . ManagedPolicy . fromAwsManagedPolicyName ( \"AmazonEKS_CNI_Policy\" )] }); The example above is using the CNI policy sufficient for ENI allocation for IPv4. Please consult the AWS documentation for IPv6 policies. Note : when using IRSA account with the VPC-CNI plug-in, the node instance role does not need the AmazonEKS_CNI_Policy. It can be removed from the node instance role by supplying a custom role. Example blueprint with Node Instance Role without CNI policy: const nodeRole = new blueprints . CreateRoleProvider ( \"blueprint-node-role\" , new iam . ServicePrincipal ( \"ec2.amazonaws.com\" ), [ iam . ManagedPolicy . fromAwsManagedPolicyName ( \"AmazonEKSWorkerNodePolicy\" ), iam . ManagedPolicy . fromAwsManagedPolicyName ( \"AmazonEC2ContainerRegistryReadOnly\" ), iam . ManagedPolicy . fromAwsManagedPolicyName ( \"AmazonSSMManagedInstanceCore\" ) ]); const clusterProvider = new blueprints . GenericClusterProvider ({ version : KubernetesVersion.V1_24 , managedNodeGroups : [ { id : \"mng1\" , instanceTypes : [ new ec2 . InstanceType ( 'm5.4xlarge' )], nodeRole : blueprints.getNamedResource ( \"node-role\" ) as iam . Role } ] }); blueprints . EksBlueprint . builder () . addOns (... addOns ) . resourceProvider ( \"node-role\" , nodeRole ) . clusterProvider ( clusterProvider ) . build ( scope , \"blueprint\" , props );","title":"VPC-CNI Service Account and IRSA config"},{"location":"addons/vpc-cni/#configuration-options","text":"version : Pass in the optional vpc-cni plugin version compatible with kubernetes-cluster version as shown below # Assuming cluster version is 1.19, below command shows versions of the vpc-cni add-on available for the specified cluster's version. aws eks describe-addon-versions \\ --addon-name vpc-cni \\ --kubernetes-version 1 .23 \\ --query \"addons[].addonVersions[].[addonVersion, compatibilities[].defaultVersion]\" \\ --output text # Output v1.12.1-eksbuild.2 False v1.12.1-eksbuild.1 False v1.12.0-eksbuild.2 False v1.12.0-eksbuild.1 False v1.11.4-eksbuild.3 False v1.11.4-eksbuild.2 False v1.11.4-eksbuild.1 False v1.11.3-eksbuild.3 False v1.11.3-eksbuild.2 False v1.11.3-eksbuild.1 False v1.11.2-eksbuild.3 False ... ... v1.7.5-eksbuild.1 False v1.6.3-eksbuild.2 False","title":"Configuration Options"},{"location":"addons/vpc-cni/#validation","text":"To validate that vpc-cni add-on is running, ensure that the pod is in Running state. $ kubectl -n kube-system get ds aws-node -oyaml|grep AWS_VPC_K8S_CNI_CUSTOM_NETWORK_CFG -A1 # Assuming cluster-name is my-cluster, below command shows the version of vpc-cni add-on installed. Check if it is same as the version installed via EKS add-on aws eks describe-addon \\ --cluster-name my-cluster \\ --addon-name vpc-cni \\ --query \"addon.addonVersion\" \\ --output text # Output v1.12.1-eksbuild.2","title":"Validation"},{"location":"addons/vpc-cni/#functionality","text":"Applies VPC CNI add-on to Amazon EKS cluster.","title":"Functionality"},{"location":"addons/vpc-cni/#custom-networking","text":"We are installing VPC CNI addon of CDK EKS blueprints ahead of your data plane node creation so we dont have a need to cordon and drain the nodes to gracefully shutdown the Pods and then terminate the nodes. VPC CNI addon will be first installed on Amazon EKS Control Plane, then data plan nodes will be deployed and then all the other day 2 operational addons you have opted in will be installed. This solves IP exhaustion via custom networking of VPC CNI addon out of the box without any manual intervention. Please check our Amazon EKS Best Practices Guide for Networking for more information on custom networking.","title":"Custom Networking"},{"location":"addons/vpc-cni/#references","text":"Reference amazon-vpc-cni-k8s to learn more about different VPC CNI configuration Values Reference VpcCniAddon to learn more about this addon Reference Amazon EKS Best Practices Guide for Networking to learn about Amazon EKS networking best practices Reference Custom Networking Tutorial to learn how custome networking is manually setup on your Amazon EKS cluster.","title":"References"},{"location":"addons/xray-adot-addon/","text":"AWS X-Ray ADOT Add-on \u00b6 AWS X-Ray helps developers analyze and debug production, distributed applications, such as those built using a microservices architecture. This add-on deploys an AWS Distro for OpenTelemetry (ADOT) Collector for X-Ray which receives traces from the application and sends the same to X-Ray console. You can change the mode to Daemonset, StatefulSet, and Sidecar depending upon your deployment strategy. This add-on is not automatically installed when you first create a cluster, it must be added to the cluster in order to setup X-Ray for remote write traces. For more information on the add-on, please review the user guide . Prerequisites \u00b6 cert-manager Blueprints add-on. adot EKS Blueprints add-on. Usage \u00b6 This add-on can used with two different patterns : Pattern # 1 : Simple and Easy - Using all default property values. This pattern deploys an ADOT collector in the default namespace with deployment as the mode to write traces to X-Ray console. import * as cdk from 'aws-cdk-lib' ; import * as blueprints from '@aws-quickstart/eks-blueprints' ; const app = new cdk . App (); const addOn = new blueprints . addons . XrayAdotAddOn (); const blueprint = blueprints . EksBlueprint . builder () . addOns ( addOn ) . build ( app , 'my-stack-name' ); Pattern # 2 : Overriding Property value for different deployment Modes. This pattern deploys an ADOT collector on the namespace specified in namespace , name specified in name with daemonset as the mode to X-Ray console. Deployment mode can be overridden to any of these values - deployment , daemonset , statefulset , sidecar . import * as cdk from 'aws-cdk-lib' ; import * as blueprints from '@aws-quickstart/eks-blueprints' ; const app = new cdk . App (); const addOn = new blueprints . addons . XrayAdotAddOn ({ deploymentMode : XrayDeploymentMode.DAEMONSET , namespace : 'default' , name : 'adot-collector-xray' }); const blueprint = blueprints . EksBlueprint . builder () . addOns ( addOn ) . build ( app , 'my-stack-name' ); Validation \u00b6 To validate that X-Ray add-on is installed properly, ensure that the required kubernetes resources are running in the cluster kubectl get all -n default Output \u00b6 NAME READY STATUS RESTARTS AGE pod/otel-collector-xray-collector-6fc44d9bbf-xdfpg 1 /1 Running 0 6m44s pod/traffic-generator-86f86d84cc-s78wv 0 /1 Terminating 0 128m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE service/kubernetes ClusterIP 172 .20.0.1 <none> 443 /TCP 3d service/otel-collector-xray-collector ClusterIP 172 .20.83.240 <none> 4317 /TCP,4318/TCP 6m46s service/otel-collector-xray-collector-headless ClusterIP None <none> 4317 /TCP,4318/TCP 6m46s service/otel-collector-xray-collector-monitoring ClusterIP 172 .20.2.85 <none> 8888 /TCP 6m46s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/otel-collector-xray-collector 1 /1 1 1 6m44s NAME DESIRED CURRENT READY AGE replicaset.apps/otel-collector-xray-collector-6fc44d9bbf 1 1 1 6m44s Functionality \u00b6 Applies the X-Ray ADOT add-on to an Amazon EKS cluster.","title":"AWS XRay ADOT"},{"location":"addons/xray-adot-addon/#aws-x-ray-adot-add-on","text":"AWS X-Ray helps developers analyze and debug production, distributed applications, such as those built using a microservices architecture. This add-on deploys an AWS Distro for OpenTelemetry (ADOT) Collector for X-Ray which receives traces from the application and sends the same to X-Ray console. You can change the mode to Daemonset, StatefulSet, and Sidecar depending upon your deployment strategy. This add-on is not automatically installed when you first create a cluster, it must be added to the cluster in order to setup X-Ray for remote write traces. For more information on the add-on, please review the user guide .","title":"AWS X-Ray ADOT Add-on"},{"location":"addons/xray-adot-addon/#prerequisites","text":"cert-manager Blueprints add-on. adot EKS Blueprints add-on.","title":"Prerequisites"},{"location":"addons/xray-adot-addon/#usage","text":"This add-on can used with two different patterns : Pattern # 1 : Simple and Easy - Using all default property values. This pattern deploys an ADOT collector in the default namespace with deployment as the mode to write traces to X-Ray console. import * as cdk from 'aws-cdk-lib' ; import * as blueprints from '@aws-quickstart/eks-blueprints' ; const app = new cdk . App (); const addOn = new blueprints . addons . XrayAdotAddOn (); const blueprint = blueprints . EksBlueprint . builder () . addOns ( addOn ) . build ( app , 'my-stack-name' ); Pattern # 2 : Overriding Property value for different deployment Modes. This pattern deploys an ADOT collector on the namespace specified in namespace , name specified in name with daemonset as the mode to X-Ray console. Deployment mode can be overridden to any of these values - deployment , daemonset , statefulset , sidecar . import * as cdk from 'aws-cdk-lib' ; import * as blueprints from '@aws-quickstart/eks-blueprints' ; const app = new cdk . App (); const addOn = new blueprints . addons . XrayAdotAddOn ({ deploymentMode : XrayDeploymentMode.DAEMONSET , namespace : 'default' , name : 'adot-collector-xray' }); const blueprint = blueprints . EksBlueprint . builder () . addOns ( addOn ) . build ( app , 'my-stack-name' );","title":"Usage"},{"location":"addons/xray-adot-addon/#validation","text":"To validate that X-Ray add-on is installed properly, ensure that the required kubernetes resources are running in the cluster kubectl get all -n default","title":"Validation"},{"location":"addons/xray-adot-addon/#output","text":"NAME READY STATUS RESTARTS AGE pod/otel-collector-xray-collector-6fc44d9bbf-xdfpg 1 /1 Running 0 6m44s pod/traffic-generator-86f86d84cc-s78wv 0 /1 Terminating 0 128m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE service/kubernetes ClusterIP 172 .20.0.1 <none> 443 /TCP 3d service/otel-collector-xray-collector ClusterIP 172 .20.83.240 <none> 4317 /TCP,4318/TCP 6m46s service/otel-collector-xray-collector-headless ClusterIP None <none> 4317 /TCP,4318/TCP 6m46s service/otel-collector-xray-collector-monitoring ClusterIP 172 .20.2.85 <none> 8888 /TCP 6m46s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/otel-collector-xray-collector 1 /1 1 1 6m44s NAME DESIRED CURRENT READY AGE replicaset.apps/otel-collector-xray-collector-6fc44d9bbf 1 1 1 6m44s","title":"Output"},{"location":"addons/xray-adot-addon/#functionality","text":"Applies the X-Ray ADOT add-on to an Amazon EKS cluster.","title":"Functionality"},{"location":"addons/xray/","text":"AWS X-Ray Add-on \u00b6 This add-on is currently deprecated since the underlying manifests are incompatible with the latest versions of EKS. Please use XRay Adot Add-on . AWS X-Ray helps developers analyze and debug production, distributed applications, such as those built using a microservices architecture. The X-Ray addon provisions X-Ray daemon into an EKS cluster. This daemon exposes an internal endpoint xray-service.xray-system.svc.cluster.local:2000 that could be leveraged to aggregate and post traces to the AWS X-Ray service. For instructions on getting started with X-Ray on EKS refer to the EKS Workshop X-Ray Section . Usage \u00b6 import 'source-map-support/register' ; import * as cdk from 'aws-cdk-lib' ; import * as blueprints from '@aws-quickstart/eks-blueprints' ; const app = new cdk . App (); const addOn = new blueprints . addons . XrayAddOn (); const blueprint = blueprints . EksBlueprint . builder () . addOns ( addOn ) . build ( app , 'my-stack-name' ); Once deployed, it allows applications to be instrumented with X-Ray by leveraging the X-Ray SDK. Examples of such integration can be found on GitHub . Functionality \u00b6 Creates the xray-system namespace. Deploys the xray-daemon manifests into the cluster. Configures Kubernetes service account with IRSA ( AWSXRayDaemonWriteAccess ) for communication between the cluster and the AWS X-Ray service","title":"AWS XRay"},{"location":"addons/xray/#aws-x-ray-add-on","text":"This add-on is currently deprecated since the underlying manifests are incompatible with the latest versions of EKS. Please use XRay Adot Add-on . AWS X-Ray helps developers analyze and debug production, distributed applications, such as those built using a microservices architecture. The X-Ray addon provisions X-Ray daemon into an EKS cluster. This daemon exposes an internal endpoint xray-service.xray-system.svc.cluster.local:2000 that could be leveraged to aggregate and post traces to the AWS X-Ray service. For instructions on getting started with X-Ray on EKS refer to the EKS Workshop X-Ray Section .","title":"AWS X-Ray Add-on"},{"location":"addons/xray/#usage","text":"import 'source-map-support/register' ; import * as cdk from 'aws-cdk-lib' ; import * as blueprints from '@aws-quickstart/eks-blueprints' ; const app = new cdk . App (); const addOn = new blueprints . addons . XrayAddOn (); const blueprint = blueprints . EksBlueprint . builder () . addOns ( addOn ) . build ( app , 'my-stack-name' ); Once deployed, it allows applications to be instrumented with X-Ray by leveraging the X-Ray SDK. Examples of such integration can be found on GitHub .","title":"Usage"},{"location":"addons/xray/#functionality","text":"Creates the xray-system namespace. Deploys the xray-daemon manifests into the cluster. Configures Kubernetes service account with IRSA ( AWSXRayDaemonWriteAccess ) for communication between the cluster and the AWS X-Ray service","title":"Functionality"},{"location":"cluster-providers/","text":"Cluster Providers \u00b6 The eks-blueprints framework allows customers to easily configure the underlying EKS clusters that it provisions. This is done via Cluster Providers. Customers can leverage the Cluster Providers that the framework supports, or supply their own. The framework currently provides support for the following Cluster Providers: Cluster Provider Description GenericClusterProvider Provisions an EKS cluster with one or more managed or Auto Scaling groups as well as Fargate Profiles. AsgClusterProvider Provisions an EKS cluster with an Auto Scaling group used for compute capacity. MngClusterProvider Provisions an EKS cluster with a Managed Node group for compute capacity. FargateClusterProviders Provisions an EKS cluster which leverages AWS Fargate to run Kubernetes pods. By default, the framework will leverage the MngClusterProvider which creates a single managed node group. If you would like to add more node groups to a single cluster, you can leverage GenericClusterProvider , which allows multiple managed node groups or autoscaling (self-managed) node groups along with Fargate profiles.","title":"Overview"},{"location":"cluster-providers/#cluster-providers","text":"The eks-blueprints framework allows customers to easily configure the underlying EKS clusters that it provisions. This is done via Cluster Providers. Customers can leverage the Cluster Providers that the framework supports, or supply their own. The framework currently provides support for the following Cluster Providers: Cluster Provider Description GenericClusterProvider Provisions an EKS cluster with one or more managed or Auto Scaling groups as well as Fargate Profiles. AsgClusterProvider Provisions an EKS cluster with an Auto Scaling group used for compute capacity. MngClusterProvider Provisions an EKS cluster with a Managed Node group for compute capacity. FargateClusterProviders Provisions an EKS cluster which leverages AWS Fargate to run Kubernetes pods. By default, the framework will leverage the MngClusterProvider which creates a single managed node group. If you would like to add more node groups to a single cluster, you can leverage GenericClusterProvider , which allows multiple managed node groups or autoscaling (self-managed) node groups along with Fargate profiles.","title":"Cluster Providers"},{"location":"cluster-providers/asg-cluster-provider/","text":"Auto Scaling Group Cluster Provider \u00b6 The AsgClusterProvider allows you to provision an EKS cluster which leverages EC2 Auto Scaling groups (ASGs) for compute capacity. An Auto Scaling group contains a collection of Amazon EC2 instances that are treated as a logical grouping for the purposes of automatic scaling and management. Usage \u00b6 const props : AsgClusterProviderProps = { minSize : 1 , maxSize : 10 , desiredSize : 4 , instanceType : new InstanceType ( 'm5.large' ), machineImageType : eks.MachineImageType.AMAZON_LINUX_2 , updatePolicy : UpdatePolicy.Rolling } const clusterProvider = new blueprints . AsgClusterProvider ( props ); new blueprints . EksBlueprint ( scope , { id : 'blueprint' , [], [], clusterProvider }); Configuration \u00b6 AsgClusterProvider supports the following configuration options. Prop Description name The name for the cluster. minSize Min cluster size, must be positive integer greater than 0 (default 1). maxSize Max cluster size, must be greater than minSize (default 3). desiredSize Desired cluster size, must be greater or equal to minSize (default min-size ). instanceType Type of instance for the EKS cluster, must be a valid instance type, i.e. t3.medium (default \"m5.large\") machineImageType Machine Image Type for the Autoscaling Group. updatePolicy Update policy for the Autoscaling Group. vpcSubnets The subnets for the cluster. privateCluster If true Kubernetes API server is private. There should be public and private subnets for EKS cluster to work. For more information see Cluster VPC Considerations . Configuration can also be supplied via context variables (specify in cdk.json, cdk.context.json, ~/.cdk.json or pass with -c command line option): eks.default.min-size eks.default.max-size eks.default.desired-size eks.default.instance-type eks.default.private-cluster Configuration of the EC2 parameters through context parameters makes sense if you would like to apply default configuration to multiple clusters without the need to explicitly pass AsgClusterProviderProps to each cluster blueprint. You can find more details on the supported configuration options in the API documentation for the AsgClusterProviderProps . Bottlerocket ASG \u00b6 Bottlerocket is a Linux-based open-source operating system that is purpose-built by Amazon Web Services for running containers. Customers can leverage the AsgClusterProvider to provision EKS clusters with Bottlerocket nodes. To do so, set the machineImageType property to eks.MachineImageType.BOTTLEROCKET . const props : AsgClusterProviderProps = { minSize : 1 , maxSize : 10 , desiredSize : 4 , instanceType : new InstanceType ( 'm5.large' ), machineImageType : eks.MachineImageType.BOTTLEROCKET , updatePolicy : UpdatePolicy.Rolling } const clusterProvider = new blueprints . AsgClusterProvider ( props ); new blueprints . EksBlueprint ( scope , { id : 'blueprint' , teams , addOns , clusterProvider });","title":"ASG Cluster Provider"},{"location":"cluster-providers/asg-cluster-provider/#auto-scaling-group-cluster-provider","text":"The AsgClusterProvider allows you to provision an EKS cluster which leverages EC2 Auto Scaling groups (ASGs) for compute capacity. An Auto Scaling group contains a collection of Amazon EC2 instances that are treated as a logical grouping for the purposes of automatic scaling and management.","title":"Auto Scaling Group Cluster Provider"},{"location":"cluster-providers/asg-cluster-provider/#usage","text":"const props : AsgClusterProviderProps = { minSize : 1 , maxSize : 10 , desiredSize : 4 , instanceType : new InstanceType ( 'm5.large' ), machineImageType : eks.MachineImageType.AMAZON_LINUX_2 , updatePolicy : UpdatePolicy.Rolling } const clusterProvider = new blueprints . AsgClusterProvider ( props ); new blueprints . EksBlueprint ( scope , { id : 'blueprint' , [], [], clusterProvider });","title":"Usage"},{"location":"cluster-providers/asg-cluster-provider/#configuration","text":"AsgClusterProvider supports the following configuration options. Prop Description name The name for the cluster. minSize Min cluster size, must be positive integer greater than 0 (default 1). maxSize Max cluster size, must be greater than minSize (default 3). desiredSize Desired cluster size, must be greater or equal to minSize (default min-size ). instanceType Type of instance for the EKS cluster, must be a valid instance type, i.e. t3.medium (default \"m5.large\") machineImageType Machine Image Type for the Autoscaling Group. updatePolicy Update policy for the Autoscaling Group. vpcSubnets The subnets for the cluster. privateCluster If true Kubernetes API server is private. There should be public and private subnets for EKS cluster to work. For more information see Cluster VPC Considerations . Configuration can also be supplied via context variables (specify in cdk.json, cdk.context.json, ~/.cdk.json or pass with -c command line option): eks.default.min-size eks.default.max-size eks.default.desired-size eks.default.instance-type eks.default.private-cluster Configuration of the EC2 parameters through context parameters makes sense if you would like to apply default configuration to multiple clusters without the need to explicitly pass AsgClusterProviderProps to each cluster blueprint. You can find more details on the supported configuration options in the API documentation for the AsgClusterProviderProps .","title":"Configuration"},{"location":"cluster-providers/asg-cluster-provider/#bottlerocket-asg","text":"Bottlerocket is a Linux-based open-source operating system that is purpose-built by Amazon Web Services for running containers. Customers can leverage the AsgClusterProvider to provision EKS clusters with Bottlerocket nodes. To do so, set the machineImageType property to eks.MachineImageType.BOTTLEROCKET . const props : AsgClusterProviderProps = { minSize : 1 , maxSize : 10 , desiredSize : 4 , instanceType : new InstanceType ( 'm5.large' ), machineImageType : eks.MachineImageType.BOTTLEROCKET , updatePolicy : UpdatePolicy.Rolling } const clusterProvider = new blueprints . AsgClusterProvider ( props ); new blueprints . EksBlueprint ( scope , { id : 'blueprint' , teams , addOns , clusterProvider });","title":"Bottlerocket ASG"},{"location":"cluster-providers/fargate-cluster-provider/","text":"Fargate Cluster Provider \u00b6 The FargateClusterProvider allows you to provision an EKS cluster which runs Kubernetes pods on AWS Fargate . To create a Fargate cluster, you must provide Fargate Profiles , which allows cluster operators to specify which Pods should be run on Fargate. Usage \u00b6 In the example below, the Fargate profile indicates that all Pods in the dynatrace namespace should run on Fargate. const fargateProfiles : Map < string , eks . FargateProfileOptions > = new Map ([ [ \"dynatrace\" , { selectors : [{ namespace : \"dynatrace\" }] }] ]); const clusterProvider = new blueprints . FargateClusterProvider ({ fargateProfiles }); new blueprints . EksBlueprint ( scope , { id : 'blueprint' , [], [], clusterProvider }); Configuration \u00b6 FargateClusterProvider supports the following configuration options. Prop Description name The name for the cluster. fargateProfiles A map of Fargate profiles to use with the cluster. vpcSubnets The subnets for the cluster. privateCluster Public cluster, you will need to provide a list of subnets. There should be public and private subnets for EKS cluster to work. For more information see Cluster VPC Considerations You can find more details on the supported configuration options in the API documentation for the FargateClusterProviderProps .","title":"Fargate Cluster Provider"},{"location":"cluster-providers/fargate-cluster-provider/#fargate-cluster-provider","text":"The FargateClusterProvider allows you to provision an EKS cluster which runs Kubernetes pods on AWS Fargate . To create a Fargate cluster, you must provide Fargate Profiles , which allows cluster operators to specify which Pods should be run on Fargate.","title":"Fargate Cluster Provider"},{"location":"cluster-providers/fargate-cluster-provider/#usage","text":"In the example below, the Fargate profile indicates that all Pods in the dynatrace namespace should run on Fargate. const fargateProfiles : Map < string , eks . FargateProfileOptions > = new Map ([ [ \"dynatrace\" , { selectors : [{ namespace : \"dynatrace\" }] }] ]); const clusterProvider = new blueprints . FargateClusterProvider ({ fargateProfiles }); new blueprints . EksBlueprint ( scope , { id : 'blueprint' , [], [], clusterProvider });","title":"Usage"},{"location":"cluster-providers/fargate-cluster-provider/#configuration","text":"FargateClusterProvider supports the following configuration options. Prop Description name The name for the cluster. fargateProfiles A map of Fargate profiles to use with the cluster. vpcSubnets The subnets for the cluster. privateCluster Public cluster, you will need to provide a list of subnets. There should be public and private subnets for EKS cluster to work. For more information see Cluster VPC Considerations You can find more details on the supported configuration options in the API documentation for the FargateClusterProviderProps .","title":"Configuration"},{"location":"cluster-providers/generic-cluster-provider/","text":"Generic Cluster Provider \u00b6 The GenericClusterProvider allows you to provision an EKS cluster which leverages one or more EKS managed node groups (MNGs), or one or more autoscaling groups EC2 Auto Scaling groups for its compute capacity. Users can also configure multiple Fargate profiles along with the EC2 based compute cpacity. Today it is not possible for an Amazon EKS Cluster to propagate tags to EC2 instance worker nodes directly when you create an EKS cluster. You can create a launch template with custom tags on managedNodeGroups with GenericClusterProvider as shown in mng2-launchtemplate . This will allow you to propagate custom tags to your EC2 instance worker nodes. Note: If launchTemplate is passed with managedNodeGroups , diskSize is not allowed. Configuration \u00b6 Full list of configuration options: Generic Cluster Provider Managed Node Group Autoscaling Group Fargate Cluster Usage \u00b6 const clusterProvider = new blueprints . GenericClusterProvider ({ version : KubernetesVersion.V1_24 , serviceIpv4Cidr : \"10.43.0.0/16\" , // if needed use this to register an auth role integrate with RBAC mastersRole : blueprints.getResource ( context => { return new iam . Role ( context . scope , 'AdminRole' , { assumedBy : new AccountRootPrincipal () }); }), managedNodeGroups : [ { id : \"mng1\" , amiType : NodegroupAmiType.AL2_X86_64 , instanceTypes : [ new InstanceType ( 'm5.2xlarge' )], desiredSize : 2 , maxSize : 3 , nodeGroupSubnets : { subnetType : ec2.SubnetType.PRIVATE_WITH_EGRESS }, launchTemplate : { // You can pass Custom Tags to Launch Templates which gets propagated to worker nodes. customTags : { \"Name\" : \"Mng1\" , \"Type\" : \"Managed-Node-Group\" , \"LaunchTemplate\" : \"Custom\" , \"Instance\" : \"ONDEMAND\" } } }, { id : \"mng2-launchtemplate\" , instanceTypes : [ new ec2 . InstanceType ( 'm5.2xlarge' )], nodeGroupCapacityType : CapacityType.SPOT , desiredSize : 0 , minSize : 0 , launchTemplate : { machineImage : ec2.MachineImage.genericLinux ({ 'us-east-1' : 'ami-08e520f5673ee0894' , 'us-west-2' : 'ami-0403ff342ceb30967' , 'us-east-2' : 'ami-07109d69738d6e1ee' , 'us-west-1' : 'ami-07bda4b61dc470985' , 'us-gov-west-1' : 'ami-0e9ebbf0d3f263e9b' , 'us-gov-east-1' : 'ami-033eb9bc6daf8bfb1' }), userData : userData , // You can pass Custom Tags to Launch Templates which gets propagated to worker nodes. customTags : { \"Name\" : \"Mng2\" , \"Type\" : \"Managed-Node-Group\" , \"LaunchTemplate\" : \"Custom\" , \"Instance\" : \"SPOT\" } } } ], fargateProfiles : { \"fp1\" : { fargateProfileName : \"fp1\" , selectors : [{ namespace : \"serverless1\" }] } } }); EksBlueprint . builder () . clusterProvider ( clusterProvider ) . build ( app , blueprintID ); The Cluster configuration and node group configuration exposes a number of options that require to supply an actual CDK resource. For example cluster allows passing mastersRole , securityGroup , etc. to the cluster, while managed node group allow specifying nodeRole . All of such cases can be solved with Resource Providers . Example: const clusterProvider = new blueprints . GenericClusterProvider ({ version : KubernetesVersion.V1_24 , // if needed use this to register an auth role to integrate with RBAC mastersRole : blueprints.getResource ( context => { return new iam . Role ( context . scope , 'AdminRole' , { assumedBy : new AccountRootPrincipal () }); }), securityGroup : blueprints.getNamedResource ( \"my-cluster-security-group\" ), // assumed to be register as a resource provider under name my-cluster-security-group managedNodeGroups : [ { id : \"mng1\" , nodeRole : blueprints.getResource ( context => { const role = new iam . Role ( context . scope , 'NodeRole' , { assumedBy : new iam . ServicePrincipal ( \"ec2.amazonaws.com\" )}); ... add policies such as AmazonEKSWorkerNodePolicy and AmazonEC2ContainerRegistryReadOnly return role ; }) } }); EksBlueprint . builder () . resourceProvider ( \"my-cluster-security-group\" , { provide ( context : blueprints.ResourceContext ) : ec2 . ISecurityGroup { return ec2 . SecurityGroup . fromSecurityGroupId ( this , 'SG' , 'sg-12345' , { mutable : false }); // example for look up } }) . clusterProvider ( clusterProvider ) . build ( app , blueprintID ); Configuration \u00b6 The GenericClusterProvider supports the following configuration options. Prop Description clusterName The name for the cluster. managedNodeGroups Zero or more managed node groups. autoscalingNodeGroups Zero or more autoscaling node groups (mutually exclusive with managed node groups). fargateProfiles Zero or more Fargate profiles. version Kubernetes version for the control plane. vpc VPC for the cluster. vpcSubnets The subnets for control plane ENIs (subnet selection). privateCluster If true Kubernetes API server is private. There should be public and private subnets for EKS cluster to work. For more information see Cluster VPC Considerations . Default configuration for managed and autoscaling node groups can also be supplied via context variables (specify in cdk.json, cdk.context.json, ~/.cdk.json or pass with -c command line option): eks.default.min-size eks.default.max-size eks.default.desired-size eks.default.instance-type eks.default.private-cluster Configuration of the EC2 parameters through context parameters makes sense if you would like to apply default configuration to multiple clusters without the need to explicitly pass individual GenericProviderClusterProps to each cluster blueprint. You can find more details on the supported configuration options in the API documentation for the GenericClusterProviderProps . Upgrading Control Plane \u00b6 Upgrading Kubernetes versions via cluster configuration at present won't impact the kubelet version running on the worker nodes. Worker nodes in-place upgrade requires explicit update of the individual node groups. The property that controls it for managed node groups is amiReleaseVersion . The following demonstrates how to do so. const clusterProvider = new blueprints . GenericClusterProvider ({ version : KubernetesVersion.V1_24 , managedNodeGroups : [ { id : \"managed-1\" , amiType : NodegroupAmiType.AL2_X86_64 , amiReleaseVersion : \"1.20.4-20210519\" } ] }); Note: consult the official EKS documentation for information ion the AMI release version that matches Kubernetes versions.","title":"Generic Cluster Provider"},{"location":"cluster-providers/generic-cluster-provider/#generic-cluster-provider","text":"The GenericClusterProvider allows you to provision an EKS cluster which leverages one or more EKS managed node groups (MNGs), or one or more autoscaling groups EC2 Auto Scaling groups for its compute capacity. Users can also configure multiple Fargate profiles along with the EC2 based compute cpacity. Today it is not possible for an Amazon EKS Cluster to propagate tags to EC2 instance worker nodes directly when you create an EKS cluster. You can create a launch template with custom tags on managedNodeGroups with GenericClusterProvider as shown in mng2-launchtemplate . This will allow you to propagate custom tags to your EC2 instance worker nodes. Note: If launchTemplate is passed with managedNodeGroups , diskSize is not allowed.","title":"Generic Cluster Provider"},{"location":"cluster-providers/generic-cluster-provider/#configuration","text":"Full list of configuration options: Generic Cluster Provider Managed Node Group Autoscaling Group Fargate Cluster","title":"Configuration"},{"location":"cluster-providers/generic-cluster-provider/#usage","text":"const clusterProvider = new blueprints . GenericClusterProvider ({ version : KubernetesVersion.V1_24 , serviceIpv4Cidr : \"10.43.0.0/16\" , // if needed use this to register an auth role integrate with RBAC mastersRole : blueprints.getResource ( context => { return new iam . Role ( context . scope , 'AdminRole' , { assumedBy : new AccountRootPrincipal () }); }), managedNodeGroups : [ { id : \"mng1\" , amiType : NodegroupAmiType.AL2_X86_64 , instanceTypes : [ new InstanceType ( 'm5.2xlarge' )], desiredSize : 2 , maxSize : 3 , nodeGroupSubnets : { subnetType : ec2.SubnetType.PRIVATE_WITH_EGRESS }, launchTemplate : { // You can pass Custom Tags to Launch Templates which gets propagated to worker nodes. customTags : { \"Name\" : \"Mng1\" , \"Type\" : \"Managed-Node-Group\" , \"LaunchTemplate\" : \"Custom\" , \"Instance\" : \"ONDEMAND\" } } }, { id : \"mng2-launchtemplate\" , instanceTypes : [ new ec2 . InstanceType ( 'm5.2xlarge' )], nodeGroupCapacityType : CapacityType.SPOT , desiredSize : 0 , minSize : 0 , launchTemplate : { machineImage : ec2.MachineImage.genericLinux ({ 'us-east-1' : 'ami-08e520f5673ee0894' , 'us-west-2' : 'ami-0403ff342ceb30967' , 'us-east-2' : 'ami-07109d69738d6e1ee' , 'us-west-1' : 'ami-07bda4b61dc470985' , 'us-gov-west-1' : 'ami-0e9ebbf0d3f263e9b' , 'us-gov-east-1' : 'ami-033eb9bc6daf8bfb1' }), userData : userData , // You can pass Custom Tags to Launch Templates which gets propagated to worker nodes. customTags : { \"Name\" : \"Mng2\" , \"Type\" : \"Managed-Node-Group\" , \"LaunchTemplate\" : \"Custom\" , \"Instance\" : \"SPOT\" } } } ], fargateProfiles : { \"fp1\" : { fargateProfileName : \"fp1\" , selectors : [{ namespace : \"serverless1\" }] } } }); EksBlueprint . builder () . clusterProvider ( clusterProvider ) . build ( app , blueprintID ); The Cluster configuration and node group configuration exposes a number of options that require to supply an actual CDK resource. For example cluster allows passing mastersRole , securityGroup , etc. to the cluster, while managed node group allow specifying nodeRole . All of such cases can be solved with Resource Providers . Example: const clusterProvider = new blueprints . GenericClusterProvider ({ version : KubernetesVersion.V1_24 , // if needed use this to register an auth role to integrate with RBAC mastersRole : blueprints.getResource ( context => { return new iam . Role ( context . scope , 'AdminRole' , { assumedBy : new AccountRootPrincipal () }); }), securityGroup : blueprints.getNamedResource ( \"my-cluster-security-group\" ), // assumed to be register as a resource provider under name my-cluster-security-group managedNodeGroups : [ { id : \"mng1\" , nodeRole : blueprints.getResource ( context => { const role = new iam . Role ( context . scope , 'NodeRole' , { assumedBy : new iam . ServicePrincipal ( \"ec2.amazonaws.com\" )}); ... add policies such as AmazonEKSWorkerNodePolicy and AmazonEC2ContainerRegistryReadOnly return role ; }) } }); EksBlueprint . builder () . resourceProvider ( \"my-cluster-security-group\" , { provide ( context : blueprints.ResourceContext ) : ec2 . ISecurityGroup { return ec2 . SecurityGroup . fromSecurityGroupId ( this , 'SG' , 'sg-12345' , { mutable : false }); // example for look up } }) . clusterProvider ( clusterProvider ) . build ( app , blueprintID );","title":"Usage"},{"location":"cluster-providers/generic-cluster-provider/#configuration_1","text":"The GenericClusterProvider supports the following configuration options. Prop Description clusterName The name for the cluster. managedNodeGroups Zero or more managed node groups. autoscalingNodeGroups Zero or more autoscaling node groups (mutually exclusive with managed node groups). fargateProfiles Zero or more Fargate profiles. version Kubernetes version for the control plane. vpc VPC for the cluster. vpcSubnets The subnets for control plane ENIs (subnet selection). privateCluster If true Kubernetes API server is private. There should be public and private subnets for EKS cluster to work. For more information see Cluster VPC Considerations . Default configuration for managed and autoscaling node groups can also be supplied via context variables (specify in cdk.json, cdk.context.json, ~/.cdk.json or pass with -c command line option): eks.default.min-size eks.default.max-size eks.default.desired-size eks.default.instance-type eks.default.private-cluster Configuration of the EC2 parameters through context parameters makes sense if you would like to apply default configuration to multiple clusters without the need to explicitly pass individual GenericProviderClusterProps to each cluster blueprint. You can find more details on the supported configuration options in the API documentation for the GenericClusterProviderProps .","title":"Configuration"},{"location":"cluster-providers/generic-cluster-provider/#upgrading-control-plane","text":"Upgrading Kubernetes versions via cluster configuration at present won't impact the kubelet version running on the worker nodes. Worker nodes in-place upgrade requires explicit update of the individual node groups. The property that controls it for managed node groups is amiReleaseVersion . The following demonstrates how to do so. const clusterProvider = new blueprints . GenericClusterProvider ({ version : KubernetesVersion.V1_24 , managedNodeGroups : [ { id : \"managed-1\" , amiType : NodegroupAmiType.AL2_X86_64 , amiReleaseVersion : \"1.20.4-20210519\" } ] }); Note: consult the official EKS documentation for information ion the AMI release version that matches Kubernetes versions.","title":"Upgrading Control Plane"},{"location":"cluster-providers/mng-cluster-provider/","text":"Managed Node Group Cluster Provider \u00b6 The MngClusterProvider allows you to provision an EKS cluster which leverages EKS managed node groups (MNGs) for compute capacity. MNGs automate the provisioning and lifecycle management of nodes (Amazon EC2 instances) for Amazon EKS Kubernetes clusters. Usage \u00b6 const props : MngClusterProviderProps = { minSize : 1 , maxSize : 10 , desiredSize : 4 , instanceTypes : [ new InstanceType ( 'm5.large' )], amiType : NodegroupAmiType.AL2_X86_64 , nodeGroupCapacityType : CapacityType.ON_DEMAND , version : KubernetesVersion.V1_24 , amiReleaseVersion : \"1.20.4-20210519\" // this will upgrade kubelet to 1.20.4 } const clusterProvider = new blueprints . MngClusterProvider ( props ); new blueprints . EksBlueprint ( scope , { id : 'blueprint' , [], [], clusterProvider }); Configuration \u00b6 The MngClusterProvider supports the following configuration options. Prop Description name The name for the cluster. @Deprecated clusterName Cluster name minSize Min cluster size, must be positive integer greater than 0 (default 1). maxSize Max cluster size, must be greater than minSize (default 3). desiredSize Desired cluster size, must be greater or equal to minSize (default min-size ). instanceTypes Type of instance for the EKS cluster, must be a valid instance type, i.e. t3.medium (default \"m5.large\") amiType The AMI type for the managed node group. amiReleaseVersion The AMI Kubernetes release version for the node group. customAmi The custom AMI and the userData for the node group, amiType and amiReleaseVersion will be ignored if this is set. nodeGroupCapacityType The capacity type for the node group (on demand or spot). vpcSubnets The subnets for the cluster. privateCluster If true Kubernetes API server is private. There should be public and private subnets for EKS cluster to work. For more information see Cluster VPC Considerations . Configuration can also be supplied via context variables (specify in cdk.json, cdk.context.json, ~/.cdk.json or pass with -c command line option): eks.default.min-size eks.default.max-size eks.default.desired-size eks.default.instance-type eks.default.private-cluster Configuration of the EC2 parameters through context parameters makes sense if you would like to apply default configuration to multiple clusters without the need to explicitly pass MngClusterProviderProps to each cluster blueprint. You can find more details on the supported configuration options in the API documentation for the MngClusterProviderProps . Upgrading Worker Nodes \u00b6 Upgrading Kubernetes versions via cluster configuration at present won't impact the kubelet version running on the worker nodes. To perform an in-place upgrade of the cluster, you must also update the amiReleaseVersion property. The following demonstrates how to do so. const props : MngClusterProviderProps = { version : KubernetesVersion.V1_24 , amiReleaseVersion : \"1.20.4-20210519\" // this will upgrade kubelet to 1.20.4 } Note: consult the official EKS documentation for information ion the AMI release version that matches Kubernetes versions. Creating Clusters with Spot Capacity Type \u00b6 To create clusters which leverage Spot capacity, set the nodeGroupCapacityType value to CapacityType.SPOT const props : MngClusterProviderProps = { nodeGroupCapacityType : CapacityType.SPOT , version : KubernetesVersion.V1_24 , instanceTypes : [ new InstanceType ( 't3.large' ), new InstanceType ( 'm5.large' )], amiReleaseVersion : \"1.20.4-20210519\" // this will upgrade kubelet to 1.20.4 } Note that two attributes in this configuration are relevant for Spot: nodeGroupCapacityType and instaceTypes . The latter indicates the types of instances which could be leveraged for Spot capacity and it makes sense to have a number of instance types to maximize availability. Creating Clusters with custom AMI for the node group \u00b6 To create clusters using custom AMI for the worker nodes, set the customAmi to your custom image and provide your userData for node bootstrapping. const userData = UserData . forLinux (); userData . addCommands ( `/etc/eks/bootstrap.sh ${ cluster . clusterName } ` ); const props : MngClusterProviderProps = { nodeGroupCapacityType : CapacityType.ON_DEMAND , version : KubernetesVersion.V1_24 , instanceTypes : [ new InstanceType ( 't3.large' )], customAmi : { machineImage : MachineImage.genericLinux ({ 'us-east-1' : 'ami-0be34337b485b2609' }), userData : userData , }, }","title":"MNG Cluster Provider"},{"location":"cluster-providers/mng-cluster-provider/#managed-node-group-cluster-provider","text":"The MngClusterProvider allows you to provision an EKS cluster which leverages EKS managed node groups (MNGs) for compute capacity. MNGs automate the provisioning and lifecycle management of nodes (Amazon EC2 instances) for Amazon EKS Kubernetes clusters.","title":"Managed Node Group Cluster Provider"},{"location":"cluster-providers/mng-cluster-provider/#usage","text":"const props : MngClusterProviderProps = { minSize : 1 , maxSize : 10 , desiredSize : 4 , instanceTypes : [ new InstanceType ( 'm5.large' )], amiType : NodegroupAmiType.AL2_X86_64 , nodeGroupCapacityType : CapacityType.ON_DEMAND , version : KubernetesVersion.V1_24 , amiReleaseVersion : \"1.20.4-20210519\" // this will upgrade kubelet to 1.20.4 } const clusterProvider = new blueprints . MngClusterProvider ( props ); new blueprints . EksBlueprint ( scope , { id : 'blueprint' , [], [], clusterProvider });","title":"Usage"},{"location":"cluster-providers/mng-cluster-provider/#configuration","text":"The MngClusterProvider supports the following configuration options. Prop Description name The name for the cluster. @Deprecated clusterName Cluster name minSize Min cluster size, must be positive integer greater than 0 (default 1). maxSize Max cluster size, must be greater than minSize (default 3). desiredSize Desired cluster size, must be greater or equal to minSize (default min-size ). instanceTypes Type of instance for the EKS cluster, must be a valid instance type, i.e. t3.medium (default \"m5.large\") amiType The AMI type for the managed node group. amiReleaseVersion The AMI Kubernetes release version for the node group. customAmi The custom AMI and the userData for the node group, amiType and amiReleaseVersion will be ignored if this is set. nodeGroupCapacityType The capacity type for the node group (on demand or spot). vpcSubnets The subnets for the cluster. privateCluster If true Kubernetes API server is private. There should be public and private subnets for EKS cluster to work. For more information see Cluster VPC Considerations . Configuration can also be supplied via context variables (specify in cdk.json, cdk.context.json, ~/.cdk.json or pass with -c command line option): eks.default.min-size eks.default.max-size eks.default.desired-size eks.default.instance-type eks.default.private-cluster Configuration of the EC2 parameters through context parameters makes sense if you would like to apply default configuration to multiple clusters without the need to explicitly pass MngClusterProviderProps to each cluster blueprint. You can find more details on the supported configuration options in the API documentation for the MngClusterProviderProps .","title":"Configuration"},{"location":"cluster-providers/mng-cluster-provider/#upgrading-worker-nodes","text":"Upgrading Kubernetes versions via cluster configuration at present won't impact the kubelet version running on the worker nodes. To perform an in-place upgrade of the cluster, you must also update the amiReleaseVersion property. The following demonstrates how to do so. const props : MngClusterProviderProps = { version : KubernetesVersion.V1_24 , amiReleaseVersion : \"1.20.4-20210519\" // this will upgrade kubelet to 1.20.4 } Note: consult the official EKS documentation for information ion the AMI release version that matches Kubernetes versions.","title":"Upgrading Worker Nodes"},{"location":"cluster-providers/mng-cluster-provider/#creating-clusters-with-spot-capacity-type","text":"To create clusters which leverage Spot capacity, set the nodeGroupCapacityType value to CapacityType.SPOT const props : MngClusterProviderProps = { nodeGroupCapacityType : CapacityType.SPOT , version : KubernetesVersion.V1_24 , instanceTypes : [ new InstanceType ( 't3.large' ), new InstanceType ( 'm5.large' )], amiReleaseVersion : \"1.20.4-20210519\" // this will upgrade kubelet to 1.20.4 } Note that two attributes in this configuration are relevant for Spot: nodeGroupCapacityType and instaceTypes . The latter indicates the types of instances which could be leveraged for Spot capacity and it makes sense to have a number of instance types to maximize availability.","title":"Creating Clusters with Spot Capacity Type"},{"location":"cluster-providers/mng-cluster-provider/#creating-clusters-with-custom-ami-for-the-node-group","text":"To create clusters using custom AMI for the worker nodes, set the customAmi to your custom image and provide your userData for node bootstrapping. const userData = UserData . forLinux (); userData . addCommands ( `/etc/eks/bootstrap.sh ${ cluster . clusterName } ` ); const props : MngClusterProviderProps = { nodeGroupCapacityType : CapacityType.ON_DEMAND , version : KubernetesVersion.V1_24 , instanceTypes : [ new InstanceType ( 't3.large' )], customAmi : { machineImage : MachineImage.genericLinux ({ 'us-east-1' : 'ami-0be34337b485b2609' }), userData : userData , }, }","title":"Creating Clusters with custom AMI for the node group"},{"location":"internal/ci/","text":"CodeBuild CI \u00b6 This example shows how to enable a CodeBuild based Continuous Integration process for the Blueprints blueprint. The CodeBuild project is provisioned using a CDK application. The buildspec.yml provided deploys the sample blueprint stacks provided in examples . The buildspec can be used directly if you wish to setup the CodeBuild project manually through the console or via the CLI. Optionally, you can also provide an S3 bucket location for a cdk.context.json that contains key-values for any context you want to provide to your application such as route 53 domain, domain account, subzone, IAM users, etc. Deploy CodeBuild Project \u00b6 First, clone this project. git clone https://github.com/aws-quickstart/cdk-eks-blueprints.git cd cdk-eks-blueprints Install CDK (please review and install any missing pre-requisites for your environment) npm install -g aws-cdk@1.104.0 Install the dependencies for this project. npm install Bootstrap CDK into the target AWS account and region. env CDK_NEW_BOOTSTRAP = 1 cdk bootstrap \\ --cloudformation-execution-policies arn:aws:iam::aws:policy/AdministratorAccess \\ aws://<ACCOUNT_ID>/<AWS_REGION> Connect GitHub organization to CodeBuild (as described here ). export GITHUB_TOKEN = <personal_access_token> aws codebuild import-source-credentials \\ --server-type GITHUB \\ --auth-type PERSONAL_ACCESS_TOKEN \\ --token $GITHUB_TOKEN Optionally, upload a cdk.context.json file into an S3 bucket which can be accessed by the CodeBuild project. aws s3 cp cdk.context.json \\ s3://<s3bucket>/cdk.context.json Deploy the CodeBuild Project. Use the --parameters GitHubOwner=<value> to override the value for owner used in the CodeBuild project. If you do not specify the input parameter we will try to use aws-quickstart by default. cdk deploy -a \"npx ts-node ci/index.ts\" \\ --parameters GitHubOwner = <GitHubOwner> \\ --context eks.default.context-location = s3://<s3bucket>/cdk.context.json \" After the deployment is completed the CodeBuild project will be configured to build and deploy the blueprint stack on every pull-request merge to the main branch. Note : Update build badge url the top level README . The badge url can be obtained by running the following AWS cli command. aws codebuild batch-get-projects \\ --names QuickstartSspAmazonEksBuild | \\ jq -r '.projects[0].badge.badgeRequestUrl'","title":"CodeBuild CI"},{"location":"internal/ci/#codebuild-ci","text":"This example shows how to enable a CodeBuild based Continuous Integration process for the Blueprints blueprint. The CodeBuild project is provisioned using a CDK application. The buildspec.yml provided deploys the sample blueprint stacks provided in examples . The buildspec can be used directly if you wish to setup the CodeBuild project manually through the console or via the CLI. Optionally, you can also provide an S3 bucket location for a cdk.context.json that contains key-values for any context you want to provide to your application such as route 53 domain, domain account, subzone, IAM users, etc.","title":"CodeBuild CI"},{"location":"internal/ci/#deploy-codebuild-project","text":"First, clone this project. git clone https://github.com/aws-quickstart/cdk-eks-blueprints.git cd cdk-eks-blueprints Install CDK (please review and install any missing pre-requisites for your environment) npm install -g aws-cdk@1.104.0 Install the dependencies for this project. npm install Bootstrap CDK into the target AWS account and region. env CDK_NEW_BOOTSTRAP = 1 cdk bootstrap \\ --cloudformation-execution-policies arn:aws:iam::aws:policy/AdministratorAccess \\ aws://<ACCOUNT_ID>/<AWS_REGION> Connect GitHub organization to CodeBuild (as described here ). export GITHUB_TOKEN = <personal_access_token> aws codebuild import-source-credentials \\ --server-type GITHUB \\ --auth-type PERSONAL_ACCESS_TOKEN \\ --token $GITHUB_TOKEN Optionally, upload a cdk.context.json file into an S3 bucket which can be accessed by the CodeBuild project. aws s3 cp cdk.context.json \\ s3://<s3bucket>/cdk.context.json Deploy the CodeBuild Project. Use the --parameters GitHubOwner=<value> to override the value for owner used in the CodeBuild project. If you do not specify the input parameter we will try to use aws-quickstart by default. cdk deploy -a \"npx ts-node ci/index.ts\" \\ --parameters GitHubOwner = <GitHubOwner> \\ --context eks.default.context-location = s3://<s3bucket>/cdk.context.json \" After the deployment is completed the CodeBuild project will be configured to build and deploy the blueprint stack on every pull-request merge to the main branch. Note : Update build badge url the top level README . The badge url can be obtained by running the following AWS cli command. aws codebuild batch-get-projects \\ --names QuickstartSspAmazonEksBuild | \\ jq -r '.projects[0].badge.badgeRequestUrl'","title":"Deploy CodeBuild Project"},{"location":"internal/input-validations-framework-readme/","text":"How to use the framework \u00b6 The constraints framework implementation is located in the utils/constraints-utils.ts module import * from 'utils/constraints-utils' ; What can you use with the framework \u00b6 The constraints framework provides a set of generic classes and interfaces as well as the invocation framework to validate constraints on arbitrary objects. Validations in constraints-utils.ts \u00b6 This file holds the supported constraints and function(s) to validate constraints defined below in the rest of this document. StringConstraint \u00b6 Constructor: new StringConstraint ( minValue , maxValue ); API reference 'here' If given string length falls outside of these inclusive bounds throws detailed Zod error UrlStringConstraint \u00b6 Constructor: new UrlStringConstraint ( minValue , maxValue ); API reference 'here' If given string length falls outside of these inclusive bounds, or does not follow a proper URL format it throws detailed Zod error NumberConstraint \u00b6 Constructor: new NumberConstraint ( minValue , maxValue ); API reference 'here' If given number falls outside of these inclusive bounds throws detailed Zod error. ArrayConstraint \u00b6 Constructor: new utils . ArrayConstraint ( minValue , maxValue ); API reference 'here' If given array length falls outside of these inclusive bounds throws detailed Zod error. validateConstraints Function \u00b6 validateConstraints < T > ( constraints : ConstraintsType < T > , context : string , ...object : any ) This is the entry point to use the framework. This function can validate either a single object or an array of objects against the provided constraints. How to use the constraints-utils.ts \u00b6 You need two things when utilizing constraints-utils.ts and the following examples are from 'here' First you need a class with specified keys assigned to given constraints. Example with two keys: export class BlueprintPropsConstraints implements ConstraintsType < EksBlueprintProps > { id = new StringConstraint ( 1 , 63 ); name = new StringConstraint ( 1 , 63 ); Second you need to call the validateConstraints function: Example (note: punctuation, formatting): validateConstraints ( new BlueprintPropsConstraints , EksBlueprintProps . name , blueprintProps ); Limitations \u00b6 Currently, constraints can be defined for flat objects, and nested structures will require individual validations.","title":"Input Validation Framework"},{"location":"internal/input-validations-framework-readme/#how-to-use-the-framework","text":"The constraints framework implementation is located in the utils/constraints-utils.ts module import * from 'utils/constraints-utils' ;","title":"How to use the framework"},{"location":"internal/input-validations-framework-readme/#what-can-you-use-with-the-framework","text":"The constraints framework provides a set of generic classes and interfaces as well as the invocation framework to validate constraints on arbitrary objects.","title":"What can you use with the framework"},{"location":"internal/input-validations-framework-readme/#validations-in-constraints-utilsts","text":"This file holds the supported constraints and function(s) to validate constraints defined below in the rest of this document.","title":"Validations in constraints-utils.ts"},{"location":"internal/input-validations-framework-readme/#stringconstraint","text":"Constructor: new StringConstraint ( minValue , maxValue ); API reference 'here' If given string length falls outside of these inclusive bounds throws detailed Zod error","title":"StringConstraint"},{"location":"internal/input-validations-framework-readme/#urlstringconstraint","text":"Constructor: new UrlStringConstraint ( minValue , maxValue ); API reference 'here' If given string length falls outside of these inclusive bounds, or does not follow a proper URL format it throws detailed Zod error","title":"UrlStringConstraint"},{"location":"internal/input-validations-framework-readme/#numberconstraint","text":"Constructor: new NumberConstraint ( minValue , maxValue ); API reference 'here' If given number falls outside of these inclusive bounds throws detailed Zod error.","title":"NumberConstraint"},{"location":"internal/input-validations-framework-readme/#arrayconstraint","text":"Constructor: new utils . ArrayConstraint ( minValue , maxValue ); API reference 'here' If given array length falls outside of these inclusive bounds throws detailed Zod error.","title":"ArrayConstraint"},{"location":"internal/input-validations-framework-readme/#validateconstraints-function","text":"validateConstraints < T > ( constraints : ConstraintsType < T > , context : string , ...object : any ) This is the entry point to use the framework. This function can validate either a single object or an array of objects against the provided constraints.","title":"validateConstraints Function"},{"location":"internal/input-validations-framework-readme/#how-to-use-the-constraints-utilsts","text":"You need two things when utilizing constraints-utils.ts and the following examples are from 'here' First you need a class with specified keys assigned to given constraints. Example with two keys: export class BlueprintPropsConstraints implements ConstraintsType < EksBlueprintProps > { id = new StringConstraint ( 1 , 63 ); name = new StringConstraint ( 1 , 63 ); Second you need to call the validateConstraints function: Example (note: punctuation, formatting): validateConstraints ( new BlueprintPropsConstraints , EksBlueprintProps . name , blueprintProps );","title":"How to use the constraints-utils.ts"},{"location":"internal/input-validations-framework-readme/#limitations","text":"Currently, constraints can be defined for flat objects, and nested structures will require individual validations.","title":"Limitations"},{"location":"internal/readme-internal/","text":"Description \u00b6 This is an internal readme for development processes that should be followed for this repository. Local Development \u00b6 This project leverage Makefiles for project automation. We currently support the following commands. Lint the project with ESLint . make lint Build the project with Typescript . make build. Submitting Pull Requests \u00b6 The below instructions apply regardless of whether PR is submitted from a fork or a branch. Make sure you IDE is configured to format modified lines only. Submitting fully formatted files makes it very hard to review, and such will be rejected. Tab size is 4 and spaces (be mindful that VSCode may not be the only IDE used). he following commands produce no errors and/or warnings: npm i make build make lint make run-test cdk list Triggering E2E Testing \u00b6 The CI system attached to the project will run all stacks under examples as end-to-end integration testing. Currently it works the following way: A human maintainer reviews the pr code to ensure it is not malicious If the code is trusted and the maintainer wishes to run e2e tests, they comment on the pr with /do-e2e-tests. This will trigger the build and test. Any visibility into the state can only occur through AWS maintainers. If job succeeds, the CI bot approves the PR. If it fails it requests changes. Details on what failed will need to be manually shared with external contributors. At present shapirov103, kcoleman731 and askulkarni2 have rights to invoke the bot. Publishing \u00b6 At the moment leveraging a private NPM repository for \"shapirov\". TODO: move under aws-labs. Change version in package.json. We are currently using . . , e.g. 0.1.5 Patch version increment must be used for bug fixes, including changes in code and missing documentation. Minor version is used for new features that do not change the way customers interact with the solution. For example, new add-on, extra configuration (optional) for existing add-ons. In some cases it may be used with CDK version upgrades provided they don't cause code changes. Major version is used for non-compatible changes that will require customers to re-arch. With the exception of version 1. which will be used once the code is production ready (we have tests, pipeline, validation). Publishing (if not applied through CI): make build (compile) npm publish (this will require credentials to npm) Submitting Changes \u00b6 For direct contributors: 1. Create a feature branch and commit to that branch. 2. Create PR to the main branch. 3. After review if approved changes will be merged. For external contributors: 1. Create a fork of the repository 2. Submit a PR with the following: 1. Clear description of the feature 2. Test coverage 3. Validation instructions Validations framework link to readme \u00b6 See this document for more details.","title":"Readme"},{"location":"internal/readme-internal/#description","text":"This is an internal readme for development processes that should be followed for this repository.","title":"Description"},{"location":"internal/readme-internal/#local-development","text":"This project leverage Makefiles for project automation. We currently support the following commands. Lint the project with ESLint . make lint Build the project with Typescript . make build.","title":"Local Development"},{"location":"internal/readme-internal/#submitting-pull-requests","text":"The below instructions apply regardless of whether PR is submitted from a fork or a branch. Make sure you IDE is configured to format modified lines only. Submitting fully formatted files makes it very hard to review, and such will be rejected. Tab size is 4 and spaces (be mindful that VSCode may not be the only IDE used). he following commands produce no errors and/or warnings: npm i make build make lint make run-test cdk list","title":"Submitting Pull Requests"},{"location":"internal/readme-internal/#triggering-e2e-testing","text":"The CI system attached to the project will run all stacks under examples as end-to-end integration testing. Currently it works the following way: A human maintainer reviews the pr code to ensure it is not malicious If the code is trusted and the maintainer wishes to run e2e tests, they comment on the pr with /do-e2e-tests. This will trigger the build and test. Any visibility into the state can only occur through AWS maintainers. If job succeeds, the CI bot approves the PR. If it fails it requests changes. Details on what failed will need to be manually shared with external contributors. At present shapirov103, kcoleman731 and askulkarni2 have rights to invoke the bot.","title":"Triggering E2E Testing"},{"location":"internal/readme-internal/#publishing","text":"At the moment leveraging a private NPM repository for \"shapirov\". TODO: move under aws-labs. Change version in package.json. We are currently using . . , e.g. 0.1.5 Patch version increment must be used for bug fixes, including changes in code and missing documentation. Minor version is used for new features that do not change the way customers interact with the solution. For example, new add-on, extra configuration (optional) for existing add-ons. In some cases it may be used with CDK version upgrades provided they don't cause code changes. Major version is used for non-compatible changes that will require customers to re-arch. With the exception of version 1. which will be used once the code is production ready (we have tests, pipeline, validation). Publishing (if not applied through CI): make build (compile) npm publish (this will require credentials to npm)","title":"Publishing"},{"location":"internal/readme-internal/#submitting-changes","text":"For direct contributors: 1. Create a feature branch and commit to that branch. 2. Create PR to the main branch. 3. After review if approved changes will be merged. For external contributors: 1. Create a fork of the repository 2. Submit a PR with the following: 1. Clear description of the feature 2. Test coverage 3. Validation instructions","title":"Submitting Changes"},{"location":"internal/readme-internal/#validations-framework-link-to-readme","text":"See this document for more details.","title":"Validations framework link to readme"},{"location":"resource-providers/","text":"Resource Providers \u00b6 Terminology \u00b6 Resource A resource is a CDK construct that implements IResource interface from aws-cdk-lib which is a generic interface for any AWS resource. An example of a resource could be a hosted zone in Route53 IHostedZone , an ACM certificate ICertificate , a VPC or even a DynamoDB table which could be leveraged either in add-ons or teams. ResourceProvider A resource provider is a core Blueprints concept that enables customers to supply resources for add-ons, teams and/or post-deployment steps. Resources may be imported (e.g., if created outside of the platform) or created with the blueprint. Use Cases \u00b6 ClusterAddOn and Team implementations require AWS resources that can be shared across several constructs. For example, ExternalDnsAddOn requires an array of hosted zones that will be used for integration with Route53. NginxAddOn requires a certificate and hosted zone (for DNS validation) in order to use TLS termination. VPC may be used inside add-ons and team constructs to look up VPC CIDR and subnets. The Blueprints framework provides ability to register a resource provider under an arbitrary name and make it available in the resource context, which is available to all add-ons and teams. With this capability, customers can either use existing resource providers or create their own and reference the provided resources inside add-ons, teams or other resource providers. Resource providers may depend on resources provided by other resource providers. For example, CertificateResourceProvider relies on a hosted zone resource, which is expected to be supplied by another provider. Example use cases: As a platform user, I must create a VPC using my enterprise standards and leverage it for the EKS Blueprint. Solution: create an implementation of ResourceProvider<IVpc> (or leverage an existing one) and register it with the blueprint (see Usage). As a platform user, I need to use an existing hosted zone for all external DNS names used with ingress objects of my workloads. Solution: use a predefined ImportHostedZoneProvider or LookupHostedZoneProvider to reference the existing hosted zone. As a platform user, I need to create an S3 bucket and use it in one or more Team implementations. Solution: create an implementation for an S3 Bucket resource provider and use the supplied resource inside teams. Contracts \u00b6 The API contract for a resource provider is represented by the ResourceProvider interface from the spi/resource-contracts module. export declare interface ResourceProvider < T extends IResource = IResource > { provide ( context : ResourceContext ) : T ; } Example implementations: class VpcResourceProvider implements ResourceProvider < IVpc > { provide ( context : ResourceContext ) : IVpc { const scope = context . scope ; // stack ... } } class DynamoDbTableResourceProvider implements ResourceProvider < ITable > { provide ( context : ResourceContext ) : ITable { ... } } Access to registered resources from other resource providers and/or add-ons and teams: /** * Provides API to register resource providers and get access to the provided resources. */ export class ResourceContext { /** * Adds a new resource provider and specifies the name under which the provided resource will be registered, * @param name Specifies the name key under which the provided resources will be registered for subsequent look-ups. * @param provider Implementation of the resource provider interface * @returns the provided resource */ public add < T extends cdk . IResource = cdk . IResource > ( name : string , provider : ResourceProvider < T > ) : T { ... } /** * Gets the provided resource by the supplied name. * @param name under which the resource provider was registered * @returns the resource or undefined if the specified resource was not found */ public get < T extends cdk . IResource = cdk . IResource > ( name : string ) : T | undefined { ... } } Convenience API to access registered resources from add-ons: /** * Cluster info supplies all contextual information on the cluster configuration, registered resources and add-ons * which could be leveraged by the framework, add-on implementations and teams. */ export class ClusterInfo { ... /** * Provides the resource context object associated with this instance of the EKS Blueprint. * @returns resource context object */ public getResourceContext () : ResourceContext { return this . resourceContext ; } /** * Provides the resource registered under supplied name * @param name of the resource to be returned * @returns Resource object or undefined if no resource was found */ public getResource < T extends cdk . IResource > ( name : string ) : T | undefined { ... } /** * Same as {@link getResource} but will fail if the specified resource is not found * @param name of the resource to be returned * @returns Resource object (fails if not found) */ public getRequiredResource < T extends cdk . IResource > ( name : string ) : T { ... } } Usage \u00b6 Registering Resource Providers for a Blueprint Note: GlobalResources.HostedZone and GlobalResources.Certificate are provided for convenience as commonly referenced constants. const myVpcId = ...; // e.g. app.node.tryGetContext('my-vpc', 'default) will look up property my-vpc in the cdk.json blueprints . EksBlueprint . builder () // Specify VPC for the cluster (if not set, a new VPC will be provisioned as per EKS Best Practices) . resourceProvider ( GlobalResources . VPC , new VpcProvider ( myVpcId ) // Specify KMS Key as cluster secrets encryption key . resourceProvider ( GlobalResources . KmsKey , new CreateKmsKeyProvider ( 'my-alias-name' ) // Register hosted zone and give it a name of GlobalResources.HostedZone . resourceProvider ( GlobalResources . HostedZone , new ImportHostedZoneProvider ( 'hosted-zone-id1' , 'my.domain.com' )) . resourceProvider ( \"internal-hosted-zone\" , new ImportHostedZoneProvider ( 'hosted-zone-id2' , 'myinternal.domain.com' )) // Register certificate GlobalResources.Certificate name and reference the hosted zone registered in the previous step . resourceProvider ( GlobalResources . Certificate , new CreateCertificateProvider ( 'domain-wildcard-cert' , '*.my.domain.com' , GlobalResources . HostedZone )) . resourceProvider ( \"private-ca\" , new CreateCertificateProvider ( 'internal-wildcard-cert' , '*.myinternal.domain.com' , \"internal-hosted-zone\" )) // Create EFS file system and register it under the name of efs-file-system . resourceProvider ( \"efs-file-system\" , new CreateEfsFileSystemProvider ( 'efs-file-system' )) . addOns ( new AwsLoadBalancerControllerAddOn ()) // Use hosted zone for External DNS . addOns ( new ExternalDnsAddOn ({ hostedZoneResources : [ GlobalResources . HostedZone ]})) // Use certificate registered before with NginxAddon . addOns ( new NginxAddOn ({ certificateResourceName : GlobalResources.Certificate , externalDnsHostname : 'my.domain.com' })) . teams (...) . build ( app , 'stack-with-resource-providers' ); Registering Multiple Hosted Zones blueprints . EksBlueprint . builder () // Register hosted zone1 under the name of MyHostedZone1 . resourceProvider ( \"MyHostedZone1\" , new ImportHostedZoneProvider ( 'hosted-zone-id1' , 'my.domain.com' )) // Register zone2 under the name of MyHostedZone2 . resourceProvider ( \"MyHostedZone2\" , new ImportHostedZoneProvider ( 'hosted-zone-id2' , 'my.otherdomain.com' )) // Register certificate and reference the hosted zone1 registered in the previous steps . resourceProvider ( \"MyCert\" , new CreateCertificateProvider ( 'domain-wildcard-cert' , '*.my.domain.com' , \"MyHostedZone1\" )) . addOns ( new AwsLoadBalancerControllerAddOn ()) // Use hosted zones for External DNS . addOns ( new ExternalDnsAddOn ({ hostedZoneResources : [ \"MyHostedZone1\" , \"MyHostedZone2\" ]})) // Use certificate registered before with NginxAddon . addOns ( new NginxAddOn ({ certificateResourceName : \"MyCert\" , externalDnsHostname : 'my.domain.com' })) . teams (...) . build ( app , 'stack-with-resource-providers' ); Using Resource Providers with CDK Constructs \u00b6 Some constructs used in the EKSBlueprint stack are standard CDK constructs that accept CDK resources. For example, GenericClusterProvider (which is the basis for all cluster providers) allows passing resources like IRole , SecurityGroup and other properties that customers may find inconvenient to define with a builder pattern. Blueprints provide a convenience API to register such resources in a declarative manner. Example with an anonymous resource: const clusterProvider = new blueprints . GenericClusterProvider ({ version : KubernetesVersion.V1_24 , mastersRole : blueprints.getResource ( context => { // will generate a unique name for resource. designed for cases when resource is defined once and needed in a single place. return new iam . Role ( context . scope , 'AdminRole' , { assumedBy : new AccountRootPrincipal () }); }), managedNodeGroups : [ ... ] }); blueprints . EksBlueprint . builder () . addOns (... addOns ) . clusterProvider ( clusterProvider ) . build ( scope , blueprintID , props ); } Example with a named resource: const clusterProvider = new blueprints . GenericClusterProvider ({ version : KubernetesVersion.V1_24 , mastersRole : blueprints.getNamedResource ( \"my-role\" ), managedNodeGroups : [ ... ] }); blueprints . EksBlueprint . builder () . resourceProvider ( \"my-role\" , new blueprints . LookupRoleProvider ( \"SomeExistingRole\" )) // enables to look up this role from ClusterInfo under \"my-role\" in add-ons, etc. . addOns (... addOns ) . clusterProvider ( clusterProvider ) . build ( scope , blueprintID , props ); } Implementing Custom Resource Providers \u00b6 Select the type of the resource that you need. Let's say it will be an S3 Bucket. Note: it must be one of the derivatives/implementations of IResource interface. Implement ResourceProvider interface: class MyResourceProvider implements blueprints . ResourceProvider < s3 . IBucket > { provide ( context : blueprints.ResourceContext ) : s3 . IBucket { return new s3 . Bucket ( context . scope , \"mybucket\" ); } } Register your resource provider under an arbitrary name which must be unique in the current scope across all resource providers: blueprints . EksBlueprint . builder () . resourceProvider ( \"mybucket\" , new MyResourceProvider ()) . addOns (...) . teams (...) . build (); Use the resource inside a custom add-on: class MyCustomAddOn implements blueprints . ClusterAddOn { deploy ( clusterInfo : ClusterInfo ) : void | Promise < cdk . Construct > { const myBucket : s3.IBucket = clusterInfo . getRequiredResource ( 'mybucket' ); // will fail if mybucket does not exist // do something with the bucket } }","title":"Resource Providers"},{"location":"resource-providers/#resource-providers","text":"","title":"Resource Providers"},{"location":"resource-providers/#terminology","text":"Resource A resource is a CDK construct that implements IResource interface from aws-cdk-lib which is a generic interface for any AWS resource. An example of a resource could be a hosted zone in Route53 IHostedZone , an ACM certificate ICertificate , a VPC or even a DynamoDB table which could be leveraged either in add-ons or teams. ResourceProvider A resource provider is a core Blueprints concept that enables customers to supply resources for add-ons, teams and/or post-deployment steps. Resources may be imported (e.g., if created outside of the platform) or created with the blueprint.","title":"Terminology"},{"location":"resource-providers/#use-cases","text":"ClusterAddOn and Team implementations require AWS resources that can be shared across several constructs. For example, ExternalDnsAddOn requires an array of hosted zones that will be used for integration with Route53. NginxAddOn requires a certificate and hosted zone (for DNS validation) in order to use TLS termination. VPC may be used inside add-ons and team constructs to look up VPC CIDR and subnets. The Blueprints framework provides ability to register a resource provider under an arbitrary name and make it available in the resource context, which is available to all add-ons and teams. With this capability, customers can either use existing resource providers or create their own and reference the provided resources inside add-ons, teams or other resource providers. Resource providers may depend on resources provided by other resource providers. For example, CertificateResourceProvider relies on a hosted zone resource, which is expected to be supplied by another provider. Example use cases: As a platform user, I must create a VPC using my enterprise standards and leverage it for the EKS Blueprint. Solution: create an implementation of ResourceProvider<IVpc> (or leverage an existing one) and register it with the blueprint (see Usage). As a platform user, I need to use an existing hosted zone for all external DNS names used with ingress objects of my workloads. Solution: use a predefined ImportHostedZoneProvider or LookupHostedZoneProvider to reference the existing hosted zone. As a platform user, I need to create an S3 bucket and use it in one or more Team implementations. Solution: create an implementation for an S3 Bucket resource provider and use the supplied resource inside teams.","title":"Use Cases"},{"location":"resource-providers/#contracts","text":"The API contract for a resource provider is represented by the ResourceProvider interface from the spi/resource-contracts module. export declare interface ResourceProvider < T extends IResource = IResource > { provide ( context : ResourceContext ) : T ; } Example implementations: class VpcResourceProvider implements ResourceProvider < IVpc > { provide ( context : ResourceContext ) : IVpc { const scope = context . scope ; // stack ... } } class DynamoDbTableResourceProvider implements ResourceProvider < ITable > { provide ( context : ResourceContext ) : ITable { ... } } Access to registered resources from other resource providers and/or add-ons and teams: /** * Provides API to register resource providers and get access to the provided resources. */ export class ResourceContext { /** * Adds a new resource provider and specifies the name under which the provided resource will be registered, * @param name Specifies the name key under which the provided resources will be registered for subsequent look-ups. * @param provider Implementation of the resource provider interface * @returns the provided resource */ public add < T extends cdk . IResource = cdk . IResource > ( name : string , provider : ResourceProvider < T > ) : T { ... } /** * Gets the provided resource by the supplied name. * @param name under which the resource provider was registered * @returns the resource or undefined if the specified resource was not found */ public get < T extends cdk . IResource = cdk . IResource > ( name : string ) : T | undefined { ... } } Convenience API to access registered resources from add-ons: /** * Cluster info supplies all contextual information on the cluster configuration, registered resources and add-ons * which could be leveraged by the framework, add-on implementations and teams. */ export class ClusterInfo { ... /** * Provides the resource context object associated with this instance of the EKS Blueprint. * @returns resource context object */ public getResourceContext () : ResourceContext { return this . resourceContext ; } /** * Provides the resource registered under supplied name * @param name of the resource to be returned * @returns Resource object or undefined if no resource was found */ public getResource < T extends cdk . IResource > ( name : string ) : T | undefined { ... } /** * Same as {@link getResource} but will fail if the specified resource is not found * @param name of the resource to be returned * @returns Resource object (fails if not found) */ public getRequiredResource < T extends cdk . IResource > ( name : string ) : T { ... } }","title":"Contracts"},{"location":"resource-providers/#usage","text":"Registering Resource Providers for a Blueprint Note: GlobalResources.HostedZone and GlobalResources.Certificate are provided for convenience as commonly referenced constants. const myVpcId = ...; // e.g. app.node.tryGetContext('my-vpc', 'default) will look up property my-vpc in the cdk.json blueprints . EksBlueprint . builder () // Specify VPC for the cluster (if not set, a new VPC will be provisioned as per EKS Best Practices) . resourceProvider ( GlobalResources . VPC , new VpcProvider ( myVpcId ) // Specify KMS Key as cluster secrets encryption key . resourceProvider ( GlobalResources . KmsKey , new CreateKmsKeyProvider ( 'my-alias-name' ) // Register hosted zone and give it a name of GlobalResources.HostedZone . resourceProvider ( GlobalResources . HostedZone , new ImportHostedZoneProvider ( 'hosted-zone-id1' , 'my.domain.com' )) . resourceProvider ( \"internal-hosted-zone\" , new ImportHostedZoneProvider ( 'hosted-zone-id2' , 'myinternal.domain.com' )) // Register certificate GlobalResources.Certificate name and reference the hosted zone registered in the previous step . resourceProvider ( GlobalResources . Certificate , new CreateCertificateProvider ( 'domain-wildcard-cert' , '*.my.domain.com' , GlobalResources . HostedZone )) . resourceProvider ( \"private-ca\" , new CreateCertificateProvider ( 'internal-wildcard-cert' , '*.myinternal.domain.com' , \"internal-hosted-zone\" )) // Create EFS file system and register it under the name of efs-file-system . resourceProvider ( \"efs-file-system\" , new CreateEfsFileSystemProvider ( 'efs-file-system' )) . addOns ( new AwsLoadBalancerControllerAddOn ()) // Use hosted zone for External DNS . addOns ( new ExternalDnsAddOn ({ hostedZoneResources : [ GlobalResources . HostedZone ]})) // Use certificate registered before with NginxAddon . addOns ( new NginxAddOn ({ certificateResourceName : GlobalResources.Certificate , externalDnsHostname : 'my.domain.com' })) . teams (...) . build ( app , 'stack-with-resource-providers' ); Registering Multiple Hosted Zones blueprints . EksBlueprint . builder () // Register hosted zone1 under the name of MyHostedZone1 . resourceProvider ( \"MyHostedZone1\" , new ImportHostedZoneProvider ( 'hosted-zone-id1' , 'my.domain.com' )) // Register zone2 under the name of MyHostedZone2 . resourceProvider ( \"MyHostedZone2\" , new ImportHostedZoneProvider ( 'hosted-zone-id2' , 'my.otherdomain.com' )) // Register certificate and reference the hosted zone1 registered in the previous steps . resourceProvider ( \"MyCert\" , new CreateCertificateProvider ( 'domain-wildcard-cert' , '*.my.domain.com' , \"MyHostedZone1\" )) . addOns ( new AwsLoadBalancerControllerAddOn ()) // Use hosted zones for External DNS . addOns ( new ExternalDnsAddOn ({ hostedZoneResources : [ \"MyHostedZone1\" , \"MyHostedZone2\" ]})) // Use certificate registered before with NginxAddon . addOns ( new NginxAddOn ({ certificateResourceName : \"MyCert\" , externalDnsHostname : 'my.domain.com' })) . teams (...) . build ( app , 'stack-with-resource-providers' );","title":"Usage"},{"location":"resource-providers/#using-resource-providers-with-cdk-constructs","text":"Some constructs used in the EKSBlueprint stack are standard CDK constructs that accept CDK resources. For example, GenericClusterProvider (which is the basis for all cluster providers) allows passing resources like IRole , SecurityGroup and other properties that customers may find inconvenient to define with a builder pattern. Blueprints provide a convenience API to register such resources in a declarative manner. Example with an anonymous resource: const clusterProvider = new blueprints . GenericClusterProvider ({ version : KubernetesVersion.V1_24 , mastersRole : blueprints.getResource ( context => { // will generate a unique name for resource. designed for cases when resource is defined once and needed in a single place. return new iam . Role ( context . scope , 'AdminRole' , { assumedBy : new AccountRootPrincipal () }); }), managedNodeGroups : [ ... ] }); blueprints . EksBlueprint . builder () . addOns (... addOns ) . clusterProvider ( clusterProvider ) . build ( scope , blueprintID , props ); } Example with a named resource: const clusterProvider = new blueprints . GenericClusterProvider ({ version : KubernetesVersion.V1_24 , mastersRole : blueprints.getNamedResource ( \"my-role\" ), managedNodeGroups : [ ... ] }); blueprints . EksBlueprint . builder () . resourceProvider ( \"my-role\" , new blueprints . LookupRoleProvider ( \"SomeExistingRole\" )) // enables to look up this role from ClusterInfo under \"my-role\" in add-ons, etc. . addOns (... addOns ) . clusterProvider ( clusterProvider ) . build ( scope , blueprintID , props ); }","title":"Using Resource Providers with CDK Constructs"},{"location":"resource-providers/#implementing-custom-resource-providers","text":"Select the type of the resource that you need. Let's say it will be an S3 Bucket. Note: it must be one of the derivatives/implementations of IResource interface. Implement ResourceProvider interface: class MyResourceProvider implements blueprints . ResourceProvider < s3 . IBucket > { provide ( context : blueprints.ResourceContext ) : s3 . IBucket { return new s3 . Bucket ( context . scope , \"mybucket\" ); } } Register your resource provider under an arbitrary name which must be unique in the current scope across all resource providers: blueprints . EksBlueprint . builder () . resourceProvider ( \"mybucket\" , new MyResourceProvider ()) . addOns (...) . teams (...) . build (); Use the resource inside a custom add-on: class MyCustomAddOn implements blueprints . ClusterAddOn { deploy ( clusterInfo : ClusterInfo ) : void | Promise < cdk . Construct > { const myBucket : s3.IBucket = clusterInfo . getRequiredResource ( 'mybucket' ); // will fail if mybucket does not exist // do something with the bucket } }","title":"Implementing Custom Resource Providers"},{"location":"teams/aws-batch-on-eks-team/","text":"AWS Batch on EKS Team \u00b6 The AWS Batch on EKS Team extends the ApplicationTeam and allows the Batch on EKS team to manage the namespace where the Batch Jobs are deployed. This team MUST be used in conjuction with EMR on EKS AddOn . The AWS Batch on EKS Team allows you to create a Compute Environment and a job queue to attach to the compute environment. Job queues are where jobs are submitted, and where they reside until they can be scheduled in the compute environment. Usage \u00b6 import 'source-map-support/register' ; import * as cdk from 'aws-cdk-lib' ; import * as blueprints from '@aws-quickstart/eks-blueprints' ; const app = new cdk . App (); const addOn = new blueprints . addons . AwsBatchAddOn (); const batchTeam : BatchEksTeamProps = { name : 'batch-a' , namespace : 'aws-batch' , envName : 'batch-a-comp-env' , computeResources : { envType : BatchEnvType.EC2 , allocationStrategy : BatchAllocationStrategy.BEST , priority : 10 , minvCpus : 0 , maxvCpus : 128 , instanceTypes : [ \"m5\" , \"t3.large\" ] }, jobQueueName : 'team-a-job-queue' , }; const blueprint = blueprints . EksBlueprint . builder () . addOns ( addOn ) . teams ( new blueprints . BatchEksTeam ( batchTeam )) . build ( app , 'my-stack-name' ); Create a Job definition \u00b6 Once you deploy the addon and the team, to run a batch job on EKS, you must first define a job. AWS Batch job definitions specify how jobs are to be run. The following is an example job definition you can set using AWS CLI: cat <<EOF > ./batch-eks-job-definition.json { \"jobDefinitionName\": \"MyJobOnEks_Sleep\", \"type\": \"container\", \"eksProperties\": { \"podProperties\": { \"hostNetwork\": true, \"containers\": [ { \"image\": \"public.ecr.aws/amazonlinux/amazonlinux:2\", \"command\": [ \"sleep\", \"60\" ], \"resources\": { \"limits\": { \"cpu\": \"1\", \"memory\": \"1024Mi\" } } } ] } } } EOF aws batch register-job-definition --cli-input-json file://./batch-eks-job-definition.json Submit a Job \u00b6 Using the job definition, you can define and deploy a specific job using the following example AWS CLI command: aws batch submit-job --job-queue team-a-job-queue \\ --job-definition MyJobOnEks_Sleep --job-name My-Eks-Job1 You will get an output that lists the Job ID: { \"jobArn\": \"arn:aws:batch:us-west-2:123456789012:job/9518c9eb-b261-4732-a38d-54caf1d22229\", \"jobName\": \"My-Eks-Job1\", \"jobId\": \"9518c9eb-b261-4732-a38d-54caf1d22229\" } Verify Job completion \u00b6 You can see the job by running the following command: aws batch describe-jobs --job <jobId-from-submit-response> { \"jobs\": [ { \"jobArn\": \"arn:aws:batch:us-west-2:123456789012:job/9518c9eb-b261-4732-a38d-54caf1d22229\", \"jobName\": \"My-Eks-Job1\", \"jobId\": \"9518c9eb-b261-4732-a38d-54caf1d22229\", \"jobQueue\": \"arn:aws:batch:us-west-2:123456789012:job-queue/team-a-job-queue\", \"status\": \"RUNNABLE\", \"attempts\": [], \"createdAt\": 1676581820093, \"dependsOn\": [], \"jobDefinition\": \"arn:aws:batch:us-west-2:123456789012:job-definition/MyJobOnEks_Sleep:1\", \"parameters\": {}, \"tags\": {}, \"platformCapabilities\": [], \"eksProperties\": { \"podProperties\": { \"hostNetwork\": true, \"containers\": [ { \"image\": \"public.ecr.aws/amazonlinux/amazonlinux:2\", \"command\": [ \"sleep\", \"60\" ], \"args\": [], \"env\": [], \"resources\": { \"limits\": { \"memory\": \"1024Mi\", \"cpu\": \"1\" } }, \"volumeMounts\": [] } ], \"volumes\": [] } }, \"eksAttempts\": [] } ] } After a while, you can check that the pod has been created (and eventually deleted after job completion) under the aws-batch namespace to run the job: kubectl get pod -n aws-batch You can also check that the job has been completed by running the describe job command again and seeing the output. There should be description of the pod and node assignment for the job under eksAttempts : ....................... \"eksAttempts\": [ { \"containers\": [ { \"exitCode\": 0, \"reason\": \"Completed\" } ], \"podName\": \"aws-batch.fa024ec9-4232-3e82-b09b-4ba6ba396ec2\", \"nodeName\": \"ip-10-0-81-102.us-west-2.compute.internal\", \"startedAt\": 1676581976000, \"stoppedAt\": 1676582036000 } ] .......................","title":"AWS Batch on EKS Team"},{"location":"teams/aws-batch-on-eks-team/#aws-batch-on-eks-team","text":"The AWS Batch on EKS Team extends the ApplicationTeam and allows the Batch on EKS team to manage the namespace where the Batch Jobs are deployed. This team MUST be used in conjuction with EMR on EKS AddOn . The AWS Batch on EKS Team allows you to create a Compute Environment and a job queue to attach to the compute environment. Job queues are where jobs are submitted, and where they reside until they can be scheduled in the compute environment.","title":"AWS Batch on EKS Team"},{"location":"teams/aws-batch-on-eks-team/#usage","text":"import 'source-map-support/register' ; import * as cdk from 'aws-cdk-lib' ; import * as blueprints from '@aws-quickstart/eks-blueprints' ; const app = new cdk . App (); const addOn = new blueprints . addons . AwsBatchAddOn (); const batchTeam : BatchEksTeamProps = { name : 'batch-a' , namespace : 'aws-batch' , envName : 'batch-a-comp-env' , computeResources : { envType : BatchEnvType.EC2 , allocationStrategy : BatchAllocationStrategy.BEST , priority : 10 , minvCpus : 0 , maxvCpus : 128 , instanceTypes : [ \"m5\" , \"t3.large\" ] }, jobQueueName : 'team-a-job-queue' , }; const blueprint = blueprints . EksBlueprint . builder () . addOns ( addOn ) . teams ( new blueprints . BatchEksTeam ( batchTeam )) . build ( app , 'my-stack-name' );","title":"Usage"},{"location":"teams/aws-batch-on-eks-team/#create-a-job-definition","text":"Once you deploy the addon and the team, to run a batch job on EKS, you must first define a job. AWS Batch job definitions specify how jobs are to be run. The following is an example job definition you can set using AWS CLI: cat <<EOF > ./batch-eks-job-definition.json { \"jobDefinitionName\": \"MyJobOnEks_Sleep\", \"type\": \"container\", \"eksProperties\": { \"podProperties\": { \"hostNetwork\": true, \"containers\": [ { \"image\": \"public.ecr.aws/amazonlinux/amazonlinux:2\", \"command\": [ \"sleep\", \"60\" ], \"resources\": { \"limits\": { \"cpu\": \"1\", \"memory\": \"1024Mi\" } } } ] } } } EOF aws batch register-job-definition --cli-input-json file://./batch-eks-job-definition.json","title":"Create a Job definition"},{"location":"teams/aws-batch-on-eks-team/#submit-a-job","text":"Using the job definition, you can define and deploy a specific job using the following example AWS CLI command: aws batch submit-job --job-queue team-a-job-queue \\ --job-definition MyJobOnEks_Sleep --job-name My-Eks-Job1 You will get an output that lists the Job ID: { \"jobArn\": \"arn:aws:batch:us-west-2:123456789012:job/9518c9eb-b261-4732-a38d-54caf1d22229\", \"jobName\": \"My-Eks-Job1\", \"jobId\": \"9518c9eb-b261-4732-a38d-54caf1d22229\" }","title":"Submit a Job"},{"location":"teams/aws-batch-on-eks-team/#verify-job-completion","text":"You can see the job by running the following command: aws batch describe-jobs --job <jobId-from-submit-response> { \"jobs\": [ { \"jobArn\": \"arn:aws:batch:us-west-2:123456789012:job/9518c9eb-b261-4732-a38d-54caf1d22229\", \"jobName\": \"My-Eks-Job1\", \"jobId\": \"9518c9eb-b261-4732-a38d-54caf1d22229\", \"jobQueue\": \"arn:aws:batch:us-west-2:123456789012:job-queue/team-a-job-queue\", \"status\": \"RUNNABLE\", \"attempts\": [], \"createdAt\": 1676581820093, \"dependsOn\": [], \"jobDefinition\": \"arn:aws:batch:us-west-2:123456789012:job-definition/MyJobOnEks_Sleep:1\", \"parameters\": {}, \"tags\": {}, \"platformCapabilities\": [], \"eksProperties\": { \"podProperties\": { \"hostNetwork\": true, \"containers\": [ { \"image\": \"public.ecr.aws/amazonlinux/amazonlinux:2\", \"command\": [ \"sleep\", \"60\" ], \"args\": [], \"env\": [], \"resources\": { \"limits\": { \"memory\": \"1024Mi\", \"cpu\": \"1\" } }, \"volumeMounts\": [] } ], \"volumes\": [] } }, \"eksAttempts\": [] } ] } After a while, you can check that the pod has been created (and eventually deleted after job completion) under the aws-batch namespace to run the job: kubectl get pod -n aws-batch You can also check that the job has been completed by running the describe job command again and seeing the output. There should be description of the pod and node assignment for the job under eksAttempts : ....................... \"eksAttempts\": [ { \"containers\": [ { \"exitCode\": 0, \"reason\": \"Completed\" } ], \"podName\": \"aws-batch.fa024ec9-4232-3e82-b09b-4ba6ba396ec2\", \"nodeName\": \"ip-10-0-81-102.us-west-2.compute.internal\", \"startedAt\": 1676581976000, \"stoppedAt\": 1676582036000 } ] .......................","title":"Verify Job completion"},{"location":"teams/emr-eks-team/","text":"EMR on EKS Team \u00b6 The EMR on EKS Team extends the ApplicationTeam and allows the EMR on EKS team to manage the namespace where the virtual cluster is deployed. This team MUST be used in conjuction with EMR on EKS AddOn . The EMR on EKS Team allows you to create a Virtual Cluster and job Execution Roles that are used by the job to access data in Amazon S3, AWS Glue Data Catalog or any other AWS resources that you need to interact with. The job execution roles are scoped with IRSA to be only assumed by pods deployed by EMR on EKS in the namespace of the virtual cluster. You can learn more about the condition applied here . The IAM roles will have the following format: NAME-AWS_REGION-EKS_CLUSTER_NAME . Usage \u00b6 import 'source-map-support/register' ; import * as cdk from 'aws-cdk-lib' ; import * as blueprints from '@aws-quickstart/eks-blueprints' ; const app = new cdk . App (); const addOn = new blueprints . addons . EmrEksAddOn (); //The policy to be attached to the EMR on EKS execution role const executionRolePolicyStatement : PolicyStatement [] = [ new PolicyStatement ({ resources : [ '*' ], actions : [ 's3:*' ], }), new PolicyStatement ({ resources : [ '*' ], actions : [ 'glue:*' ], }), new PolicyStatement ({ resources : [ '*' ], actions : [ 'logs:*' , ], }), ]; const dataTeam : EmrEksTeamProps = { name : 'dataTeam' , virtualClusterName : 'batchJob' , virtualClusterNamespace : 'batchjob' , createNamespace : true , users : [ new ArnPrincipal ( `arn:aws:iam:: ${ YOUR_IAM_ACCOUNT } :user/user1` ), new ArnPrincipal ( `arn:aws:iam:: ${ YOUR_IAM_ACCOUNT } :user/user2` ) ], userRoleArn : new ArnPrincipal ( `arn:aws:iam:: ${ YOUR_IAM_ACCOUNT } :role/role1` ), executionRoles : [ { executionRoleIamPolicyStatement : executionRolePolicyStatement , executionRoleName : 'myBlueprintExecRole' } ] }; const blueprint = blueprints . EksBlueprint . builder () . addOns ( addOn ) . teams ( new blueprints . EmrEksTeam ( dataTeam )) . build ( app , 'my-stack-name' ); Submit a job \u00b6 Once you deploy the blueprint you will have as output the Virtual Cluster id . You can use the id and the execution role for which you supplied a policy to submit jobs. Below you can find an example of a job you can submit with AWS CLI. aws emr-containers start-job-run \\ --virtual-cluster-id=$VIRTUAL_CLUSTER_ID \\ --name=pi-2 \\ --execution-role-arn=$EMR_ROLE_ARN \\ --release-label=emr-6.8.0-latest \\ --job-driver='{ \"sparkSubmitJobDriver\": { \"entryPoint\": \"local:///usr/lib/spark/examples/src/main/python/pi.py\", \"sparkSubmitParameters\": \"--conf spark.executor.instances=1 --conf spark.executor.memory=2G --conf spark.executor.cores=1 --conf spark.driver.cores=1\" } }' Verify job submission \u00b6 Once you submit a job you can verify that it is running from the AWS console on the EMR service that the job is running. Below you can see a screenshot of the console.","title":"EMR on EKS Team"},{"location":"teams/emr-eks-team/#emr-on-eks-team","text":"The EMR on EKS Team extends the ApplicationTeam and allows the EMR on EKS team to manage the namespace where the virtual cluster is deployed. This team MUST be used in conjuction with EMR on EKS AddOn . The EMR on EKS Team allows you to create a Virtual Cluster and job Execution Roles that are used by the job to access data in Amazon S3, AWS Glue Data Catalog or any other AWS resources that you need to interact with. The job execution roles are scoped with IRSA to be only assumed by pods deployed by EMR on EKS in the namespace of the virtual cluster. You can learn more about the condition applied here . The IAM roles will have the following format: NAME-AWS_REGION-EKS_CLUSTER_NAME .","title":"EMR on EKS Team"},{"location":"teams/emr-eks-team/#usage","text":"import 'source-map-support/register' ; import * as cdk from 'aws-cdk-lib' ; import * as blueprints from '@aws-quickstart/eks-blueprints' ; const app = new cdk . App (); const addOn = new blueprints . addons . EmrEksAddOn (); //The policy to be attached to the EMR on EKS execution role const executionRolePolicyStatement : PolicyStatement [] = [ new PolicyStatement ({ resources : [ '*' ], actions : [ 's3:*' ], }), new PolicyStatement ({ resources : [ '*' ], actions : [ 'glue:*' ], }), new PolicyStatement ({ resources : [ '*' ], actions : [ 'logs:*' , ], }), ]; const dataTeam : EmrEksTeamProps = { name : 'dataTeam' , virtualClusterName : 'batchJob' , virtualClusterNamespace : 'batchjob' , createNamespace : true , users : [ new ArnPrincipal ( `arn:aws:iam:: ${ YOUR_IAM_ACCOUNT } :user/user1` ), new ArnPrincipal ( `arn:aws:iam:: ${ YOUR_IAM_ACCOUNT } :user/user2` ) ], userRoleArn : new ArnPrincipal ( `arn:aws:iam:: ${ YOUR_IAM_ACCOUNT } :role/role1` ), executionRoles : [ { executionRoleIamPolicyStatement : executionRolePolicyStatement , executionRoleName : 'myBlueprintExecRole' } ] }; const blueprint = blueprints . EksBlueprint . builder () . addOns ( addOn ) . teams ( new blueprints . EmrEksTeam ( dataTeam )) . build ( app , 'my-stack-name' );","title":"Usage"},{"location":"teams/emr-eks-team/#submit-a-job","text":"Once you deploy the blueprint you will have as output the Virtual Cluster id . You can use the id and the execution role for which you supplied a policy to submit jobs. Below you can find an example of a job you can submit with AWS CLI. aws emr-containers start-job-run \\ --virtual-cluster-id=$VIRTUAL_CLUSTER_ID \\ --name=pi-2 \\ --execution-role-arn=$EMR_ROLE_ARN \\ --release-label=emr-6.8.0-latest \\ --job-driver='{ \"sparkSubmitJobDriver\": { \"entryPoint\": \"local:///usr/lib/spark/examples/src/main/python/pi.py\", \"sparkSubmitParameters\": \"--conf spark.executor.instances=1 --conf spark.executor.memory=2G --conf spark.executor.cores=1 --conf spark.driver.cores=1\" } }'","title":"Submit a job"},{"location":"teams/emr-eks-team/#verify-job-submission","text":"Once you submit a job you can verify that it is running from the AWS console on the EMR service that the job is running. Below you can see a screenshot of the console.","title":"Verify job submission"},{"location":"teams/teams/","text":"Teams \u00b6 The eks-blueprints framework provides support for onboarding and managing teams and easily configuring cluster access. We currently support two Team types: ApplicationTeam and PlatformTeam . ApplicationTeam represent teams managing workloads running in cluster namespaces and PlatformTeam represents platform administrators who have admin access (masters group) to clusters. You are also able to create your own team implementations by creating classes that inherits from Team . ApplicationTeam \u00b6 To create an ApplicationTeam for your cluster, simply implement a class that extends ApplicationTeam . You will need to supply a team name, an array of users, and (optionally) a directory where you may optionally place any policy definitions and generic manifests for the team. These manifests will be applied by the platform and will be outside of the team control NOTE: When the manifests are applied, namespaces are not checked. Therefore, you are responsible for namespace settings in the yaml files. export class TeamAwesome extends ApplicationTeam { constructor ( app : App ) { super ({ name : \"team-awesome\" , users : [ new ArnPrincipal ( `arn:aws:iam:: ${ YOUR_IAM_ACCOUNT } :user/user1` ), new ArnPrincipal ( `arn:aws:iam:: ${ YOUR_IAM_ACCOUNT } :user/user2` ) ], teamManifestDir : './examples/teams/team-awesome/' }); } } The ApplicationTeam will do the following: Create a namespace Register quotas Register IAM users for cross-account access Create a shared role for cluster access. Alternatively, an existing role can be supplied. Register provided users/role in the awsAuth map for kubectl and console access to the cluster and namespace. (Optionally) read all additional manifests (e.g., network policies, OPA policies, others) stored in a provided directory, and applies them. PlatformTeam \u00b6 To create an PlatformTeam for your cluster, simply implement a class that extends PlatformTeam . You will need to supply a team name and an array of users. export class TeamAwesome extends PlatformTeam { constructor ( app : App ) { super ({ name : \"team-awesome\" , users : [ new ArnPrincipal ( `arn:aws:iam:: ${ YOUR_IAM_ACCOUNT } :user/user1` ), new ArnPrincipal ( `arn:aws:iam:: ${ YOUR_IAM_ACCOUNT } :user/user2` ) ] }); } } The PlatformTeam class does the following: registers IAM users for admin access to the cluster ( kubectl and console) registers an existing role (or create a new role) for cluster access with trust relationship with the provided/created role To reduce verbosity for some of the use cases, such as for platform teams, when in reality the use case is simply to enable admin cluster access for a specific role the blueprint provides support for add-hoc team creation as well. For example: const adminTeam = new PlatformTeam ( { name : \"second-adminteam\" , // make sure this is unique within organization userRoleArn : ` ${ YOUR_ROLE_ARN } ` ; }) NOTE: YOUR_ROLE_ARN should be stripped of any path component, as explained in the User Guide . So, for example, arn:aws:iam::111122223333:role/team/developers/eks-admin should be changed to arn:aws:iam::111122223333:role/eks-admin . DefaultTeamRoles \u00b6 The DefaultTeamRoles class provides default RBAC configuration for ApplicationTeams : Cluster role, group identity and cluster role bindings to view nodes and namespaces Namespace role and role binding for the group to view pods, deployments, daemonsets, services Team Benefits \u00b6 By managing teams via infrastructure as code, we achieve the following benefits: Self-documenting code Centralized logic related to the team Clear place where to add additional provisioning, for example adding Kubernetes Service Accounts and/or infrastructure, such as S3 buckets IDE support to locate the required team, e.g. CTRL+T in VSCode to lookup class name. The example above is shown for a platform team, but it could be similarly applied to a regular team with restricted access. Cluster Access ( kubectl ) \u00b6 The stack output will contain the kubeconfig update command, which should be shared with the development and platform teams. ${teamname}teamrole arn:aws:iam::${account}:role/west-dev-${teamname}AccessRole3CDA6927-1QA4S3TYMY36N platformteamadmin arn:aws:iam::${account}:role/west-dev-${platform-team-name}AccessRole57468BEC-8JYMM0HZZ2CE teamtroisaiamrole arn:aws:iam::${account}:role/west-dev-westdevinfbackendRole861AD63A-2K9W8X4DDF46 westdevConfigCommand1AE70258 aws eks update-kubeconfig --name west-dev --region us-west-1 --role-arn arn:aws:iam::${account}:role/west-dev-westdevMastersRole509E4B82-101MDZNTGFF08 Note the last command is to update kubeconfig with the proper context to access cluster using kubectl . The last argument of this command is --role-arn which by default is set to the cluster master role. Developers (members of each team) should use the role name for the team role, such as burnhamteamrole for team name burnham . Platform administrators must use the role output for their team name, such as platformteamadmin in the above example. Console Access \u00b6 Provided that each team has received the name of the role that was created for the cluster access, each team member listed in the users section will be able to assume the role in the target account. To do that, users should use \"Switch Roles\" function in the console and specify the provided role. This will enable EKS console access to list clusters and to get console visibility into the workloads that belong to the team. Examples \u00b6 There are a few team examples under /teams folder. The example for team-burnham includes a way to specify IAM users through a local or project CDK context. Project context is defined in cdk.json under context key and local context is defined in ~/.cdk.json under context key. Example: \u279c cat ~/.cdk.json { \"context\": { \"team-burnham.users\": \"arn:aws:iam::YOUR_ACCOUNT:user/dev1,arn:aws:iam::YOUR_ACCOUNT:user/dev2\" } }","title":"Teams"},{"location":"teams/teams/#teams","text":"The eks-blueprints framework provides support for onboarding and managing teams and easily configuring cluster access. We currently support two Team types: ApplicationTeam and PlatformTeam . ApplicationTeam represent teams managing workloads running in cluster namespaces and PlatformTeam represents platform administrators who have admin access (masters group) to clusters. You are also able to create your own team implementations by creating classes that inherits from Team .","title":"Teams"},{"location":"teams/teams/#applicationteam","text":"To create an ApplicationTeam for your cluster, simply implement a class that extends ApplicationTeam . You will need to supply a team name, an array of users, and (optionally) a directory where you may optionally place any policy definitions and generic manifests for the team. These manifests will be applied by the platform and will be outside of the team control NOTE: When the manifests are applied, namespaces are not checked. Therefore, you are responsible for namespace settings in the yaml files. export class TeamAwesome extends ApplicationTeam { constructor ( app : App ) { super ({ name : \"team-awesome\" , users : [ new ArnPrincipal ( `arn:aws:iam:: ${ YOUR_IAM_ACCOUNT } :user/user1` ), new ArnPrincipal ( `arn:aws:iam:: ${ YOUR_IAM_ACCOUNT } :user/user2` ) ], teamManifestDir : './examples/teams/team-awesome/' }); } } The ApplicationTeam will do the following: Create a namespace Register quotas Register IAM users for cross-account access Create a shared role for cluster access. Alternatively, an existing role can be supplied. Register provided users/role in the awsAuth map for kubectl and console access to the cluster and namespace. (Optionally) read all additional manifests (e.g., network policies, OPA policies, others) stored in a provided directory, and applies them.","title":"ApplicationTeam"},{"location":"teams/teams/#platformteam","text":"To create an PlatformTeam for your cluster, simply implement a class that extends PlatformTeam . You will need to supply a team name and an array of users. export class TeamAwesome extends PlatformTeam { constructor ( app : App ) { super ({ name : \"team-awesome\" , users : [ new ArnPrincipal ( `arn:aws:iam:: ${ YOUR_IAM_ACCOUNT } :user/user1` ), new ArnPrincipal ( `arn:aws:iam:: ${ YOUR_IAM_ACCOUNT } :user/user2` ) ] }); } } The PlatformTeam class does the following: registers IAM users for admin access to the cluster ( kubectl and console) registers an existing role (or create a new role) for cluster access with trust relationship with the provided/created role To reduce verbosity for some of the use cases, such as for platform teams, when in reality the use case is simply to enable admin cluster access for a specific role the blueprint provides support for add-hoc team creation as well. For example: const adminTeam = new PlatformTeam ( { name : \"second-adminteam\" , // make sure this is unique within organization userRoleArn : ` ${ YOUR_ROLE_ARN } ` ; }) NOTE: YOUR_ROLE_ARN should be stripped of any path component, as explained in the User Guide . So, for example, arn:aws:iam::111122223333:role/team/developers/eks-admin should be changed to arn:aws:iam::111122223333:role/eks-admin .","title":"PlatformTeam"},{"location":"teams/teams/#defaultteamroles","text":"The DefaultTeamRoles class provides default RBAC configuration for ApplicationTeams : Cluster role, group identity and cluster role bindings to view nodes and namespaces Namespace role and role binding for the group to view pods, deployments, daemonsets, services","title":"DefaultTeamRoles"},{"location":"teams/teams/#team-benefits","text":"By managing teams via infrastructure as code, we achieve the following benefits: Self-documenting code Centralized logic related to the team Clear place where to add additional provisioning, for example adding Kubernetes Service Accounts and/or infrastructure, such as S3 buckets IDE support to locate the required team, e.g. CTRL+T in VSCode to lookup class name. The example above is shown for a platform team, but it could be similarly applied to a regular team with restricted access.","title":"Team Benefits"},{"location":"teams/teams/#cluster-access-kubectl","text":"The stack output will contain the kubeconfig update command, which should be shared with the development and platform teams. ${teamname}teamrole arn:aws:iam::${account}:role/west-dev-${teamname}AccessRole3CDA6927-1QA4S3TYMY36N platformteamadmin arn:aws:iam::${account}:role/west-dev-${platform-team-name}AccessRole57468BEC-8JYMM0HZZ2CE teamtroisaiamrole arn:aws:iam::${account}:role/west-dev-westdevinfbackendRole861AD63A-2K9W8X4DDF46 westdevConfigCommand1AE70258 aws eks update-kubeconfig --name west-dev --region us-west-1 --role-arn arn:aws:iam::${account}:role/west-dev-westdevMastersRole509E4B82-101MDZNTGFF08 Note the last command is to update kubeconfig with the proper context to access cluster using kubectl . The last argument of this command is --role-arn which by default is set to the cluster master role. Developers (members of each team) should use the role name for the team role, such as burnhamteamrole for team name burnham . Platform administrators must use the role output for their team name, such as platformteamadmin in the above example.","title":"Cluster Access (kubectl)"},{"location":"teams/teams/#console-access","text":"Provided that each team has received the name of the role that was created for the cluster access, each team member listed in the users section will be able to assume the role in the target account. To do that, users should use \"Switch Roles\" function in the console and specify the provided role. This will enable EKS console access to list clusters and to get console visibility into the workloads that belong to the team.","title":"Console Access"},{"location":"teams/teams/#examples","text":"There are a few team examples under /teams folder. The example for team-burnham includes a way to specify IAM users through a local or project CDK context. Project context is defined in cdk.json under context key and local context is defined in ~/.cdk.json under context key. Example: \u279c cat ~/.cdk.json { \"context\": { \"team-burnham.users\": \"arn:aws:iam::YOUR_ACCOUNT:user/dev1,arn:aws:iam::YOUR_ACCOUNT:user/dev2\" } }","title":"Examples"}]}